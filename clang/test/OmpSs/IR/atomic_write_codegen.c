// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs
// RUN: %clang_cc1 -x c -no-opaque-pointers -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -x c -no-opaque-pointers -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -x c -no-opaque-pointers -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics
#ifndef HEADER
#define HEADER

_Bool bv, bx;
char cv, cx;
unsigned char ucv, ucx;
short sv, sx;
unsigned short usv, usx;
int iv, ix;
unsigned int uiv, uix;
long lv, lx;
unsigned long ulv, ulx;
long long llv, llx;
unsigned long long ullv, ullx;
float fv, fx;
double dv, dx;
long double ldv, ldx;
_Complex int civ, cix;
_Complex float cfv, cfx;
_Complex double cdv, cdx;

typedef int int4 __attribute__((__vector_size__(16)));
int4 int4x;

struct BitFields {
  int : 32;
  int a : 31;
} bfx;

struct BitFields_packed {
  int : 32;
  int a : 31;
} __attribute__ ((__packed__)) bfx_packed;

struct BitFields2 {
  int : 31;
  int a : 1;
} bfx2;

struct BitFields2_packed {
  int : 31;
  int a : 1;
} __attribute__ ((__packed__)) bfx2_packed;

struct BitFields3 {
  int : 11;
  int a : 14;
} bfx3;

struct BitFields3_packed {
  int : 11;
  int a : 14;
} __attribute__ ((__packed__)) bfx3_packed;

struct BitFields4 {
  short : 16;
  int a: 1;
  long b : 7;
} bfx4;

struct BitFields4_packed {
  short : 16;
  int a: 1;
  long b : 7;
} __attribute__ ((__packed__)) bfx4_packed;

typedef float float2 __attribute__((ext_vector_type(2)));
float2 float2x;

#if defined(__x86_64__)
// Register "0" is currently an invalid register for global register variables.
// Use "esp" instead of "0".
// register int rix __asm__("0");
register int rix __asm__("esp");
#endif

int main(void) {
#pragma oss atomic write
 __imag(civ) = 1;
#pragma oss atomic write
  bx = bv;
#pragma oss atomic write release
  cx = cv;
#pragma oss atomic write
  ucx = ucv;
#pragma oss atomic write
  sx = sv;
#pragma oss atomic write
  usx = usv;
#pragma oss atomic write
  ix = iv;
#pragma oss atomic write
  uix = uiv;
#pragma oss atomic write
  lx = lv;
#pragma oss atomic write
  ulx = ulv;
#pragma oss atomic write
  llx = llv;
#pragma oss atomic write
  ullx = ullv;
#pragma oss atomic write
  fx = fv;
#pragma oss atomic write
  dx = dv;
#pragma oss atomic write
  ldx = ldv;
#pragma oss atomic write
  cix = civ;
#pragma oss atomic write
  cfx = cfv;
#pragma oss atomic seq_cst write
  cdx = cdv;
#pragma oss atomic write
  ulx = bv;
#pragma oss atomic write
  bx = cv;
#pragma oss atomic write, seq_cst
  cx = ucv;
#pragma oss atomic write
  ulx = sv;
#pragma oss atomic write
  lx = usv;
#pragma oss atomic seq_cst, write
  uix = iv;
#pragma oss atomic write
  ix = uiv;
#pragma oss atomic write
  cix = lv;
#pragma oss atomic write
  fx = ulv;
#pragma oss atomic write
  dx = llv;
#pragma oss atomic write
  ldx = ullv;
#pragma oss atomic write
  cix = fv;
#pragma oss atomic write
  sx = dv;
#pragma oss atomic write
  bx = ldv;
#pragma oss atomic write
  bx = civ;
#pragma oss atomic write
  usx = cfv;
#pragma oss atomic write
  llx = cdv;
#pragma oss atomic write
  int4x[sv] = bv;
#pragma oss atomic write
  bfx.a = ldv;
#pragma oss atomic write
  bfx_packed.a = ldv;
#pragma oss atomic write
  bfx2.a = ldv;
#pragma oss atomic write
  bfx2_packed.a = ldv;
#pragma oss atomic write
  bfx3.a = ldv;
#pragma oss atomic write
  bfx3_packed.a = ldv;
#pragma oss atomic write
  bfx4.a = ldv;
#pragma oss atomic write
  bfx4_packed.a = ldv;
#pragma oss atomic write
  bfx4.b = ldv;
#pragma oss atomic relaxed write
  bfx4_packed.b = ldv;
#pragma oss atomic write relaxed
  float2x.x = ulv;
#if defined(__x86_64__)
#pragma oss atomic write seq_cst
  dv = rix;
#endif
  return 0;
}

#endif
// LIN64-LABEL: define {{[^@]+}}@main
// LIN64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG6:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca x86_fp80, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP1:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP2:%.*]] = alloca { float, float }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP3:%.*]] = alloca { double, double }, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP10:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca x86_fp80, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP16:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP32:%.*]] = alloca <4 x i32>, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP33:%.*]] = alloca <4 x i32>, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP36:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP39:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP41:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP51:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP60:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP70:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP78:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP80:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP91:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP101:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP110:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP120:%.*]] = alloca i64, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP130:%.*]] = alloca <2 x float>, align 8
// LIN64-NEXT:    store i32 0, i32* [[RETVAL]], align 4
// LIN64-NEXT:    store atomic i32 1, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1) monotonic, align 4, !dbg [[DBG10:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = load i8, i8* @bv, align 1, !dbg [[DBG11:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[TMP0]] to i1, !dbg [[DBG11]]
// LIN64-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG12:![0-9]+]]
// LIN64-NEXT:    store atomic i8 [[FROMBOOL]], i8* @bx monotonic, align 1, !dbg [[DBG12]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i8, i8* @cv, align 1, !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    store atomic i8 [[TMP1]], i8* @cx release, align 1, !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    fence release
// LIN64-NEXT:    [[TMP2:%.*]] = load i8, i8* @ucv, align 1, !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    store atomic i8 [[TMP2]], i8* @ucx monotonic, align 1, !dbg [[DBG16:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = load i16, i16* @sv, align 2, !dbg [[DBG17:![0-9]+]]
// LIN64-NEXT:    store atomic i16 [[TMP3]], i16* @sx monotonic, align 2, !dbg [[DBG18:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = load i16, i16* @usv, align 2, !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    store atomic i16 [[TMP4]], i16* @usx monotonic, align 2, !dbg [[DBG20:![0-9]+]]
// LIN64-NEXT:    [[TMP5:%.*]] = load i32, i32* @iv, align 4, !dbg [[DBG21:![0-9]+]]
// LIN64-NEXT:    store atomic i32 [[TMP5]], i32* @ix monotonic, align 4, !dbg [[DBG22:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = load i32, i32* @uiv, align 4, !dbg [[DBG23:![0-9]+]]
// LIN64-NEXT:    store atomic i32 [[TMP6]], i32* @uix monotonic, align 4, !dbg [[DBG24:![0-9]+]]
// LIN64-NEXT:    [[TMP7:%.*]] = load i64, i64* @lv, align 8, !dbg [[DBG25:![0-9]+]]
// LIN64-NEXT:    store atomic i64 [[TMP7]], i64* @lx monotonic, align 8, !dbg [[DBG26:![0-9]+]]
// LIN64-NEXT:    [[TMP8:%.*]] = load i64, i64* @ulv, align 8, !dbg [[DBG27:![0-9]+]]
// LIN64-NEXT:    store atomic i64 [[TMP8]], i64* @ulx monotonic, align 8, !dbg [[DBG28:![0-9]+]]
// LIN64-NEXT:    [[TMP9:%.*]] = load i64, i64* @llv, align 8, !dbg [[DBG29:![0-9]+]]
// LIN64-NEXT:    store atomic i64 [[TMP9]], i64* @llx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// LIN64-NEXT:    [[TMP10:%.*]] = load i64, i64* @ullv, align 8, !dbg [[DBG31:![0-9]+]]
// LIN64-NEXT:    store atomic i64 [[TMP10]], i64* @ullx monotonic, align 8, !dbg [[DBG32:![0-9]+]]
// LIN64-NEXT:    [[TMP11:%.*]] = load float, float* @fv, align 4, !dbg [[DBG33:![0-9]+]]
// LIN64-NEXT:    [[TMP12:%.*]] = bitcast float [[TMP11]] to i32, !dbg [[DBG34:![0-9]+]]
// LIN64-NEXT:    store atomic i32 [[TMP12]], i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG34]]
// LIN64-NEXT:    [[TMP13:%.*]] = load double, double* @dv, align 8, !dbg [[DBG35:![0-9]+]]
// LIN64-NEXT:    [[TMP14:%.*]] = bitcast double [[TMP13]] to i64, !dbg [[DBG36:![0-9]+]]
// LIN64-NEXT:    store atomic i64 [[TMP14]], i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG36]]
// LIN64-NEXT:    [[TMP15:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG37:![0-9]+]]
// LIN64-NEXT:    [[TMP16:%.*]] = bitcast x86_fp80* [[ATOMIC_TEMP]] to i8*, !dbg [[DBG38:![0-9]+]]
// LIN64-NEXT:    call void @llvm.memset.p0i8.i64(i8* align 16 [[TMP16]], i8 0, i64 16, i1 false), !dbg [[DBG38]]
// LIN64-NEXT:    store x86_fp80 [[TMP15]], x86_fp80* [[ATOMIC_TEMP]], align 16, !dbg [[DBG38]]
// LIN64-NEXT:    [[TMP17:%.*]] = bitcast x86_fp80* [[ATOMIC_TEMP]] to i8*, !dbg [[DBG38]]
// LIN64-NEXT:    call void @__atomic_store(i64 noundef 16, i8* noundef bitcast (x86_fp80* @ldx to i8*), i8* noundef [[TMP17]], i32 noundef 0), !dbg [[DBG38]]
// LIN64-NEXT:    [[CIV_REAL:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG39:![0-9]+]]
// LIN64-NEXT:    [[CIV_IMAG:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG39]]
// LIN64-NEXT:    [[ATOMIC_TEMP1_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP1]], i32 0, i32 0, !dbg [[DBG40:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP1_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP1]], i32 0, i32 1, !dbg [[DBG40]]
// LIN64-NEXT:    store i32 [[CIV_REAL]], i32* [[ATOMIC_TEMP1_REALP]], align 4, !dbg [[DBG40]]
// LIN64-NEXT:    store i32 [[CIV_IMAG]], i32* [[ATOMIC_TEMP1_IMAGP]], align 4, !dbg [[DBG40]]
// LIN64-NEXT:    [[TMP18:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP1]] to i8*, !dbg [[DBG40]]
// LIN64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP18]], i32 noundef 0), !dbg [[DBG40]]
// LIN64-NEXT:    [[CFV_REAL:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG41:![0-9]+]]
// LIN64-NEXT:    [[CFV_IMAG:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG41]]
// LIN64-NEXT:    [[ATOMIC_TEMP2_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP2]], i32 0, i32 0, !dbg [[DBG42:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP2_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP2]], i32 0, i32 1, !dbg [[DBG42]]
// LIN64-NEXT:    store float [[CFV_REAL]], float* [[ATOMIC_TEMP2_REALP]], align 4, !dbg [[DBG42]]
// LIN64-NEXT:    store float [[CFV_IMAG]], float* [[ATOMIC_TEMP2_IMAGP]], align 4, !dbg [[DBG42]]
// LIN64-NEXT:    [[TMP19:%.*]] = bitcast { float, float }* [[ATOMIC_TEMP2]] to i8*, !dbg [[DBG42]]
// LIN64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ float, float }* @cfx to i8*), i8* noundef [[TMP19]], i32 noundef 0), !dbg [[DBG42]]
// LIN64-NEXT:    [[CDV_REAL:%.*]] = load double, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG43:![0-9]+]]
// LIN64-NEXT:    [[CDV_IMAG:%.*]] = load double, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG43]]
// LIN64-NEXT:    [[ATOMIC_TEMP3_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP3]], i32 0, i32 0, !dbg [[DBG44:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP3_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP3]], i32 0, i32 1, !dbg [[DBG44]]
// LIN64-NEXT:    store double [[CDV_REAL]], double* [[ATOMIC_TEMP3_REALP]], align 8, !dbg [[DBG44]]
// LIN64-NEXT:    store double [[CDV_IMAG]], double* [[ATOMIC_TEMP3_IMAGP]], align 8, !dbg [[DBG44]]
// LIN64-NEXT:    [[TMP20:%.*]] = bitcast { double, double }* [[ATOMIC_TEMP3]] to i8*, !dbg [[DBG44]]
// LIN64-NEXT:    call void @__atomic_store(i64 noundef 16, i8* noundef bitcast ({ double, double }* @cdx to i8*), i8* noundef [[TMP20]], i32 noundef 5), !dbg [[DBG44]]
// LIN64-NEXT:    fence release
// LIN64-NEXT:    [[TMP21:%.*]] = load i8, i8* @bv, align 1, !dbg [[DBG45:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL4:%.*]] = trunc i8 [[TMP21]] to i1, !dbg [[DBG45]]
// LIN64-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL4]] to i64, !dbg [[DBG45]]
// LIN64-NEXT:    store atomic i64 [[CONV]], i64* @ulx monotonic, align 8, !dbg [[DBG46:![0-9]+]]
// LIN64-NEXT:    [[TMP22:%.*]] = load i8, i8* @cv, align 1, !dbg [[DBG47:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL5:%.*]] = icmp ne i8 [[TMP22]], 0, !dbg [[DBG47]]
// LIN64-NEXT:    [[FROMBOOL6:%.*]] = zext i1 [[TOBOOL5]] to i8, !dbg [[DBG48:![0-9]+]]
// LIN64-NEXT:    store atomic i8 [[FROMBOOL6]], i8* @bx monotonic, align 1, !dbg [[DBG48]]
// LIN64-NEXT:    [[TMP23:%.*]] = load i8, i8* @ucv, align 1, !dbg [[DBG49:![0-9]+]]
// LIN64-NEXT:    store atomic i8 [[TMP23]], i8* @cx seq_cst, align 1, !dbg [[DBG50:![0-9]+]]
// LIN64-NEXT:    fence release
// LIN64-NEXT:    [[TMP24:%.*]] = load i16, i16* @sv, align 2, !dbg [[DBG51:![0-9]+]]
// LIN64-NEXT:    [[CONV7:%.*]] = sext i16 [[TMP24]] to i64, !dbg [[DBG51]]
// LIN64-NEXT:    store atomic i64 [[CONV7]], i64* @ulx monotonic, align 8, !dbg [[DBG52:![0-9]+]]
// LIN64-NEXT:    [[TMP25:%.*]] = load i16, i16* @usv, align 2, !dbg [[DBG53:![0-9]+]]
// LIN64-NEXT:    [[CONV8:%.*]] = zext i16 [[TMP25]] to i64, !dbg [[DBG53]]
// LIN64-NEXT:    store atomic i64 [[CONV8]], i64* @lx monotonic, align 8, !dbg [[DBG54:![0-9]+]]
// LIN64-NEXT:    [[TMP26:%.*]] = load i32, i32* @iv, align 4, !dbg [[DBG55:![0-9]+]]
// LIN64-NEXT:    store atomic i32 [[TMP26]], i32* @uix seq_cst, align 4, !dbg [[DBG56:![0-9]+]]
// LIN64-NEXT:    fence release
// LIN64-NEXT:    [[TMP27:%.*]] = load i32, i32* @uiv, align 4, !dbg [[DBG57:![0-9]+]]
// LIN64-NEXT:    store atomic i32 [[TMP27]], i32* @ix monotonic, align 4, !dbg [[DBG58:![0-9]+]]
// LIN64-NEXT:    [[TMP28:%.*]] = load i64, i64* @lv, align 8, !dbg [[DBG59:![0-9]+]]
// LIN64-NEXT:    [[CONV9:%.*]] = trunc i64 [[TMP28]] to i32, !dbg [[DBG59]]
// LIN64-NEXT:    [[ATOMIC_TEMP10_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP10]], i32 0, i32 0, !dbg [[DBG60:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP10_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP10]], i32 0, i32 1, !dbg [[DBG60]]
// LIN64-NEXT:    store i32 [[CONV9]], i32* [[ATOMIC_TEMP10_REALP]], align 4, !dbg [[DBG60]]
// LIN64-NEXT:    store i32 0, i32* [[ATOMIC_TEMP10_IMAGP]], align 4, !dbg [[DBG60]]
// LIN64-NEXT:    [[TMP29:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP10]] to i8*, !dbg [[DBG60]]
// LIN64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP29]], i32 noundef 0), !dbg [[DBG60]]
// LIN64-NEXT:    [[TMP30:%.*]] = load i64, i64* @ulv, align 8, !dbg [[DBG61:![0-9]+]]
// LIN64-NEXT:    [[CONV11:%.*]] = uitofp i64 [[TMP30]] to float, !dbg [[DBG61]]
// LIN64-NEXT:    [[TMP31:%.*]] = bitcast float [[CONV11]] to i32, !dbg [[DBG62:![0-9]+]]
// LIN64-NEXT:    store atomic i32 [[TMP31]], i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG62]]
// LIN64-NEXT:    [[TMP32:%.*]] = load i64, i64* @llv, align 8, !dbg [[DBG63:![0-9]+]]
// LIN64-NEXT:    [[CONV12:%.*]] = sitofp i64 [[TMP32]] to double, !dbg [[DBG63]]
// LIN64-NEXT:    [[TMP33:%.*]] = bitcast double [[CONV12]] to i64, !dbg [[DBG64:![0-9]+]]
// LIN64-NEXT:    store atomic i64 [[TMP33]], i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG64]]
// LIN64-NEXT:    [[TMP34:%.*]] = load i64, i64* @ullv, align 8, !dbg [[DBG65:![0-9]+]]
// LIN64-NEXT:    [[CONV13:%.*]] = uitofp i64 [[TMP34]] to x86_fp80, !dbg [[DBG65]]
// LIN64-NEXT:    [[TMP35:%.*]] = bitcast x86_fp80* [[ATOMIC_TEMP14]] to i8*, !dbg [[DBG66:![0-9]+]]
// LIN64-NEXT:    call void @llvm.memset.p0i8.i64(i8* align 16 [[TMP35]], i8 0, i64 16, i1 false), !dbg [[DBG66]]
// LIN64-NEXT:    store x86_fp80 [[CONV13]], x86_fp80* [[ATOMIC_TEMP14]], align 16, !dbg [[DBG66]]
// LIN64-NEXT:    [[TMP36:%.*]] = bitcast x86_fp80* [[ATOMIC_TEMP14]] to i8*, !dbg [[DBG66]]
// LIN64-NEXT:    call void @__atomic_store(i64 noundef 16, i8* noundef bitcast (x86_fp80* @ldx to i8*), i8* noundef [[TMP36]], i32 noundef 0), !dbg [[DBG66]]
// LIN64-NEXT:    [[TMP37:%.*]] = load float, float* @fv, align 4, !dbg [[DBG67:![0-9]+]]
// LIN64-NEXT:    [[CONV15:%.*]] = fptosi float [[TMP37]] to i32, !dbg [[DBG67]]
// LIN64-NEXT:    [[ATOMIC_TEMP16_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP16]], i32 0, i32 0, !dbg [[DBG68:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP16_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP16]], i32 0, i32 1, !dbg [[DBG68]]
// LIN64-NEXT:    store i32 [[CONV15]], i32* [[ATOMIC_TEMP16_REALP]], align 4, !dbg [[DBG68]]
// LIN64-NEXT:    store i32 0, i32* [[ATOMIC_TEMP16_IMAGP]], align 4, !dbg [[DBG68]]
// LIN64-NEXT:    [[TMP38:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP16]] to i8*, !dbg [[DBG68]]
// LIN64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP38]], i32 noundef 0), !dbg [[DBG68]]
// LIN64-NEXT:    [[TMP39:%.*]] = load double, double* @dv, align 8, !dbg [[DBG69:![0-9]+]]
// LIN64-NEXT:    [[CONV17:%.*]] = fptosi double [[TMP39]] to i16, !dbg [[DBG69]]
// LIN64-NEXT:    store atomic i16 [[CONV17]], i16* @sx monotonic, align 2, !dbg [[DBG70:![0-9]+]]
// LIN64-NEXT:    [[TMP40:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG71:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL18:%.*]] = fcmp une x86_fp80 [[TMP40]], 0xK00000000000000000000, !dbg [[DBG71]]
// LIN64-NEXT:    [[FROMBOOL19:%.*]] = zext i1 [[TOBOOL18]] to i8, !dbg [[DBG72:![0-9]+]]
// LIN64-NEXT:    store atomic i8 [[FROMBOOL19]], i8* @bx monotonic, align 1, !dbg [[DBG72]]
// LIN64-NEXT:    [[CIV_REAL20:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG73:![0-9]+]]
// LIN64-NEXT:    [[CIV_IMAG21:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG73]]
// LIN64-NEXT:    [[TOBOOL22:%.*]] = icmp ne i32 [[CIV_REAL20]], 0, !dbg [[DBG73]]
// LIN64-NEXT:    [[TOBOOL23:%.*]] = icmp ne i32 [[CIV_IMAG21]], 0, !dbg [[DBG73]]
// LIN64-NEXT:    [[TOBOOL24:%.*]] = or i1 [[TOBOOL22]], [[TOBOOL23]], !dbg [[DBG73]]
// LIN64-NEXT:    [[FROMBOOL25:%.*]] = zext i1 [[TOBOOL24]] to i8, !dbg [[DBG74:![0-9]+]]
// LIN64-NEXT:    store atomic i8 [[FROMBOOL25]], i8* @bx monotonic, align 1, !dbg [[DBG74]]
// LIN64-NEXT:    [[CFV_REAL26:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG75:![0-9]+]]
// LIN64-NEXT:    [[CONV27:%.*]] = fptoui float [[CFV_REAL26]] to i16, !dbg [[DBG75]]
// LIN64-NEXT:    store atomic i16 [[CONV27]], i16* @usx monotonic, align 2, !dbg [[DBG76:![0-9]+]]
// LIN64-NEXT:    [[CDV_REAL28:%.*]] = load double, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG77:![0-9]+]]
// LIN64-NEXT:    [[CONV29:%.*]] = fptosi double [[CDV_REAL28]] to i64, !dbg [[DBG77]]
// LIN64-NEXT:    store atomic i64 [[CONV29]], i64* @llx monotonic, align 8, !dbg [[DBG78:![0-9]+]]
// LIN64-NEXT:    [[TMP41:%.*]] = load i16, i16* @sv, align 2, !dbg [[DBG79:![0-9]+]]
// LIN64-NEXT:    [[TMP42:%.*]] = load i8, i8* @bv, align 1, !dbg [[DBG80:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL30:%.*]] = trunc i8 [[TMP42]] to i1, !dbg [[DBG80]]
// LIN64-NEXT:    [[CONV31:%.*]] = zext i1 [[TOBOOL30]] to i32, !dbg [[DBG80]]
// LIN64-NEXT:    [[TMP43:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP32]] to i8*, !dbg [[DBG81:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (<4 x i32>* @int4x to i8*), i8* noundef [[TMP43]], i32 noundef 0), !dbg [[DBG81]]
// LIN64-NEXT:    br label [[ATOMIC_CONT:%.*]], !dbg [[DBG81]]
// LIN64:       atomic_cont:
// LIN64-NEXT:    [[TMP44:%.*]] = load <4 x i32>, <4 x i32>* [[ATOMIC_TEMP32]], align 16, !dbg [[DBG81]]
// LIN64-NEXT:    store <4 x i32> [[TMP44]], <4 x i32>* [[ATOMIC_TEMP33]], align 16, !dbg [[DBG81]]
// LIN64-NEXT:    [[TMP45:%.*]] = load <4 x i32>, <4 x i32>* [[ATOMIC_TEMP33]], align 16, !dbg [[DBG81]]
// LIN64-NEXT:    [[VECINS:%.*]] = insertelement <4 x i32> [[TMP45]], i32 [[CONV31]], i16 [[TMP41]], !dbg [[DBG81]]
// LIN64-NEXT:    store <4 x i32> [[VECINS]], <4 x i32>* [[ATOMIC_TEMP33]], align 16, !dbg [[DBG81]]
// LIN64-NEXT:    [[TMP46:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP32]] to i8*, !dbg [[DBG81]]
// LIN64-NEXT:    [[TMP47:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP33]] to i8*, !dbg [[DBG81]]
// LIN64-NEXT:    [[CALL:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 16, i8* noundef bitcast (<4 x i32>* @int4x to i8*), i8* noundef [[TMP46]], i8* noundef [[TMP47]], i32 noundef 0, i32 noundef 0), !dbg [[DBG81]]
// LIN64-NEXT:    br i1 [[CALL]], label [[ATOMIC_EXIT:%.*]], label [[ATOMIC_CONT]], !dbg [[DBG81]]
// LIN64:       atomic_exit:
// LIN64-NEXT:    [[TMP48:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG82:![0-9]+]]
// LIN64-NEXT:    [[CONV34:%.*]] = fptosi x86_fp80 [[TMP48]] to i32, !dbg [[DBG82]]
// LIN64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i32, i32* bitcast (i8* getelementptr (i8, i8* bitcast (%struct.BitFields* @bfx to i8*), i64 4) to i32*) monotonic, align 4, !dbg [[DBG83:![0-9]+]]
// LIN64-NEXT:    br label [[ATOMIC_CONT35:%.*]], !dbg [[DBG83]]
// LIN64:       atomic_cont35:
// LIN64-NEXT:    [[TMP49:%.*]] = phi i32 [ [[ATOMIC_LOAD]], [[ATOMIC_EXIT]] ], [ [[TMP52:%.*]], [[ATOMIC_CONT35]] ], !dbg [[DBG83]]
// LIN64-NEXT:    store i32 [[TMP49]], i32* [[ATOMIC_TEMP36]], align 4, !dbg [[DBG83]]
// LIN64-NEXT:    [[BF_LOAD:%.*]] = load i32, i32* [[ATOMIC_TEMP36]], align 4, !dbg [[DBG83]]
// LIN64-NEXT:    [[BF_VALUE:%.*]] = and i32 [[CONV34]], 2147483647, !dbg [[DBG83]]
// LIN64-NEXT:    [[BF_CLEAR:%.*]] = and i32 [[BF_LOAD]], -2147483648, !dbg [[DBG83]]
// LIN64-NEXT:    [[BF_SET:%.*]] = or i32 [[BF_CLEAR]], [[BF_VALUE]], !dbg [[DBG83]]
// LIN64-NEXT:    store i32 [[BF_SET]], i32* [[ATOMIC_TEMP36]], align 4, !dbg [[DBG83]]
// LIN64-NEXT:    [[TMP50:%.*]] = load i32, i32* [[ATOMIC_TEMP36]], align 4, !dbg [[DBG83]]
// LIN64-NEXT:    [[TMP51:%.*]] = cmpxchg i32* bitcast (i8* getelementptr (i8, i8* bitcast (%struct.BitFields* @bfx to i8*), i64 4) to i32*), i32 [[TMP49]], i32 [[TMP50]] monotonic monotonic, align 4, !dbg [[DBG83]]
// LIN64-NEXT:    [[TMP52]] = extractvalue { i32, i1 } [[TMP51]], 0, !dbg [[DBG83]]
// LIN64-NEXT:    [[TMP53:%.*]] = extractvalue { i32, i1 } [[TMP51]], 1, !dbg [[DBG83]]
// LIN64-NEXT:    br i1 [[TMP53]], label [[ATOMIC_EXIT37:%.*]], label [[ATOMIC_CONT35]], !dbg [[DBG83]]
// LIN64:       atomic_exit37:
// LIN64-NEXT:    [[TMP54:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG84:![0-9]+]]
// LIN64-NEXT:    [[CONV38:%.*]] = fptosi x86_fp80 [[TMP54]] to i32, !dbg [[DBG84]]
// LIN64-NEXT:    [[TMP55:%.*]] = bitcast i32* [[ATOMIC_TEMP39]] to i8*, !dbg [[DBG85:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 4, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i64 4), i8* noundef [[TMP55]], i32 noundef 0), !dbg [[DBG85]]
// LIN64-NEXT:    br label [[ATOMIC_CONT40:%.*]], !dbg [[DBG85]]
// LIN64:       atomic_cont40:
// LIN64-NEXT:    [[TMP56:%.*]] = load i32, i32* [[ATOMIC_TEMP39]], align 1, !dbg [[DBG85]]
// LIN64-NEXT:    store i32 [[TMP56]], i32* [[ATOMIC_TEMP41]], align 1, !dbg [[DBG85]]
// LIN64-NEXT:    [[BF_LOAD42:%.*]] = load i32, i32* [[ATOMIC_TEMP41]], align 1, !dbg [[DBG85]]
// LIN64-NEXT:    [[BF_VALUE43:%.*]] = and i32 [[CONV38]], 2147483647, !dbg [[DBG85]]
// LIN64-NEXT:    [[BF_CLEAR44:%.*]] = and i32 [[BF_LOAD42]], -2147483648, !dbg [[DBG85]]
// LIN64-NEXT:    [[BF_SET45:%.*]] = or i32 [[BF_CLEAR44]], [[BF_VALUE43]], !dbg [[DBG85]]
// LIN64-NEXT:    store i32 [[BF_SET45]], i32* [[ATOMIC_TEMP41]], align 1, !dbg [[DBG85]]
// LIN64-NEXT:    [[TMP57:%.*]] = bitcast i32* [[ATOMIC_TEMP39]] to i8*, !dbg [[DBG85]]
// LIN64-NEXT:    [[TMP58:%.*]] = bitcast i32* [[ATOMIC_TEMP41]] to i8*, !dbg [[DBG85]]
// LIN64-NEXT:    [[CALL46:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 4, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i64 4), i8* noundef [[TMP57]], i8* noundef [[TMP58]], i32 noundef 0, i32 noundef 0), !dbg [[DBG85]]
// LIN64-NEXT:    br i1 [[CALL46]], label [[ATOMIC_EXIT47:%.*]], label [[ATOMIC_CONT40]], !dbg [[DBG85]]
// LIN64:       atomic_exit47:
// LIN64-NEXT:    [[TMP59:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG86:![0-9]+]]
// LIN64-NEXT:    [[CONV48:%.*]] = fptosi x86_fp80 [[TMP59]] to i32, !dbg [[DBG86]]
// LIN64-NEXT:    [[ATOMIC_LOAD49:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS2:%.*]], %struct.BitFields2* @bfx2, i32 0, i32 0) monotonic, align 4, !dbg [[DBG87:![0-9]+]]
// LIN64-NEXT:    br label [[ATOMIC_CONT50:%.*]], !dbg [[DBG87]]
// LIN64:       atomic_cont50:
// LIN64-NEXT:    [[TMP60:%.*]] = phi i32 [ [[ATOMIC_LOAD49]], [[ATOMIC_EXIT47]] ], [ [[TMP63:%.*]], [[ATOMIC_CONT50]] ], !dbg [[DBG87]]
// LIN64-NEXT:    store i32 [[TMP60]], i32* [[ATOMIC_TEMP51]], align 4, !dbg [[DBG87]]
// LIN64-NEXT:    [[BF_LOAD52:%.*]] = load i32, i32* [[ATOMIC_TEMP51]], align 4, !dbg [[DBG87]]
// LIN64-NEXT:    [[BF_VALUE53:%.*]] = and i32 [[CONV48]], 1, !dbg [[DBG87]]
// LIN64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_VALUE53]], 31, !dbg [[DBG87]]
// LIN64-NEXT:    [[BF_CLEAR54:%.*]] = and i32 [[BF_LOAD52]], 2147483647, !dbg [[DBG87]]
// LIN64-NEXT:    [[BF_SET55:%.*]] = or i32 [[BF_CLEAR54]], [[BF_SHL]], !dbg [[DBG87]]
// LIN64-NEXT:    store i32 [[BF_SET55]], i32* [[ATOMIC_TEMP51]], align 4, !dbg [[DBG87]]
// LIN64-NEXT:    [[TMP61:%.*]] = load i32, i32* [[ATOMIC_TEMP51]], align 4, !dbg [[DBG87]]
// LIN64-NEXT:    [[TMP62:%.*]] = cmpxchg i32* getelementptr inbounds ([[STRUCT_BITFIELDS2]], %struct.BitFields2* @bfx2, i32 0, i32 0), i32 [[TMP60]], i32 [[TMP61]] monotonic monotonic, align 4, !dbg [[DBG87]]
// LIN64-NEXT:    [[TMP63]] = extractvalue { i32, i1 } [[TMP62]], 0, !dbg [[DBG87]]
// LIN64-NEXT:    [[TMP64:%.*]] = extractvalue { i32, i1 } [[TMP62]], 1, !dbg [[DBG87]]
// LIN64-NEXT:    br i1 [[TMP64]], label [[ATOMIC_EXIT56:%.*]], label [[ATOMIC_CONT50]], !dbg [[DBG87]]
// LIN64:       atomic_exit56:
// LIN64-NEXT:    [[TMP65:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG88:![0-9]+]]
// LIN64-NEXT:    [[CONV57:%.*]] = fptosi x86_fp80 [[TMP65]] to i32, !dbg [[DBG88]]
// LIN64-NEXT:    [[ATOMIC_LOAD58:%.*]] = load atomic i8, i8* getelementptr (i8, i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*), i64 3) monotonic, align 1, !dbg [[DBG89:![0-9]+]]
// LIN64-NEXT:    br label [[ATOMIC_CONT59:%.*]], !dbg [[DBG89]]
// LIN64:       atomic_cont59:
// LIN64-NEXT:    [[TMP66:%.*]] = phi i8 [ [[ATOMIC_LOAD58]], [[ATOMIC_EXIT56]] ], [ [[TMP71:%.*]], [[ATOMIC_CONT59]] ], !dbg [[DBG89]]
// LIN64-NEXT:    [[TMP67:%.*]] = bitcast i32* [[ATOMIC_TEMP60]] to i8*, !dbg [[DBG89]]
// LIN64-NEXT:    store i8 [[TMP66]], i8* [[TMP67]], align 1, !dbg [[DBG89]]
// LIN64-NEXT:    [[TMP68:%.*]] = trunc i32 [[CONV57]] to i8, !dbg [[DBG89]]
// LIN64-NEXT:    [[BF_LOAD61:%.*]] = load i8, i8* [[TMP67]], align 1, !dbg [[DBG89]]
// LIN64-NEXT:    [[BF_VALUE62:%.*]] = and i8 [[TMP68]], 1, !dbg [[DBG89]]
// LIN64-NEXT:    [[BF_SHL63:%.*]] = shl i8 [[BF_VALUE62]], 7, !dbg [[DBG89]]
// LIN64-NEXT:    [[BF_CLEAR64:%.*]] = and i8 [[BF_LOAD61]], 127, !dbg [[DBG89]]
// LIN64-NEXT:    [[BF_SET65:%.*]] = or i8 [[BF_CLEAR64]], [[BF_SHL63]], !dbg [[DBG89]]
// LIN64-NEXT:    store i8 [[BF_SET65]], i8* [[TMP67]], align 1, !dbg [[DBG89]]
// LIN64-NEXT:    [[TMP69:%.*]] = load i8, i8* [[TMP67]], align 1, !dbg [[DBG89]]
// LIN64-NEXT:    [[TMP70:%.*]] = cmpxchg i8* getelementptr (i8, i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*), i64 3), i8 [[TMP66]], i8 [[TMP69]] monotonic monotonic, align 1, !dbg [[DBG89]]
// LIN64-NEXT:    [[TMP71]] = extractvalue { i8, i1 } [[TMP70]], 0, !dbg [[DBG89]]
// LIN64-NEXT:    [[TMP72:%.*]] = extractvalue { i8, i1 } [[TMP70]], 1, !dbg [[DBG89]]
// LIN64-NEXT:    br i1 [[TMP72]], label [[ATOMIC_EXIT66:%.*]], label [[ATOMIC_CONT59]], !dbg [[DBG89]]
// LIN64:       atomic_exit66:
// LIN64-NEXT:    [[TMP73:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG90:![0-9]+]]
// LIN64-NEXT:    [[CONV67:%.*]] = fptosi x86_fp80 [[TMP73]] to i32, !dbg [[DBG90]]
// LIN64-NEXT:    [[ATOMIC_LOAD68:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS3:%.*]], %struct.BitFields3* @bfx3, i32 0, i32 0) monotonic, align 4, !dbg [[DBG91:![0-9]+]]
// LIN64-NEXT:    br label [[ATOMIC_CONT69:%.*]], !dbg [[DBG91]]
// LIN64:       atomic_cont69:
// LIN64-NEXT:    [[TMP74:%.*]] = phi i32 [ [[ATOMIC_LOAD68]], [[ATOMIC_EXIT66]] ], [ [[TMP77:%.*]], [[ATOMIC_CONT69]] ], !dbg [[DBG91]]
// LIN64-NEXT:    store i32 [[TMP74]], i32* [[ATOMIC_TEMP70]], align 4, !dbg [[DBG91]]
// LIN64-NEXT:    [[BF_LOAD71:%.*]] = load i32, i32* [[ATOMIC_TEMP70]], align 4, !dbg [[DBG91]]
// LIN64-NEXT:    [[BF_VALUE72:%.*]] = and i32 [[CONV67]], 16383, !dbg [[DBG91]]
// LIN64-NEXT:    [[BF_SHL73:%.*]] = shl i32 [[BF_VALUE72]], 11, !dbg [[DBG91]]
// LIN64-NEXT:    [[BF_CLEAR74:%.*]] = and i32 [[BF_LOAD71]], -33552385, !dbg [[DBG91]]
// LIN64-NEXT:    [[BF_SET75:%.*]] = or i32 [[BF_CLEAR74]], [[BF_SHL73]], !dbg [[DBG91]]
// LIN64-NEXT:    store i32 [[BF_SET75]], i32* [[ATOMIC_TEMP70]], align 4, !dbg [[DBG91]]
// LIN64-NEXT:    [[TMP75:%.*]] = load i32, i32* [[ATOMIC_TEMP70]], align 4, !dbg [[DBG91]]
// LIN64-NEXT:    [[TMP76:%.*]] = cmpxchg i32* getelementptr inbounds ([[STRUCT_BITFIELDS3]], %struct.BitFields3* @bfx3, i32 0, i32 0), i32 [[TMP74]], i32 [[TMP75]] monotonic monotonic, align 4, !dbg [[DBG91]]
// LIN64-NEXT:    [[TMP77]] = extractvalue { i32, i1 } [[TMP76]], 0, !dbg [[DBG91]]
// LIN64-NEXT:    [[TMP78:%.*]] = extractvalue { i32, i1 } [[TMP76]], 1, !dbg [[DBG91]]
// LIN64-NEXT:    br i1 [[TMP78]], label [[ATOMIC_EXIT76:%.*]], label [[ATOMIC_CONT69]], !dbg [[DBG91]]
// LIN64:       atomic_exit76:
// LIN64-NEXT:    [[TMP79:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG92:![0-9]+]]
// LIN64-NEXT:    [[CONV77:%.*]] = fptosi x86_fp80 [[TMP79]] to i32, !dbg [[DBG92]]
// LIN64-NEXT:    [[TMP80:%.*]] = bitcast i32* [[ATOMIC_TEMP78]] to i24*, !dbg [[DBG93:![0-9]+]]
// LIN64-NEXT:    [[TMP81:%.*]] = bitcast i24* [[TMP80]] to i8*, !dbg [[DBG93]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 3, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i64 1), i8* noundef [[TMP81]], i32 noundef 0), !dbg [[DBG93]]
// LIN64-NEXT:    br label [[ATOMIC_CONT79:%.*]], !dbg [[DBG93]]
// LIN64:       atomic_cont79:
// LIN64-NEXT:    [[TMP82:%.*]] = bitcast i32* [[ATOMIC_TEMP80]] to i24*, !dbg [[DBG93]]
// LIN64-NEXT:    [[TMP83:%.*]] = load i24, i24* [[TMP80]], align 1, !dbg [[DBG93]]
// LIN64-NEXT:    store i24 [[TMP83]], i24* [[TMP82]], align 1, !dbg [[DBG93]]
// LIN64-NEXT:    [[TMP84:%.*]] = trunc i32 [[CONV77]] to i24, !dbg [[DBG93]]
// LIN64-NEXT:    [[BF_LOAD81:%.*]] = load i24, i24* [[TMP82]], align 1, !dbg [[DBG93]]
// LIN64-NEXT:    [[BF_VALUE82:%.*]] = and i24 [[TMP84]], 16383, !dbg [[DBG93]]
// LIN64-NEXT:    [[BF_SHL83:%.*]] = shl i24 [[BF_VALUE82]], 3, !dbg [[DBG93]]
// LIN64-NEXT:    [[BF_CLEAR84:%.*]] = and i24 [[BF_LOAD81]], -131065, !dbg [[DBG93]]
// LIN64-NEXT:    [[BF_SET85:%.*]] = or i24 [[BF_CLEAR84]], [[BF_SHL83]], !dbg [[DBG93]]
// LIN64-NEXT:    store i24 [[BF_SET85]], i24* [[TMP82]], align 1, !dbg [[DBG93]]
// LIN64-NEXT:    [[TMP85:%.*]] = bitcast i24* [[TMP80]] to i8*, !dbg [[DBG93]]
// LIN64-NEXT:    [[TMP86:%.*]] = bitcast i24* [[TMP82]] to i8*, !dbg [[DBG93]]
// LIN64-NEXT:    [[CALL86:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 3, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i64 1), i8* noundef [[TMP85]], i8* noundef [[TMP86]], i32 noundef 0, i32 noundef 0), !dbg [[DBG93]]
// LIN64-NEXT:    br i1 [[CALL86]], label [[ATOMIC_EXIT87:%.*]], label [[ATOMIC_CONT79]], !dbg [[DBG93]]
// LIN64:       atomic_exit87:
// LIN64-NEXT:    [[TMP87:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG94:![0-9]+]]
// LIN64-NEXT:    [[CONV88:%.*]] = fptosi x86_fp80 [[TMP87]] to i32, !dbg [[DBG94]]
// LIN64-NEXT:    [[ATOMIC_LOAD89:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG95:![0-9]+]]
// LIN64-NEXT:    br label [[ATOMIC_CONT90:%.*]], !dbg [[DBG95]]
// LIN64:       atomic_cont90:
// LIN64-NEXT:    [[TMP88:%.*]] = phi i64 [ [[ATOMIC_LOAD89]], [[ATOMIC_EXIT87]] ], [ [[TMP92:%.*]], [[ATOMIC_CONT90]] ], !dbg [[DBG95]]
// LIN64-NEXT:    store i64 [[TMP88]], i64* [[ATOMIC_TEMP91]], align 8, !dbg [[DBG95]]
// LIN64-NEXT:    [[TMP89:%.*]] = zext i32 [[CONV88]] to i64, !dbg [[DBG95]]
// LIN64-NEXT:    [[BF_LOAD92:%.*]] = load i64, i64* [[ATOMIC_TEMP91]], align 8, !dbg [[DBG95]]
// LIN64-NEXT:    [[BF_VALUE93:%.*]] = and i64 [[TMP89]], 1, !dbg [[DBG95]]
// LIN64-NEXT:    [[BF_SHL94:%.*]] = shl i64 [[BF_VALUE93]], 16, !dbg [[DBG95]]
// LIN64-NEXT:    [[BF_CLEAR95:%.*]] = and i64 [[BF_LOAD92]], -65537, !dbg [[DBG95]]
// LIN64-NEXT:    [[BF_SET96:%.*]] = or i64 [[BF_CLEAR95]], [[BF_SHL94]], !dbg [[DBG95]]
// LIN64-NEXT:    store i64 [[BF_SET96]], i64* [[ATOMIC_TEMP91]], align 8, !dbg [[DBG95]]
// LIN64-NEXT:    [[TMP90:%.*]] = load i64, i64* [[ATOMIC_TEMP91]], align 8, !dbg [[DBG95]]
// LIN64-NEXT:    [[TMP91:%.*]] = cmpxchg i64* bitcast (%struct.BitFields4* @bfx4 to i64*), i64 [[TMP88]], i64 [[TMP90]] monotonic monotonic, align 8, !dbg [[DBG95]]
// LIN64-NEXT:    [[TMP92]] = extractvalue { i64, i1 } [[TMP91]], 0, !dbg [[DBG95]]
// LIN64-NEXT:    [[TMP93:%.*]] = extractvalue { i64, i1 } [[TMP91]], 1, !dbg [[DBG95]]
// LIN64-NEXT:    br i1 [[TMP93]], label [[ATOMIC_EXIT97:%.*]], label [[ATOMIC_CONT90]], !dbg [[DBG95]]
// LIN64:       atomic_exit97:
// LIN64-NEXT:    [[TMP94:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG96:![0-9]+]]
// LIN64-NEXT:    [[CONV98:%.*]] = fptosi x86_fp80 [[TMP94]] to i32, !dbg [[DBG96]]
// LIN64-NEXT:    [[ATOMIC_LOAD99:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) monotonic, align 1, !dbg [[DBG97:![0-9]+]]
// LIN64-NEXT:    br label [[ATOMIC_CONT100:%.*]], !dbg [[DBG97]]
// LIN64:       atomic_cont100:
// LIN64-NEXT:    [[TMP95:%.*]] = phi i8 [ [[ATOMIC_LOAD99]], [[ATOMIC_EXIT97]] ], [ [[TMP100:%.*]], [[ATOMIC_CONT100]] ], !dbg [[DBG97]]
// LIN64-NEXT:    [[TMP96:%.*]] = bitcast i32* [[ATOMIC_TEMP101]] to i8*, !dbg [[DBG97]]
// LIN64-NEXT:    store i8 [[TMP95]], i8* [[TMP96]], align 1, !dbg [[DBG97]]
// LIN64-NEXT:    [[TMP97:%.*]] = trunc i32 [[CONV98]] to i8, !dbg [[DBG97]]
// LIN64-NEXT:    [[BF_LOAD102:%.*]] = load i8, i8* [[TMP96]], align 1, !dbg [[DBG97]]
// LIN64-NEXT:    [[BF_VALUE103:%.*]] = and i8 [[TMP97]], 1, !dbg [[DBG97]]
// LIN64-NEXT:    [[BF_CLEAR104:%.*]] = and i8 [[BF_LOAD102]], -2, !dbg [[DBG97]]
// LIN64-NEXT:    [[BF_SET105:%.*]] = or i8 [[BF_CLEAR104]], [[BF_VALUE103]], !dbg [[DBG97]]
// LIN64-NEXT:    store i8 [[BF_SET105]], i8* [[TMP96]], align 1, !dbg [[DBG97]]
// LIN64-NEXT:    [[TMP98:%.*]] = load i8, i8* [[TMP96]], align 1, !dbg [[DBG97]]
// LIN64-NEXT:    [[TMP99:%.*]] = cmpxchg i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2), i8 [[TMP95]], i8 [[TMP98]] monotonic monotonic, align 1, !dbg [[DBG97]]
// LIN64-NEXT:    [[TMP100]] = extractvalue { i8, i1 } [[TMP99]], 0, !dbg [[DBG97]]
// LIN64-NEXT:    [[TMP101:%.*]] = extractvalue { i8, i1 } [[TMP99]], 1, !dbg [[DBG97]]
// LIN64-NEXT:    br i1 [[TMP101]], label [[ATOMIC_EXIT106:%.*]], label [[ATOMIC_CONT100]], !dbg [[DBG97]]
// LIN64:       atomic_exit106:
// LIN64-NEXT:    [[TMP102:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG98:![0-9]+]]
// LIN64-NEXT:    [[CONV107:%.*]] = fptosi x86_fp80 [[TMP102]] to i64, !dbg [[DBG98]]
// LIN64-NEXT:    [[ATOMIC_LOAD108:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG99:![0-9]+]]
// LIN64-NEXT:    br label [[ATOMIC_CONT109:%.*]], !dbg [[DBG99]]
// LIN64:       atomic_cont109:
// LIN64-NEXT:    [[TMP103:%.*]] = phi i64 [ [[ATOMIC_LOAD108]], [[ATOMIC_EXIT106]] ], [ [[TMP106:%.*]], [[ATOMIC_CONT109]] ], !dbg [[DBG99]]
// LIN64-NEXT:    store i64 [[TMP103]], i64* [[ATOMIC_TEMP110]], align 8, !dbg [[DBG99]]
// LIN64-NEXT:    [[BF_LOAD111:%.*]] = load i64, i64* [[ATOMIC_TEMP110]], align 8, !dbg [[DBG99]]
// LIN64-NEXT:    [[BF_VALUE112:%.*]] = and i64 [[CONV107]], 127, !dbg [[DBG99]]
// LIN64-NEXT:    [[BF_SHL113:%.*]] = shl i64 [[BF_VALUE112]], 17, !dbg [[DBG99]]
// LIN64-NEXT:    [[BF_CLEAR114:%.*]] = and i64 [[BF_LOAD111]], -16646145, !dbg [[DBG99]]
// LIN64-NEXT:    [[BF_SET115:%.*]] = or i64 [[BF_CLEAR114]], [[BF_SHL113]], !dbg [[DBG99]]
// LIN64-NEXT:    store i64 [[BF_SET115]], i64* [[ATOMIC_TEMP110]], align 8, !dbg [[DBG99]]
// LIN64-NEXT:    [[TMP104:%.*]] = load i64, i64* [[ATOMIC_TEMP110]], align 8, !dbg [[DBG99]]
// LIN64-NEXT:    [[TMP105:%.*]] = cmpxchg i64* bitcast (%struct.BitFields4* @bfx4 to i64*), i64 [[TMP103]], i64 [[TMP104]] monotonic monotonic, align 8, !dbg [[DBG99]]
// LIN64-NEXT:    [[TMP106]] = extractvalue { i64, i1 } [[TMP105]], 0, !dbg [[DBG99]]
// LIN64-NEXT:    [[TMP107:%.*]] = extractvalue { i64, i1 } [[TMP105]], 1, !dbg [[DBG99]]
// LIN64-NEXT:    br i1 [[TMP107]], label [[ATOMIC_EXIT116:%.*]], label [[ATOMIC_CONT109]], !dbg [[DBG99]]
// LIN64:       atomic_exit116:
// LIN64-NEXT:    [[TMP108:%.*]] = load x86_fp80, x86_fp80* @ldv, align 16, !dbg [[DBG100:![0-9]+]]
// LIN64-NEXT:    [[CONV117:%.*]] = fptosi x86_fp80 [[TMP108]] to i64, !dbg [[DBG100]]
// LIN64-NEXT:    [[ATOMIC_LOAD118:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) monotonic, align 1, !dbg [[DBG101:![0-9]+]]
// LIN64-NEXT:    br label [[ATOMIC_CONT119:%.*]], !dbg [[DBG101]]
// LIN64:       atomic_cont119:
// LIN64-NEXT:    [[TMP109:%.*]] = phi i8 [ [[ATOMIC_LOAD118]], [[ATOMIC_EXIT116]] ], [ [[TMP114:%.*]], [[ATOMIC_CONT119]] ], !dbg [[DBG101]]
// LIN64-NEXT:    [[TMP110:%.*]] = bitcast i64* [[ATOMIC_TEMP120]] to i8*, !dbg [[DBG101]]
// LIN64-NEXT:    store i8 [[TMP109]], i8* [[TMP110]], align 1, !dbg [[DBG101]]
// LIN64-NEXT:    [[TMP111:%.*]] = trunc i64 [[CONV117]] to i8, !dbg [[DBG101]]
// LIN64-NEXT:    [[BF_LOAD121:%.*]] = load i8, i8* [[TMP110]], align 1, !dbg [[DBG101]]
// LIN64-NEXT:    [[BF_VALUE122:%.*]] = and i8 [[TMP111]], 127, !dbg [[DBG101]]
// LIN64-NEXT:    [[BF_SHL123:%.*]] = shl i8 [[BF_VALUE122]], 1, !dbg [[DBG101]]
// LIN64-NEXT:    [[BF_CLEAR124:%.*]] = and i8 [[BF_LOAD121]], 1, !dbg [[DBG101]]
// LIN64-NEXT:    [[BF_SET125:%.*]] = or i8 [[BF_CLEAR124]], [[BF_SHL123]], !dbg [[DBG101]]
// LIN64-NEXT:    store i8 [[BF_SET125]], i8* [[TMP110]], align 1, !dbg [[DBG101]]
// LIN64-NEXT:    [[TMP112:%.*]] = load i8, i8* [[TMP110]], align 1, !dbg [[DBG101]]
// LIN64-NEXT:    [[TMP113:%.*]] = cmpxchg i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2), i8 [[TMP109]], i8 [[TMP112]] monotonic monotonic, align 1, !dbg [[DBG101]]
// LIN64-NEXT:    [[TMP114]] = extractvalue { i8, i1 } [[TMP113]], 0, !dbg [[DBG101]]
// LIN64-NEXT:    [[TMP115:%.*]] = extractvalue { i8, i1 } [[TMP113]], 1, !dbg [[DBG101]]
// LIN64-NEXT:    br i1 [[TMP115]], label [[ATOMIC_EXIT126:%.*]], label [[ATOMIC_CONT119]], !dbg [[DBG101]]
// LIN64:       atomic_exit126:
// LIN64-NEXT:    [[TMP116:%.*]] = load i64, i64* @ulv, align 8, !dbg [[DBG102:![0-9]+]]
// LIN64-NEXT:    [[CONV127:%.*]] = uitofp i64 [[TMP116]] to float, !dbg [[DBG102]]
// LIN64-NEXT:    [[ATOMIC_LOAD128:%.*]] = load atomic i64, i64* bitcast (<2 x float>* @float2x to i64*) monotonic, align 8, !dbg [[DBG103:![0-9]+]]
// LIN64-NEXT:    br label [[ATOMIC_CONT129:%.*]], !dbg [[DBG103]]
// LIN64:       atomic_cont129:
// LIN64-NEXT:    [[TMP117:%.*]] = phi i64 [ [[ATOMIC_LOAD128]], [[ATOMIC_EXIT126]] ], [ [[TMP123:%.*]], [[ATOMIC_CONT129]] ], !dbg [[DBG103]]
// LIN64-NEXT:    [[TMP118:%.*]] = bitcast <2 x float>* [[ATOMIC_TEMP130]] to i64*, !dbg [[DBG103]]
// LIN64-NEXT:    store i64 [[TMP117]], i64* [[TMP118]], align 8, !dbg [[DBG103]]
// LIN64-NEXT:    [[TMP119:%.*]] = load <2 x float>, <2 x float>* [[ATOMIC_TEMP130]], align 8, !dbg [[DBG103]]
// LIN64-NEXT:    [[TMP120:%.*]] = insertelement <2 x float> [[TMP119]], float [[CONV127]], i64 0, !dbg [[DBG103]]
// LIN64-NEXT:    store <2 x float> [[TMP120]], <2 x float>* [[ATOMIC_TEMP130]], align 8, !dbg [[DBG103]]
// LIN64-NEXT:    [[TMP121:%.*]] = load i64, i64* [[TMP118]], align 8, !dbg [[DBG103]]
// LIN64-NEXT:    [[TMP122:%.*]] = cmpxchg i64* bitcast (<2 x float>* @float2x to i64*), i64 [[TMP117]], i64 [[TMP121]] monotonic monotonic, align 8, !dbg [[DBG103]]
// LIN64-NEXT:    [[TMP123]] = extractvalue { i64, i1 } [[TMP122]], 0, !dbg [[DBG103]]
// LIN64-NEXT:    [[TMP124:%.*]] = extractvalue { i64, i1 } [[TMP122]], 1, !dbg [[DBG103]]
// LIN64-NEXT:    br i1 [[TMP124]], label [[ATOMIC_EXIT131:%.*]], label [[ATOMIC_CONT129]], !dbg [[DBG103]]
// LIN64:       atomic_exit131:
// LIN64-NEXT:    [[TMP125:%.*]] = call i32 @llvm.read_register.i32(metadata [[META2:![0-9]+]]), !dbg [[DBG104:![0-9]+]]
// LIN64-NEXT:    [[CONV132:%.*]] = sitofp i32 [[TMP125]] to double, !dbg [[DBG104]]
// LIN64-NEXT:    [[TMP126:%.*]] = bitcast double [[CONV132]] to i64, !dbg [[DBG105:![0-9]+]]
// LIN64-NEXT:    store atomic i64 [[TMP126]], i64* bitcast (double* @dv to i64*) seq_cst, align 8, !dbg [[DBG105]]
// LIN64-NEXT:    fence release
// LIN64-NEXT:    ret i32 0, !dbg [[DBG106:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@main
// PPC64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca ppc_fp128, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP1:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP2:%.*]] = alloca { float, float }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP3:%.*]] = alloca { double, double }, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP10:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca ppc_fp128, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP16:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP32:%.*]] = alloca <4 x i32>, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP33:%.*]] = alloca <4 x i32>, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP36:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP39:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP41:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP52:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP61:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP70:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP78:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP80:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP91:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP101:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP111:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP121:%.*]] = alloca i64, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP130:%.*]] = alloca <2 x float>, align 8
// PPC64-NEXT:    store i32 0, i32* [[RETVAL]], align 4
// PPC64-NEXT:    store atomic i32 1, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1) monotonic, align 4, !dbg [[DBG9:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = load i8, i8* @bv, align 1, !dbg [[DBG10:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[TMP0]] to i1, !dbg [[DBG10]]
// PPC64-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG11:![0-9]+]]
// PPC64-NEXT:    store atomic i8 [[FROMBOOL]], i8* @bx monotonic, align 1, !dbg [[DBG11]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i8, i8* @cv, align 1, !dbg [[DBG12:![0-9]+]]
// PPC64-NEXT:    store atomic i8 [[TMP1]], i8* @cx release, align 1, !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    fence release
// PPC64-NEXT:    [[TMP2:%.*]] = load i8, i8* @ucv, align 1, !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    store atomic i8 [[TMP2]], i8* @ucx monotonic, align 1, !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = load i16, i16* @sv, align 2, !dbg [[DBG16:![0-9]+]]
// PPC64-NEXT:    store atomic i16 [[TMP3]], i16* @sx monotonic, align 2, !dbg [[DBG17:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = load i16, i16* @usv, align 2, !dbg [[DBG18:![0-9]+]]
// PPC64-NEXT:    store atomic i16 [[TMP4]], i16* @usx monotonic, align 2, !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    [[TMP5:%.*]] = load i32, i32* @iv, align 4, !dbg [[DBG20:![0-9]+]]
// PPC64-NEXT:    store atomic i32 [[TMP5]], i32* @ix monotonic, align 4, !dbg [[DBG21:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = load i32, i32* @uiv, align 4, !dbg [[DBG22:![0-9]+]]
// PPC64-NEXT:    store atomic i32 [[TMP6]], i32* @uix monotonic, align 4, !dbg [[DBG23:![0-9]+]]
// PPC64-NEXT:    [[TMP7:%.*]] = load i64, i64* @lv, align 8, !dbg [[DBG24:![0-9]+]]
// PPC64-NEXT:    store atomic i64 [[TMP7]], i64* @lx monotonic, align 8, !dbg [[DBG25:![0-9]+]]
// PPC64-NEXT:    [[TMP8:%.*]] = load i64, i64* @ulv, align 8, !dbg [[DBG26:![0-9]+]]
// PPC64-NEXT:    store atomic i64 [[TMP8]], i64* @ulx monotonic, align 8, !dbg [[DBG27:![0-9]+]]
// PPC64-NEXT:    [[TMP9:%.*]] = load i64, i64* @llv, align 8, !dbg [[DBG28:![0-9]+]]
// PPC64-NEXT:    store atomic i64 [[TMP9]], i64* @llx monotonic, align 8, !dbg [[DBG29:![0-9]+]]
// PPC64-NEXT:    [[TMP10:%.*]] = load i64, i64* @ullv, align 8, !dbg [[DBG30:![0-9]+]]
// PPC64-NEXT:    store atomic i64 [[TMP10]], i64* @ullx monotonic, align 8, !dbg [[DBG31:![0-9]+]]
// PPC64-NEXT:    [[TMP11:%.*]] = load float, float* @fv, align 4, !dbg [[DBG32:![0-9]+]]
// PPC64-NEXT:    [[TMP12:%.*]] = bitcast float [[TMP11]] to i32, !dbg [[DBG33:![0-9]+]]
// PPC64-NEXT:    store atomic i32 [[TMP12]], i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG33]]
// PPC64-NEXT:    [[TMP13:%.*]] = load double, double* @dv, align 8, !dbg [[DBG34:![0-9]+]]
// PPC64-NEXT:    [[TMP14:%.*]] = bitcast double [[TMP13]] to i64, !dbg [[DBG35:![0-9]+]]
// PPC64-NEXT:    store atomic i64 [[TMP14]], i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG35]]
// PPC64-NEXT:    [[TMP15:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG36:![0-9]+]]
// PPC64-NEXT:    store ppc_fp128 [[TMP15]], ppc_fp128* [[ATOMIC_TEMP]], align 16, !dbg [[DBG37:![0-9]+]]
// PPC64-NEXT:    [[TMP16:%.*]] = bitcast ppc_fp128* [[ATOMIC_TEMP]] to i8*, !dbg [[DBG37]]
// PPC64-NEXT:    call void @__atomic_store(i64 noundef 16, i8* noundef bitcast (ppc_fp128* @ldx to i8*), i8* noundef [[TMP16]], i32 noundef signext 0), !dbg [[DBG37]]
// PPC64-NEXT:    [[CIV_REAL:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG38:![0-9]+]]
// PPC64-NEXT:    [[CIV_IMAG:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_TEMP1_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP1]], i32 0, i32 0, !dbg [[DBG39:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP1_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP1]], i32 0, i32 1, !dbg [[DBG39]]
// PPC64-NEXT:    store i32 [[CIV_REAL]], i32* [[ATOMIC_TEMP1_REALP]], align 4, !dbg [[DBG39]]
// PPC64-NEXT:    store i32 [[CIV_IMAG]], i32* [[ATOMIC_TEMP1_IMAGP]], align 4, !dbg [[DBG39]]
// PPC64-NEXT:    [[TMP17:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP1]] to i8*, !dbg [[DBG39]]
// PPC64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP17]], i32 noundef signext 0), !dbg [[DBG39]]
// PPC64-NEXT:    [[CFV_REAL:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG40:![0-9]+]]
// PPC64-NEXT:    [[CFV_IMAG:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG40]]
// PPC64-NEXT:    [[ATOMIC_TEMP2_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP2]], i32 0, i32 0, !dbg [[DBG41:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP2_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP2]], i32 0, i32 1, !dbg [[DBG41]]
// PPC64-NEXT:    store float [[CFV_REAL]], float* [[ATOMIC_TEMP2_REALP]], align 4, !dbg [[DBG41]]
// PPC64-NEXT:    store float [[CFV_IMAG]], float* [[ATOMIC_TEMP2_IMAGP]], align 4, !dbg [[DBG41]]
// PPC64-NEXT:    [[TMP18:%.*]] = bitcast { float, float }* [[ATOMIC_TEMP2]] to i8*, !dbg [[DBG41]]
// PPC64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ float, float }* @cfx to i8*), i8* noundef [[TMP18]], i32 noundef signext 0), !dbg [[DBG41]]
// PPC64-NEXT:    [[CDV_REAL:%.*]] = load double, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG42:![0-9]+]]
// PPC64-NEXT:    [[CDV_IMAG:%.*]] = load double, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG42]]
// PPC64-NEXT:    [[ATOMIC_TEMP3_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP3]], i32 0, i32 0, !dbg [[DBG43:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP3_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP3]], i32 0, i32 1, !dbg [[DBG43]]
// PPC64-NEXT:    store double [[CDV_REAL]], double* [[ATOMIC_TEMP3_REALP]], align 8, !dbg [[DBG43]]
// PPC64-NEXT:    store double [[CDV_IMAG]], double* [[ATOMIC_TEMP3_IMAGP]], align 8, !dbg [[DBG43]]
// PPC64-NEXT:    [[TMP19:%.*]] = bitcast { double, double }* [[ATOMIC_TEMP3]] to i8*, !dbg [[DBG43]]
// PPC64-NEXT:    call void @__atomic_store(i64 noundef 16, i8* noundef bitcast ({ double, double }* @cdx to i8*), i8* noundef [[TMP19]], i32 noundef signext 5), !dbg [[DBG43]]
// PPC64-NEXT:    fence release
// PPC64-NEXT:    [[TMP20:%.*]] = load i8, i8* @bv, align 1, !dbg [[DBG44:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL4:%.*]] = trunc i8 [[TMP20]] to i1, !dbg [[DBG44]]
// PPC64-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL4]] to i64, !dbg [[DBG44]]
// PPC64-NEXT:    store atomic i64 [[CONV]], i64* @ulx monotonic, align 8, !dbg [[DBG45:![0-9]+]]
// PPC64-NEXT:    [[TMP21:%.*]] = load i8, i8* @cv, align 1, !dbg [[DBG46:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL5:%.*]] = icmp ne i8 [[TMP21]], 0, !dbg [[DBG46]]
// PPC64-NEXT:    [[FROMBOOL6:%.*]] = zext i1 [[TOBOOL5]] to i8, !dbg [[DBG47:![0-9]+]]
// PPC64-NEXT:    store atomic i8 [[FROMBOOL6]], i8* @bx monotonic, align 1, !dbg [[DBG47]]
// PPC64-NEXT:    [[TMP22:%.*]] = load i8, i8* @ucv, align 1, !dbg [[DBG48:![0-9]+]]
// PPC64-NEXT:    store atomic i8 [[TMP22]], i8* @cx seq_cst, align 1, !dbg [[DBG49:![0-9]+]]
// PPC64-NEXT:    fence release
// PPC64-NEXT:    [[TMP23:%.*]] = load i16, i16* @sv, align 2, !dbg [[DBG50:![0-9]+]]
// PPC64-NEXT:    [[CONV7:%.*]] = sext i16 [[TMP23]] to i64, !dbg [[DBG50]]
// PPC64-NEXT:    store atomic i64 [[CONV7]], i64* @ulx monotonic, align 8, !dbg [[DBG51:![0-9]+]]
// PPC64-NEXT:    [[TMP24:%.*]] = load i16, i16* @usv, align 2, !dbg [[DBG52:![0-9]+]]
// PPC64-NEXT:    [[CONV8:%.*]] = zext i16 [[TMP24]] to i64, !dbg [[DBG52]]
// PPC64-NEXT:    store atomic i64 [[CONV8]], i64* @lx monotonic, align 8, !dbg [[DBG53:![0-9]+]]
// PPC64-NEXT:    [[TMP25:%.*]] = load i32, i32* @iv, align 4, !dbg [[DBG54:![0-9]+]]
// PPC64-NEXT:    store atomic i32 [[TMP25]], i32* @uix seq_cst, align 4, !dbg [[DBG55:![0-9]+]]
// PPC64-NEXT:    fence release
// PPC64-NEXT:    [[TMP26:%.*]] = load i32, i32* @uiv, align 4, !dbg [[DBG56:![0-9]+]]
// PPC64-NEXT:    store atomic i32 [[TMP26]], i32* @ix monotonic, align 4, !dbg [[DBG57:![0-9]+]]
// PPC64-NEXT:    [[TMP27:%.*]] = load i64, i64* @lv, align 8, !dbg [[DBG58:![0-9]+]]
// PPC64-NEXT:    [[CONV9:%.*]] = trunc i64 [[TMP27]] to i32, !dbg [[DBG58]]
// PPC64-NEXT:    [[ATOMIC_TEMP10_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP10]], i32 0, i32 0, !dbg [[DBG59:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP10_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP10]], i32 0, i32 1, !dbg [[DBG59]]
// PPC64-NEXT:    store i32 [[CONV9]], i32* [[ATOMIC_TEMP10_REALP]], align 4, !dbg [[DBG59]]
// PPC64-NEXT:    store i32 0, i32* [[ATOMIC_TEMP10_IMAGP]], align 4, !dbg [[DBG59]]
// PPC64-NEXT:    [[TMP28:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP10]] to i8*, !dbg [[DBG59]]
// PPC64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP28]], i32 noundef signext 0), !dbg [[DBG59]]
// PPC64-NEXT:    [[TMP29:%.*]] = load i64, i64* @ulv, align 8, !dbg [[DBG60:![0-9]+]]
// PPC64-NEXT:    [[CONV11:%.*]] = uitofp i64 [[TMP29]] to float, !dbg [[DBG60]]
// PPC64-NEXT:    [[TMP30:%.*]] = bitcast float [[CONV11]] to i32, !dbg [[DBG61:![0-9]+]]
// PPC64-NEXT:    store atomic i32 [[TMP30]], i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG61]]
// PPC64-NEXT:    [[TMP31:%.*]] = load i64, i64* @llv, align 8, !dbg [[DBG62:![0-9]+]]
// PPC64-NEXT:    [[CONV12:%.*]] = sitofp i64 [[TMP31]] to double, !dbg [[DBG62]]
// PPC64-NEXT:    [[TMP32:%.*]] = bitcast double [[CONV12]] to i64, !dbg [[DBG63:![0-9]+]]
// PPC64-NEXT:    store atomic i64 [[TMP32]], i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG63]]
// PPC64-NEXT:    [[TMP33:%.*]] = load i64, i64* @ullv, align 8, !dbg [[DBG64:![0-9]+]]
// PPC64-NEXT:    [[CONV13:%.*]] = uitofp i64 [[TMP33]] to ppc_fp128, !dbg [[DBG64]]
// PPC64-NEXT:    store ppc_fp128 [[CONV13]], ppc_fp128* [[ATOMIC_TEMP14]], align 16, !dbg [[DBG65:![0-9]+]]
// PPC64-NEXT:    [[TMP34:%.*]] = bitcast ppc_fp128* [[ATOMIC_TEMP14]] to i8*, !dbg [[DBG65]]
// PPC64-NEXT:    call void @__atomic_store(i64 noundef 16, i8* noundef bitcast (ppc_fp128* @ldx to i8*), i8* noundef [[TMP34]], i32 noundef signext 0), !dbg [[DBG65]]
// PPC64-NEXT:    [[TMP35:%.*]] = load float, float* @fv, align 4, !dbg [[DBG66:![0-9]+]]
// PPC64-NEXT:    [[CONV15:%.*]] = fptosi float [[TMP35]] to i32, !dbg [[DBG66]]
// PPC64-NEXT:    [[ATOMIC_TEMP16_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP16]], i32 0, i32 0, !dbg [[DBG67:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP16_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP16]], i32 0, i32 1, !dbg [[DBG67]]
// PPC64-NEXT:    store i32 [[CONV15]], i32* [[ATOMIC_TEMP16_REALP]], align 4, !dbg [[DBG67]]
// PPC64-NEXT:    store i32 0, i32* [[ATOMIC_TEMP16_IMAGP]], align 4, !dbg [[DBG67]]
// PPC64-NEXT:    [[TMP36:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP16]] to i8*, !dbg [[DBG67]]
// PPC64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP36]], i32 noundef signext 0), !dbg [[DBG67]]
// PPC64-NEXT:    [[TMP37:%.*]] = load double, double* @dv, align 8, !dbg [[DBG68:![0-9]+]]
// PPC64-NEXT:    [[CONV17:%.*]] = fptosi double [[TMP37]] to i16, !dbg [[DBG68]]
// PPC64-NEXT:    store atomic i16 [[CONV17]], i16* @sx monotonic, align 2, !dbg [[DBG69:![0-9]+]]
// PPC64-NEXT:    [[TMP38:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG70:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL18:%.*]] = fcmp une ppc_fp128 [[TMP38]], 0xM00000000000000000000000000000000, !dbg [[DBG70]]
// PPC64-NEXT:    [[FROMBOOL19:%.*]] = zext i1 [[TOBOOL18]] to i8, !dbg [[DBG71:![0-9]+]]
// PPC64-NEXT:    store atomic i8 [[FROMBOOL19]], i8* @bx monotonic, align 1, !dbg [[DBG71]]
// PPC64-NEXT:    [[CIV_REAL20:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG72:![0-9]+]]
// PPC64-NEXT:    [[CIV_IMAG21:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG72]]
// PPC64-NEXT:    [[TOBOOL22:%.*]] = icmp ne i32 [[CIV_REAL20]], 0, !dbg [[DBG72]]
// PPC64-NEXT:    [[TOBOOL23:%.*]] = icmp ne i32 [[CIV_IMAG21]], 0, !dbg [[DBG72]]
// PPC64-NEXT:    [[TOBOOL24:%.*]] = or i1 [[TOBOOL22]], [[TOBOOL23]], !dbg [[DBG72]]
// PPC64-NEXT:    [[FROMBOOL25:%.*]] = zext i1 [[TOBOOL24]] to i8, !dbg [[DBG73:![0-9]+]]
// PPC64-NEXT:    store atomic i8 [[FROMBOOL25]], i8* @bx monotonic, align 1, !dbg [[DBG73]]
// PPC64-NEXT:    [[CFV_REAL26:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG74:![0-9]+]]
// PPC64-NEXT:    [[CONV27:%.*]] = fptoui float [[CFV_REAL26]] to i16, !dbg [[DBG74]]
// PPC64-NEXT:    store atomic i16 [[CONV27]], i16* @usx monotonic, align 2, !dbg [[DBG75:![0-9]+]]
// PPC64-NEXT:    [[CDV_REAL28:%.*]] = load double, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG76:![0-9]+]]
// PPC64-NEXT:    [[CONV29:%.*]] = fptosi double [[CDV_REAL28]] to i64, !dbg [[DBG76]]
// PPC64-NEXT:    store atomic i64 [[CONV29]], i64* @llx monotonic, align 8, !dbg [[DBG77:![0-9]+]]
// PPC64-NEXT:    [[TMP39:%.*]] = load i16, i16* @sv, align 2, !dbg [[DBG78:![0-9]+]]
// PPC64-NEXT:    [[TMP40:%.*]] = load i8, i8* @bv, align 1, !dbg [[DBG79:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL30:%.*]] = trunc i8 [[TMP40]] to i1, !dbg [[DBG79]]
// PPC64-NEXT:    [[CONV31:%.*]] = zext i1 [[TOBOOL30]] to i32, !dbg [[DBG79]]
// PPC64-NEXT:    [[TMP41:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP32]] to i8*, !dbg [[DBG80:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (<4 x i32>* @int4x to i8*), i8* noundef [[TMP41]], i32 noundef signext 0), !dbg [[DBG80]]
// PPC64-NEXT:    br label [[ATOMIC_CONT:%.*]], !dbg [[DBG80]]
// PPC64:       atomic_cont:
// PPC64-NEXT:    [[TMP42:%.*]] = load <4 x i32>, <4 x i32>* [[ATOMIC_TEMP32]], align 16, !dbg [[DBG80]]
// PPC64-NEXT:    store <4 x i32> [[TMP42]], <4 x i32>* [[ATOMIC_TEMP33]], align 16, !dbg [[DBG80]]
// PPC64-NEXT:    [[TMP43:%.*]] = load <4 x i32>, <4 x i32>* [[ATOMIC_TEMP33]], align 16, !dbg [[DBG80]]
// PPC64-NEXT:    [[VECINS:%.*]] = insertelement <4 x i32> [[TMP43]], i32 [[CONV31]], i16 [[TMP39]], !dbg [[DBG80]]
// PPC64-NEXT:    store <4 x i32> [[VECINS]], <4 x i32>* [[ATOMIC_TEMP33]], align 16, !dbg [[DBG80]]
// PPC64-NEXT:    [[TMP44:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP32]] to i8*, !dbg [[DBG80]]
// PPC64-NEXT:    [[TMP45:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP33]] to i8*, !dbg [[DBG80]]
// PPC64-NEXT:    [[CALL:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 16, i8* noundef bitcast (<4 x i32>* @int4x to i8*), i8* noundef [[TMP44]], i8* noundef [[TMP45]], i32 noundef signext 0, i32 noundef signext 0), !dbg [[DBG80]]
// PPC64-NEXT:    br i1 [[CALL]], label [[ATOMIC_EXIT:%.*]], label [[ATOMIC_CONT]], !dbg [[DBG80]]
// PPC64:       atomic_exit:
// PPC64-NEXT:    [[TMP46:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG81:![0-9]+]]
// PPC64-NEXT:    [[CONV34:%.*]] = fptosi ppc_fp128 [[TMP46]] to i32, !dbg [[DBG81]]
// PPC64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i32, i32* bitcast (%struct.BitFields* @bfx to i32*) monotonic, align 4, !dbg [[DBG82:![0-9]+]]
// PPC64-NEXT:    br label [[ATOMIC_CONT35:%.*]], !dbg [[DBG82]]
// PPC64:       atomic_cont35:
// PPC64-NEXT:    [[TMP47:%.*]] = phi i32 [ [[ATOMIC_LOAD]], [[ATOMIC_EXIT]] ], [ [[TMP50:%.*]], [[ATOMIC_CONT35]] ], !dbg [[DBG82]]
// PPC64-NEXT:    store i32 [[TMP47]], i32* [[ATOMIC_TEMP36]], align 4, !dbg [[DBG82]]
// PPC64-NEXT:    [[BF_LOAD:%.*]] = load i32, i32* [[ATOMIC_TEMP36]], align 4, !dbg [[DBG82]]
// PPC64-NEXT:    [[BF_VALUE:%.*]] = and i32 [[CONV34]], 2147483647, !dbg [[DBG82]]
// PPC64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_VALUE]], 1, !dbg [[DBG82]]
// PPC64-NEXT:    [[BF_CLEAR:%.*]] = and i32 [[BF_LOAD]], 1, !dbg [[DBG82]]
// PPC64-NEXT:    [[BF_SET:%.*]] = or i32 [[BF_CLEAR]], [[BF_SHL]], !dbg [[DBG82]]
// PPC64-NEXT:    store i32 [[BF_SET]], i32* [[ATOMIC_TEMP36]], align 4, !dbg [[DBG82]]
// PPC64-NEXT:    [[TMP48:%.*]] = load i32, i32* [[ATOMIC_TEMP36]], align 4, !dbg [[DBG82]]
// PPC64-NEXT:    [[TMP49:%.*]] = cmpxchg i32* bitcast (%struct.BitFields* @bfx to i32*), i32 [[TMP47]], i32 [[TMP48]] monotonic monotonic, align 4, !dbg [[DBG82]]
// PPC64-NEXT:    [[TMP50]] = extractvalue { i32, i1 } [[TMP49]], 0, !dbg [[DBG82]]
// PPC64-NEXT:    [[TMP51:%.*]] = extractvalue { i32, i1 } [[TMP49]], 1, !dbg [[DBG82]]
// PPC64-NEXT:    br i1 [[TMP51]], label [[ATOMIC_EXIT37:%.*]], label [[ATOMIC_CONT35]], !dbg [[DBG82]]
// PPC64:       atomic_exit37:
// PPC64-NEXT:    [[TMP52:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG83:![0-9]+]]
// PPC64-NEXT:    [[CONV38:%.*]] = fptosi ppc_fp128 [[TMP52]] to i32, !dbg [[DBG83]]
// PPC64-NEXT:    [[TMP53:%.*]] = bitcast i32* [[ATOMIC_TEMP39]] to i8*, !dbg [[DBG84:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 4, i8* noundef bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i8* noundef [[TMP53]], i32 noundef signext 0), !dbg [[DBG84]]
// PPC64-NEXT:    br label [[ATOMIC_CONT40:%.*]], !dbg [[DBG84]]
// PPC64:       atomic_cont40:
// PPC64-NEXT:    [[TMP54:%.*]] = load i32, i32* [[ATOMIC_TEMP39]], align 1, !dbg [[DBG84]]
// PPC64-NEXT:    store i32 [[TMP54]], i32* [[ATOMIC_TEMP41]], align 1, !dbg [[DBG84]]
// PPC64-NEXT:    [[BF_LOAD42:%.*]] = load i32, i32* [[ATOMIC_TEMP41]], align 1, !dbg [[DBG84]]
// PPC64-NEXT:    [[BF_VALUE43:%.*]] = and i32 [[CONV38]], 2147483647, !dbg [[DBG84]]
// PPC64-NEXT:    [[BF_SHL44:%.*]] = shl i32 [[BF_VALUE43]], 1, !dbg [[DBG84]]
// PPC64-NEXT:    [[BF_CLEAR45:%.*]] = and i32 [[BF_LOAD42]], 1, !dbg [[DBG84]]
// PPC64-NEXT:    [[BF_SET46:%.*]] = or i32 [[BF_CLEAR45]], [[BF_SHL44]], !dbg [[DBG84]]
// PPC64-NEXT:    store i32 [[BF_SET46]], i32* [[ATOMIC_TEMP41]], align 1, !dbg [[DBG84]]
// PPC64-NEXT:    [[TMP55:%.*]] = bitcast i32* [[ATOMIC_TEMP39]] to i8*, !dbg [[DBG84]]
// PPC64-NEXT:    [[TMP56:%.*]] = bitcast i32* [[ATOMIC_TEMP41]] to i8*, !dbg [[DBG84]]
// PPC64-NEXT:    [[CALL47:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 4, i8* noundef bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i8* noundef [[TMP55]], i8* noundef [[TMP56]], i32 noundef signext 0, i32 noundef signext 0), !dbg [[DBG84]]
// PPC64-NEXT:    br i1 [[CALL47]], label [[ATOMIC_EXIT48:%.*]], label [[ATOMIC_CONT40]], !dbg [[DBG84]]
// PPC64:       atomic_exit48:
// PPC64-NEXT:    [[TMP57:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG85:![0-9]+]]
// PPC64-NEXT:    [[CONV49:%.*]] = fptosi ppc_fp128 [[TMP57]] to i32, !dbg [[DBG85]]
// PPC64-NEXT:    [[ATOMIC_LOAD50:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS2:%.*]], %struct.BitFields2* @bfx2, i32 0, i32 0) monotonic, align 4, !dbg [[DBG86:![0-9]+]]
// PPC64-NEXT:    br label [[ATOMIC_CONT51:%.*]], !dbg [[DBG86]]
// PPC64:       atomic_cont51:
// PPC64-NEXT:    [[TMP58:%.*]] = phi i32 [ [[ATOMIC_LOAD50]], [[ATOMIC_EXIT48]] ], [ [[TMP61:%.*]], [[ATOMIC_CONT51]] ], !dbg [[DBG86]]
// PPC64-NEXT:    store i32 [[TMP58]], i32* [[ATOMIC_TEMP52]], align 4, !dbg [[DBG86]]
// PPC64-NEXT:    [[BF_LOAD53:%.*]] = load i32, i32* [[ATOMIC_TEMP52]], align 4, !dbg [[DBG86]]
// PPC64-NEXT:    [[BF_VALUE54:%.*]] = and i32 [[CONV49]], 1, !dbg [[DBG86]]
// PPC64-NEXT:    [[BF_CLEAR55:%.*]] = and i32 [[BF_LOAD53]], -2, !dbg [[DBG86]]
// PPC64-NEXT:    [[BF_SET56:%.*]] = or i32 [[BF_CLEAR55]], [[BF_VALUE54]], !dbg [[DBG86]]
// PPC64-NEXT:    store i32 [[BF_SET56]], i32* [[ATOMIC_TEMP52]], align 4, !dbg [[DBG86]]
// PPC64-NEXT:    [[TMP59:%.*]] = load i32, i32* [[ATOMIC_TEMP52]], align 4, !dbg [[DBG86]]
// PPC64-NEXT:    [[TMP60:%.*]] = cmpxchg i32* getelementptr inbounds ([[STRUCT_BITFIELDS2]], %struct.BitFields2* @bfx2, i32 0, i32 0), i32 [[TMP58]], i32 [[TMP59]] monotonic monotonic, align 4, !dbg [[DBG86]]
// PPC64-NEXT:    [[TMP61]] = extractvalue { i32, i1 } [[TMP60]], 0, !dbg [[DBG86]]
// PPC64-NEXT:    [[TMP62:%.*]] = extractvalue { i32, i1 } [[TMP60]], 1, !dbg [[DBG86]]
// PPC64-NEXT:    br i1 [[TMP62]], label [[ATOMIC_EXIT57:%.*]], label [[ATOMIC_CONT51]], !dbg [[DBG86]]
// PPC64:       atomic_exit57:
// PPC64-NEXT:    [[TMP63:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG87:![0-9]+]]
// PPC64-NEXT:    [[CONV58:%.*]] = fptosi ppc_fp128 [[TMP63]] to i32, !dbg [[DBG87]]
// PPC64-NEXT:    [[ATOMIC_LOAD59:%.*]] = load atomic i8, i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*) monotonic, align 1, !dbg [[DBG88:![0-9]+]]
// PPC64-NEXT:    br label [[ATOMIC_CONT60:%.*]], !dbg [[DBG88]]
// PPC64:       atomic_cont60:
// PPC64-NEXT:    [[TMP64:%.*]] = phi i8 [ [[ATOMIC_LOAD59]], [[ATOMIC_EXIT57]] ], [ [[TMP69:%.*]], [[ATOMIC_CONT60]] ], !dbg [[DBG88]]
// PPC64-NEXT:    [[TMP65:%.*]] = bitcast i32* [[ATOMIC_TEMP61]] to i8*, !dbg [[DBG88]]
// PPC64-NEXT:    store i8 [[TMP64]], i8* [[TMP65]], align 1, !dbg [[DBG88]]
// PPC64-NEXT:    [[TMP66:%.*]] = trunc i32 [[CONV58]] to i8, !dbg [[DBG88]]
// PPC64-NEXT:    [[BF_LOAD62:%.*]] = load i8, i8* [[TMP65]], align 1, !dbg [[DBG88]]
// PPC64-NEXT:    [[BF_VALUE63:%.*]] = and i8 [[TMP66]], 1, !dbg [[DBG88]]
// PPC64-NEXT:    [[BF_CLEAR64:%.*]] = and i8 [[BF_LOAD62]], -2, !dbg [[DBG88]]
// PPC64-NEXT:    [[BF_SET65:%.*]] = or i8 [[BF_CLEAR64]], [[BF_VALUE63]], !dbg [[DBG88]]
// PPC64-NEXT:    store i8 [[BF_SET65]], i8* [[TMP65]], align 1, !dbg [[DBG88]]
// PPC64-NEXT:    [[TMP67:%.*]] = load i8, i8* [[TMP65]], align 1, !dbg [[DBG88]]
// PPC64-NEXT:    [[TMP68:%.*]] = cmpxchg i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*), i8 [[TMP64]], i8 [[TMP67]] monotonic monotonic, align 1, !dbg [[DBG88]]
// PPC64-NEXT:    [[TMP69]] = extractvalue { i8, i1 } [[TMP68]], 0, !dbg [[DBG88]]
// PPC64-NEXT:    [[TMP70:%.*]] = extractvalue { i8, i1 } [[TMP68]], 1, !dbg [[DBG88]]
// PPC64-NEXT:    br i1 [[TMP70]], label [[ATOMIC_EXIT66:%.*]], label [[ATOMIC_CONT60]], !dbg [[DBG88]]
// PPC64:       atomic_exit66:
// PPC64-NEXT:    [[TMP71:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG89:![0-9]+]]
// PPC64-NEXT:    [[CONV67:%.*]] = fptosi ppc_fp128 [[TMP71]] to i32, !dbg [[DBG89]]
// PPC64-NEXT:    [[ATOMIC_LOAD68:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS3:%.*]], %struct.BitFields3* @bfx3, i32 0, i32 0) monotonic, align 4, !dbg [[DBG90:![0-9]+]]
// PPC64-NEXT:    br label [[ATOMIC_CONT69:%.*]], !dbg [[DBG90]]
// PPC64:       atomic_cont69:
// PPC64-NEXT:    [[TMP72:%.*]] = phi i32 [ [[ATOMIC_LOAD68]], [[ATOMIC_EXIT66]] ], [ [[TMP75:%.*]], [[ATOMIC_CONT69]] ], !dbg [[DBG90]]
// PPC64-NEXT:    store i32 [[TMP72]], i32* [[ATOMIC_TEMP70]], align 4, !dbg [[DBG90]]
// PPC64-NEXT:    [[BF_LOAD71:%.*]] = load i32, i32* [[ATOMIC_TEMP70]], align 4, !dbg [[DBG90]]
// PPC64-NEXT:    [[BF_VALUE72:%.*]] = and i32 [[CONV67]], 16383, !dbg [[DBG90]]
// PPC64-NEXT:    [[BF_SHL73:%.*]] = shl i32 [[BF_VALUE72]], 7, !dbg [[DBG90]]
// PPC64-NEXT:    [[BF_CLEAR74:%.*]] = and i32 [[BF_LOAD71]], -2097025, !dbg [[DBG90]]
// PPC64-NEXT:    [[BF_SET75:%.*]] = or i32 [[BF_CLEAR74]], [[BF_SHL73]], !dbg [[DBG90]]
// PPC64-NEXT:    store i32 [[BF_SET75]], i32* [[ATOMIC_TEMP70]], align 4, !dbg [[DBG90]]
// PPC64-NEXT:    [[TMP73:%.*]] = load i32, i32* [[ATOMIC_TEMP70]], align 4, !dbg [[DBG90]]
// PPC64-NEXT:    [[TMP74:%.*]] = cmpxchg i32* getelementptr inbounds ([[STRUCT_BITFIELDS3]], %struct.BitFields3* @bfx3, i32 0, i32 0), i32 [[TMP72]], i32 [[TMP73]] monotonic monotonic, align 4, !dbg [[DBG90]]
// PPC64-NEXT:    [[TMP75]] = extractvalue { i32, i1 } [[TMP74]], 0, !dbg [[DBG90]]
// PPC64-NEXT:    [[TMP76:%.*]] = extractvalue { i32, i1 } [[TMP74]], 1, !dbg [[DBG90]]
// PPC64-NEXT:    br i1 [[TMP76]], label [[ATOMIC_EXIT76:%.*]], label [[ATOMIC_CONT69]], !dbg [[DBG90]]
// PPC64:       atomic_exit76:
// PPC64-NEXT:    [[TMP77:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG91:![0-9]+]]
// PPC64-NEXT:    [[CONV77:%.*]] = fptosi ppc_fp128 [[TMP77]] to i32, !dbg [[DBG91]]
// PPC64-NEXT:    [[TMP78:%.*]] = bitcast i32* [[ATOMIC_TEMP78]] to i24*, !dbg [[DBG92:![0-9]+]]
// PPC64-NEXT:    [[TMP79:%.*]] = bitcast i24* [[TMP78]] to i8*, !dbg [[DBG92]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 3, i8* noundef bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i8* noundef [[TMP79]], i32 noundef signext 0), !dbg [[DBG92]]
// PPC64-NEXT:    br label [[ATOMIC_CONT79:%.*]], !dbg [[DBG92]]
// PPC64:       atomic_cont79:
// PPC64-NEXT:    [[TMP80:%.*]] = bitcast i32* [[ATOMIC_TEMP80]] to i24*, !dbg [[DBG92]]
// PPC64-NEXT:    [[TMP81:%.*]] = load i24, i24* [[TMP78]], align 1, !dbg [[DBG92]]
// PPC64-NEXT:    store i24 [[TMP81]], i24* [[TMP80]], align 1, !dbg [[DBG92]]
// PPC64-NEXT:    [[TMP82:%.*]] = trunc i32 [[CONV77]] to i24, !dbg [[DBG92]]
// PPC64-NEXT:    [[BF_LOAD81:%.*]] = load i24, i24* [[TMP80]], align 1, !dbg [[DBG92]]
// PPC64-NEXT:    [[BF_VALUE82:%.*]] = and i24 [[TMP82]], 16383, !dbg [[DBG92]]
// PPC64-NEXT:    [[BF_SHL83:%.*]] = shl i24 [[BF_VALUE82]], 7, !dbg [[DBG92]]
// PPC64-NEXT:    [[BF_CLEAR84:%.*]] = and i24 [[BF_LOAD81]], -2097025, !dbg [[DBG92]]
// PPC64-NEXT:    [[BF_SET85:%.*]] = or i24 [[BF_CLEAR84]], [[BF_SHL83]], !dbg [[DBG92]]
// PPC64-NEXT:    store i24 [[BF_SET85]], i24* [[TMP80]], align 1, !dbg [[DBG92]]
// PPC64-NEXT:    [[TMP83:%.*]] = bitcast i24* [[TMP78]] to i8*, !dbg [[DBG92]]
// PPC64-NEXT:    [[TMP84:%.*]] = bitcast i24* [[TMP80]] to i8*, !dbg [[DBG92]]
// PPC64-NEXT:    [[CALL86:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 3, i8* noundef bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i8* noundef [[TMP83]], i8* noundef [[TMP84]], i32 noundef signext 0, i32 noundef signext 0), !dbg [[DBG92]]
// PPC64-NEXT:    br i1 [[CALL86]], label [[ATOMIC_EXIT87:%.*]], label [[ATOMIC_CONT79]], !dbg [[DBG92]]
// PPC64:       atomic_exit87:
// PPC64-NEXT:    [[TMP85:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG93:![0-9]+]]
// PPC64-NEXT:    [[CONV88:%.*]] = fptosi ppc_fp128 [[TMP85]] to i32, !dbg [[DBG93]]
// PPC64-NEXT:    [[ATOMIC_LOAD89:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG94:![0-9]+]]
// PPC64-NEXT:    br label [[ATOMIC_CONT90:%.*]], !dbg [[DBG94]]
// PPC64:       atomic_cont90:
// PPC64-NEXT:    [[TMP86:%.*]] = phi i64 [ [[ATOMIC_LOAD89]], [[ATOMIC_EXIT87]] ], [ [[TMP90:%.*]], [[ATOMIC_CONT90]] ], !dbg [[DBG94]]
// PPC64-NEXT:    store i64 [[TMP86]], i64* [[ATOMIC_TEMP91]], align 8, !dbg [[DBG94]]
// PPC64-NEXT:    [[TMP87:%.*]] = zext i32 [[CONV88]] to i64, !dbg [[DBG94]]
// PPC64-NEXT:    [[BF_LOAD92:%.*]] = load i64, i64* [[ATOMIC_TEMP91]], align 8, !dbg [[DBG94]]
// PPC64-NEXT:    [[BF_VALUE93:%.*]] = and i64 [[TMP87]], 1, !dbg [[DBG94]]
// PPC64-NEXT:    [[BF_SHL94:%.*]] = shl i64 [[BF_VALUE93]], 15, !dbg [[DBG94]]
// PPC64-NEXT:    [[BF_CLEAR95:%.*]] = and i64 [[BF_LOAD92]], -32769, !dbg [[DBG94]]
// PPC64-NEXT:    [[BF_SET96:%.*]] = or i64 [[BF_CLEAR95]], [[BF_SHL94]], !dbg [[DBG94]]
// PPC64-NEXT:    store i64 [[BF_SET96]], i64* [[ATOMIC_TEMP91]], align 8, !dbg [[DBG94]]
// PPC64-NEXT:    [[TMP88:%.*]] = load i64, i64* [[ATOMIC_TEMP91]], align 8, !dbg [[DBG94]]
// PPC64-NEXT:    [[TMP89:%.*]] = cmpxchg i64* bitcast (%struct.BitFields4* @bfx4 to i64*), i64 [[TMP86]], i64 [[TMP88]] monotonic monotonic, align 8, !dbg [[DBG94]]
// PPC64-NEXT:    [[TMP90]] = extractvalue { i64, i1 } [[TMP89]], 0, !dbg [[DBG94]]
// PPC64-NEXT:    [[TMP91:%.*]] = extractvalue { i64, i1 } [[TMP89]], 1, !dbg [[DBG94]]
// PPC64-NEXT:    br i1 [[TMP91]], label [[ATOMIC_EXIT97:%.*]], label [[ATOMIC_CONT90]], !dbg [[DBG94]]
// PPC64:       atomic_exit97:
// PPC64-NEXT:    [[TMP92:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG95:![0-9]+]]
// PPC64-NEXT:    [[CONV98:%.*]] = fptosi ppc_fp128 [[TMP92]] to i32, !dbg [[DBG95]]
// PPC64-NEXT:    [[ATOMIC_LOAD99:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i32 0) monotonic, align 1, !dbg [[DBG96:![0-9]+]]
// PPC64-NEXT:    br label [[ATOMIC_CONT100:%.*]], !dbg [[DBG96]]
// PPC64:       atomic_cont100:
// PPC64-NEXT:    [[TMP93:%.*]] = phi i8 [ [[ATOMIC_LOAD99]], [[ATOMIC_EXIT97]] ], [ [[TMP98:%.*]], [[ATOMIC_CONT100]] ], !dbg [[DBG96]]
// PPC64-NEXT:    [[TMP94:%.*]] = bitcast i32* [[ATOMIC_TEMP101]] to i8*, !dbg [[DBG96]]
// PPC64-NEXT:    store i8 [[TMP93]], i8* [[TMP94]], align 1, !dbg [[DBG96]]
// PPC64-NEXT:    [[TMP95:%.*]] = trunc i32 [[CONV98]] to i8, !dbg [[DBG96]]
// PPC64-NEXT:    [[BF_LOAD102:%.*]] = load i8, i8* [[TMP94]], align 1, !dbg [[DBG96]]
// PPC64-NEXT:    [[BF_VALUE103:%.*]] = and i8 [[TMP95]], 1, !dbg [[DBG96]]
// PPC64-NEXT:    [[BF_SHL104:%.*]] = shl i8 [[BF_VALUE103]], 7, !dbg [[DBG96]]
// PPC64-NEXT:    [[BF_CLEAR105:%.*]] = and i8 [[BF_LOAD102]], 127, !dbg [[DBG96]]
// PPC64-NEXT:    [[BF_SET106:%.*]] = or i8 [[BF_CLEAR105]], [[BF_SHL104]], !dbg [[DBG96]]
// PPC64-NEXT:    store i8 [[BF_SET106]], i8* [[TMP94]], align 1, !dbg [[DBG96]]
// PPC64-NEXT:    [[TMP96:%.*]] = load i8, i8* [[TMP94]], align 1, !dbg [[DBG96]]
// PPC64-NEXT:    [[TMP97:%.*]] = cmpxchg i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i32 0), i8 [[TMP93]], i8 [[TMP96]] monotonic monotonic, align 1, !dbg [[DBG96]]
// PPC64-NEXT:    [[TMP98]] = extractvalue { i8, i1 } [[TMP97]], 0, !dbg [[DBG96]]
// PPC64-NEXT:    [[TMP99:%.*]] = extractvalue { i8, i1 } [[TMP97]], 1, !dbg [[DBG96]]
// PPC64-NEXT:    br i1 [[TMP99]], label [[ATOMIC_EXIT107:%.*]], label [[ATOMIC_CONT100]], !dbg [[DBG96]]
// PPC64:       atomic_exit107:
// PPC64-NEXT:    [[TMP100:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG97:![0-9]+]]
// PPC64-NEXT:    [[CONV108:%.*]] = fptosi ppc_fp128 [[TMP100]] to i64, !dbg [[DBG97]]
// PPC64-NEXT:    [[ATOMIC_LOAD109:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG98:![0-9]+]]
// PPC64-NEXT:    br label [[ATOMIC_CONT110:%.*]], !dbg [[DBG98]]
// PPC64:       atomic_cont110:
// PPC64-NEXT:    [[TMP101:%.*]] = phi i64 [ [[ATOMIC_LOAD109]], [[ATOMIC_EXIT107]] ], [ [[TMP104:%.*]], [[ATOMIC_CONT110]] ], !dbg [[DBG98]]
// PPC64-NEXT:    store i64 [[TMP101]], i64* [[ATOMIC_TEMP111]], align 8, !dbg [[DBG98]]
// PPC64-NEXT:    [[BF_LOAD112:%.*]] = load i64, i64* [[ATOMIC_TEMP111]], align 8, !dbg [[DBG98]]
// PPC64-NEXT:    [[BF_VALUE113:%.*]] = and i64 [[CONV108]], 127, !dbg [[DBG98]]
// PPC64-NEXT:    [[BF_SHL114:%.*]] = shl i64 [[BF_VALUE113]], 8, !dbg [[DBG98]]
// PPC64-NEXT:    [[BF_CLEAR115:%.*]] = and i64 [[BF_LOAD112]], -32513, !dbg [[DBG98]]
// PPC64-NEXT:    [[BF_SET116:%.*]] = or i64 [[BF_CLEAR115]], [[BF_SHL114]], !dbg [[DBG98]]
// PPC64-NEXT:    store i64 [[BF_SET116]], i64* [[ATOMIC_TEMP111]], align 8, !dbg [[DBG98]]
// PPC64-NEXT:    [[TMP102:%.*]] = load i64, i64* [[ATOMIC_TEMP111]], align 8, !dbg [[DBG98]]
// PPC64-NEXT:    [[TMP103:%.*]] = cmpxchg i64* bitcast (%struct.BitFields4* @bfx4 to i64*), i64 [[TMP101]], i64 [[TMP102]] monotonic monotonic, align 8, !dbg [[DBG98]]
// PPC64-NEXT:    [[TMP104]] = extractvalue { i64, i1 } [[TMP103]], 0, !dbg [[DBG98]]
// PPC64-NEXT:    [[TMP105:%.*]] = extractvalue { i64, i1 } [[TMP103]], 1, !dbg [[DBG98]]
// PPC64-NEXT:    br i1 [[TMP105]], label [[ATOMIC_EXIT117:%.*]], label [[ATOMIC_CONT110]], !dbg [[DBG98]]
// PPC64:       atomic_exit117:
// PPC64-NEXT:    [[TMP106:%.*]] = load ppc_fp128, ppc_fp128* @ldv, align 16, !dbg [[DBG99:![0-9]+]]
// PPC64-NEXT:    [[CONV118:%.*]] = fptosi ppc_fp128 [[TMP106]] to i64, !dbg [[DBG99]]
// PPC64-NEXT:    [[ATOMIC_LOAD119:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i32 0) monotonic, align 1, !dbg [[DBG100:![0-9]+]]
// PPC64-NEXT:    br label [[ATOMIC_CONT120:%.*]], !dbg [[DBG100]]
// PPC64:       atomic_cont120:
// PPC64-NEXT:    [[TMP107:%.*]] = phi i8 [ [[ATOMIC_LOAD119]], [[ATOMIC_EXIT117]] ], [ [[TMP112:%.*]], [[ATOMIC_CONT120]] ], !dbg [[DBG100]]
// PPC64-NEXT:    [[TMP108:%.*]] = bitcast i64* [[ATOMIC_TEMP121]] to i8*, !dbg [[DBG100]]
// PPC64-NEXT:    store i8 [[TMP107]], i8* [[TMP108]], align 1, !dbg [[DBG100]]
// PPC64-NEXT:    [[TMP109:%.*]] = trunc i64 [[CONV118]] to i8, !dbg [[DBG100]]
// PPC64-NEXT:    [[BF_LOAD122:%.*]] = load i8, i8* [[TMP108]], align 1, !dbg [[DBG100]]
// PPC64-NEXT:    [[BF_VALUE123:%.*]] = and i8 [[TMP109]], 127, !dbg [[DBG100]]
// PPC64-NEXT:    [[BF_CLEAR124:%.*]] = and i8 [[BF_LOAD122]], -128, !dbg [[DBG100]]
// PPC64-NEXT:    [[BF_SET125:%.*]] = or i8 [[BF_CLEAR124]], [[BF_VALUE123]], !dbg [[DBG100]]
// PPC64-NEXT:    store i8 [[BF_SET125]], i8* [[TMP108]], align 1, !dbg [[DBG100]]
// PPC64-NEXT:    [[TMP110:%.*]] = load i8, i8* [[TMP108]], align 1, !dbg [[DBG100]]
// PPC64-NEXT:    [[TMP111:%.*]] = cmpxchg i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i32 0), i8 [[TMP107]], i8 [[TMP110]] monotonic monotonic, align 1, !dbg [[DBG100]]
// PPC64-NEXT:    [[TMP112]] = extractvalue { i8, i1 } [[TMP111]], 0, !dbg [[DBG100]]
// PPC64-NEXT:    [[TMP113:%.*]] = extractvalue { i8, i1 } [[TMP111]], 1, !dbg [[DBG100]]
// PPC64-NEXT:    br i1 [[TMP113]], label [[ATOMIC_EXIT126:%.*]], label [[ATOMIC_CONT120]], !dbg [[DBG100]]
// PPC64:       atomic_exit126:
// PPC64-NEXT:    [[TMP114:%.*]] = load i64, i64* @ulv, align 8, !dbg [[DBG101:![0-9]+]]
// PPC64-NEXT:    [[CONV127:%.*]] = uitofp i64 [[TMP114]] to float, !dbg [[DBG101]]
// PPC64-NEXT:    [[ATOMIC_LOAD128:%.*]] = load atomic i64, i64* bitcast (<2 x float>* @float2x to i64*) monotonic, align 8, !dbg [[DBG102:![0-9]+]]
// PPC64-NEXT:    br label [[ATOMIC_CONT129:%.*]], !dbg [[DBG102]]
// PPC64:       atomic_cont129:
// PPC64-NEXT:    [[TMP115:%.*]] = phi i64 [ [[ATOMIC_LOAD128]], [[ATOMIC_EXIT126]] ], [ [[TMP121:%.*]], [[ATOMIC_CONT129]] ], !dbg [[DBG102]]
// PPC64-NEXT:    [[TMP116:%.*]] = bitcast <2 x float>* [[ATOMIC_TEMP130]] to i64*, !dbg [[DBG102]]
// PPC64-NEXT:    store i64 [[TMP115]], i64* [[TMP116]], align 8, !dbg [[DBG102]]
// PPC64-NEXT:    [[TMP117:%.*]] = load <2 x float>, <2 x float>* [[ATOMIC_TEMP130]], align 8, !dbg [[DBG102]]
// PPC64-NEXT:    [[TMP118:%.*]] = insertelement <2 x float> [[TMP117]], float [[CONV127]], i64 0, !dbg [[DBG102]]
// PPC64-NEXT:    store <2 x float> [[TMP118]], <2 x float>* [[ATOMIC_TEMP130]], align 8, !dbg [[DBG102]]
// PPC64-NEXT:    [[TMP119:%.*]] = load i64, i64* [[TMP116]], align 8, !dbg [[DBG102]]
// PPC64-NEXT:    [[TMP120:%.*]] = cmpxchg i64* bitcast (<2 x float>* @float2x to i64*), i64 [[TMP115]], i64 [[TMP119]] monotonic monotonic, align 8, !dbg [[DBG102]]
// PPC64-NEXT:    [[TMP121]] = extractvalue { i64, i1 } [[TMP120]], 0, !dbg [[DBG102]]
// PPC64-NEXT:    [[TMP122:%.*]] = extractvalue { i64, i1 } [[TMP120]], 1, !dbg [[DBG102]]
// PPC64-NEXT:    br i1 [[TMP122]], label [[ATOMIC_EXIT131:%.*]], label [[ATOMIC_CONT129]], !dbg [[DBG102]]
// PPC64:       atomic_exit131:
// PPC64-NEXT:    ret i32 0, !dbg [[DBG103:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@main
// AARCH64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP1:%.*]] = alloca { float, float }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP2:%.*]] = alloca { double, double }, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP9:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP30:%.*]] = alloca <4 x i32>, align 16
// AARCH64-NEXT:    [[ATOMIC_TEMP34:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP37:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP39:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP48:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP57:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP67:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP75:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP77:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP88:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP98:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP107:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP117:%.*]] = alloca i64, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP127:%.*]] = alloca <2 x float>, align 8
// AARCH64-NEXT:    store i32 0, i32* [[RETVAL]], align 4
// AARCH64-NEXT:    store atomic i32 1, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1) monotonic, align 4, !dbg [[DBG9:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = load i8, i8* @bv, align 1, !dbg [[DBG10:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[TMP0]] to i1, !dbg [[DBG10]]
// AARCH64-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG11:![0-9]+]]
// AARCH64-NEXT:    store atomic i8 [[FROMBOOL]], i8* @bx monotonic, align 1, !dbg [[DBG11]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i8, i8* @cv, align 1, !dbg [[DBG12:![0-9]+]]
// AARCH64-NEXT:    store atomic i8 [[TMP1]], i8* @cx release, align 1, !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    fence release
// AARCH64-NEXT:    [[TMP2:%.*]] = load i8, i8* @ucv, align 1, !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    store atomic i8 [[TMP2]], i8* @ucx monotonic, align 1, !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = load i16, i16* @sv, align 2, !dbg [[DBG16:![0-9]+]]
// AARCH64-NEXT:    store atomic i16 [[TMP3]], i16* @sx monotonic, align 2, !dbg [[DBG17:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = load i16, i16* @usv, align 2, !dbg [[DBG18:![0-9]+]]
// AARCH64-NEXT:    store atomic i16 [[TMP4]], i16* @usx monotonic, align 2, !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    [[TMP5:%.*]] = load i32, i32* @iv, align 4, !dbg [[DBG20:![0-9]+]]
// AARCH64-NEXT:    store atomic i32 [[TMP5]], i32* @ix monotonic, align 4, !dbg [[DBG21:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = load i32, i32* @uiv, align 4, !dbg [[DBG22:![0-9]+]]
// AARCH64-NEXT:    store atomic i32 [[TMP6]], i32* @uix monotonic, align 4, !dbg [[DBG23:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = load i64, i64* @lv, align 8, !dbg [[DBG24:![0-9]+]]
// AARCH64-NEXT:    store atomic i64 [[TMP7]], i64* @lx monotonic, align 8, !dbg [[DBG25:![0-9]+]]
// AARCH64-NEXT:    [[TMP8:%.*]] = load i64, i64* @ulv, align 8, !dbg [[DBG26:![0-9]+]]
// AARCH64-NEXT:    store atomic i64 [[TMP8]], i64* @ulx monotonic, align 8, !dbg [[DBG27:![0-9]+]]
// AARCH64-NEXT:    [[TMP9:%.*]] = load i64, i64* @llv, align 8, !dbg [[DBG28:![0-9]+]]
// AARCH64-NEXT:    store atomic i64 [[TMP9]], i64* @llx monotonic, align 8, !dbg [[DBG29:![0-9]+]]
// AARCH64-NEXT:    [[TMP10:%.*]] = load i64, i64* @ullv, align 8, !dbg [[DBG30:![0-9]+]]
// AARCH64-NEXT:    store atomic i64 [[TMP10]], i64* @ullx monotonic, align 8, !dbg [[DBG31:![0-9]+]]
// AARCH64-NEXT:    [[TMP11:%.*]] = load float, float* @fv, align 4, !dbg [[DBG32:![0-9]+]]
// AARCH64-NEXT:    [[TMP12:%.*]] = bitcast float [[TMP11]] to i32, !dbg [[DBG33:![0-9]+]]
// AARCH64-NEXT:    store atomic i32 [[TMP12]], i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG33]]
// AARCH64-NEXT:    [[TMP13:%.*]] = load double, double* @dv, align 8, !dbg [[DBG34:![0-9]+]]
// AARCH64-NEXT:    [[TMP14:%.*]] = bitcast double [[TMP13]] to i64, !dbg [[DBG35:![0-9]+]]
// AARCH64-NEXT:    store atomic i64 [[TMP14]], i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG35]]
// AARCH64-NEXT:    [[TMP15:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG36:![0-9]+]]
// AARCH64-NEXT:    [[TMP16:%.*]] = bitcast fp128 [[TMP15]] to i128, !dbg [[DBG37:![0-9]+]]
// AARCH64-NEXT:    store atomic i128 [[TMP16]], i128* bitcast (fp128* @ldx to i128*) monotonic, align 16, !dbg [[DBG37]]
// AARCH64-NEXT:    [[CIV_REAL:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG38:![0-9]+]]
// AARCH64-NEXT:    [[CIV_IMAG:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG38]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP]], i32 0, i32 0, !dbg [[DBG39:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP]], i32 0, i32 1, !dbg [[DBG39]]
// AARCH64-NEXT:    store i32 [[CIV_REAL]], i32* [[ATOMIC_TEMP_REALP]], align 4, !dbg [[DBG39]]
// AARCH64-NEXT:    store i32 [[CIV_IMAG]], i32* [[ATOMIC_TEMP_IMAGP]], align 4, !dbg [[DBG39]]
// AARCH64-NEXT:    [[TMP17:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP]] to i8*, !dbg [[DBG39]]
// AARCH64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP17]], i32 noundef 0), !dbg [[DBG39]]
// AARCH64-NEXT:    [[CFV_REAL:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG40:![0-9]+]]
// AARCH64-NEXT:    [[CFV_IMAG:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG40]]
// AARCH64-NEXT:    [[ATOMIC_TEMP1_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP1]], i32 0, i32 0, !dbg [[DBG41:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP1_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP1]], i32 0, i32 1, !dbg [[DBG41]]
// AARCH64-NEXT:    store float [[CFV_REAL]], float* [[ATOMIC_TEMP1_REALP]], align 4, !dbg [[DBG41]]
// AARCH64-NEXT:    store float [[CFV_IMAG]], float* [[ATOMIC_TEMP1_IMAGP]], align 4, !dbg [[DBG41]]
// AARCH64-NEXT:    [[TMP18:%.*]] = bitcast { float, float }* [[ATOMIC_TEMP1]] to i8*, !dbg [[DBG41]]
// AARCH64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ float, float }* @cfx to i8*), i8* noundef [[TMP18]], i32 noundef 0), !dbg [[DBG41]]
// AARCH64-NEXT:    [[CDV_REAL:%.*]] = load double, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG42:![0-9]+]]
// AARCH64-NEXT:    [[CDV_IMAG:%.*]] = load double, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG42]]
// AARCH64-NEXT:    [[ATOMIC_TEMP2_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP2]], i32 0, i32 0, !dbg [[DBG43:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP2_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP2]], i32 0, i32 1, !dbg [[DBG43]]
// AARCH64-NEXT:    store double [[CDV_REAL]], double* [[ATOMIC_TEMP2_REALP]], align 8, !dbg [[DBG43]]
// AARCH64-NEXT:    store double [[CDV_IMAG]], double* [[ATOMIC_TEMP2_IMAGP]], align 8, !dbg [[DBG43]]
// AARCH64-NEXT:    [[TMP19:%.*]] = bitcast { double, double }* [[ATOMIC_TEMP2]] to i8*, !dbg [[DBG43]]
// AARCH64-NEXT:    call void @__atomic_store(i64 noundef 16, i8* noundef bitcast ({ double, double }* @cdx to i8*), i8* noundef [[TMP19]], i32 noundef 5), !dbg [[DBG43]]
// AARCH64-NEXT:    fence release
// AARCH64-NEXT:    [[TMP20:%.*]] = load i8, i8* @bv, align 1, !dbg [[DBG44:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL3:%.*]] = trunc i8 [[TMP20]] to i1, !dbg [[DBG44]]
// AARCH64-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL3]] to i64, !dbg [[DBG44]]
// AARCH64-NEXT:    store atomic i64 [[CONV]], i64* @ulx monotonic, align 8, !dbg [[DBG45:![0-9]+]]
// AARCH64-NEXT:    [[TMP21:%.*]] = load i8, i8* @cv, align 1, !dbg [[DBG46:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL4:%.*]] = icmp ne i8 [[TMP21]], 0, !dbg [[DBG46]]
// AARCH64-NEXT:    [[FROMBOOL5:%.*]] = zext i1 [[TOBOOL4]] to i8, !dbg [[DBG47:![0-9]+]]
// AARCH64-NEXT:    store atomic i8 [[FROMBOOL5]], i8* @bx monotonic, align 1, !dbg [[DBG47]]
// AARCH64-NEXT:    [[TMP22:%.*]] = load i8, i8* @ucv, align 1, !dbg [[DBG48:![0-9]+]]
// AARCH64-NEXT:    store atomic i8 [[TMP22]], i8* @cx seq_cst, align 1, !dbg [[DBG49:![0-9]+]]
// AARCH64-NEXT:    fence release
// AARCH64-NEXT:    [[TMP23:%.*]] = load i16, i16* @sv, align 2, !dbg [[DBG50:![0-9]+]]
// AARCH64-NEXT:    [[CONV6:%.*]] = sext i16 [[TMP23]] to i64, !dbg [[DBG50]]
// AARCH64-NEXT:    store atomic i64 [[CONV6]], i64* @ulx monotonic, align 8, !dbg [[DBG51:![0-9]+]]
// AARCH64-NEXT:    [[TMP24:%.*]] = load i16, i16* @usv, align 2, !dbg [[DBG52:![0-9]+]]
// AARCH64-NEXT:    [[CONV7:%.*]] = zext i16 [[TMP24]] to i64, !dbg [[DBG52]]
// AARCH64-NEXT:    store atomic i64 [[CONV7]], i64* @lx monotonic, align 8, !dbg [[DBG53:![0-9]+]]
// AARCH64-NEXT:    [[TMP25:%.*]] = load i32, i32* @iv, align 4, !dbg [[DBG54:![0-9]+]]
// AARCH64-NEXT:    store atomic i32 [[TMP25]], i32* @uix seq_cst, align 4, !dbg [[DBG55:![0-9]+]]
// AARCH64-NEXT:    fence release
// AARCH64-NEXT:    [[TMP26:%.*]] = load i32, i32* @uiv, align 4, !dbg [[DBG56:![0-9]+]]
// AARCH64-NEXT:    store atomic i32 [[TMP26]], i32* @ix monotonic, align 4, !dbg [[DBG57:![0-9]+]]
// AARCH64-NEXT:    [[TMP27:%.*]] = load i64, i64* @lv, align 8, !dbg [[DBG58:![0-9]+]]
// AARCH64-NEXT:    [[CONV8:%.*]] = trunc i64 [[TMP27]] to i32, !dbg [[DBG58]]
// AARCH64-NEXT:    [[ATOMIC_TEMP9_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP9]], i32 0, i32 0, !dbg [[DBG59:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP9_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP9]], i32 0, i32 1, !dbg [[DBG59]]
// AARCH64-NEXT:    store i32 [[CONV8]], i32* [[ATOMIC_TEMP9_REALP]], align 4, !dbg [[DBG59]]
// AARCH64-NEXT:    store i32 0, i32* [[ATOMIC_TEMP9_IMAGP]], align 4, !dbg [[DBG59]]
// AARCH64-NEXT:    [[TMP28:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP9]] to i8*, !dbg [[DBG59]]
// AARCH64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP28]], i32 noundef 0), !dbg [[DBG59]]
// AARCH64-NEXT:    [[TMP29:%.*]] = load i64, i64* @ulv, align 8, !dbg [[DBG60:![0-9]+]]
// AARCH64-NEXT:    [[CONV10:%.*]] = uitofp i64 [[TMP29]] to float, !dbg [[DBG60]]
// AARCH64-NEXT:    [[TMP30:%.*]] = bitcast float [[CONV10]] to i32, !dbg [[DBG61:![0-9]+]]
// AARCH64-NEXT:    store atomic i32 [[TMP30]], i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG61]]
// AARCH64-NEXT:    [[TMP31:%.*]] = load i64, i64* @llv, align 8, !dbg [[DBG62:![0-9]+]]
// AARCH64-NEXT:    [[CONV11:%.*]] = sitofp i64 [[TMP31]] to double, !dbg [[DBG62]]
// AARCH64-NEXT:    [[TMP32:%.*]] = bitcast double [[CONV11]] to i64, !dbg [[DBG63:![0-9]+]]
// AARCH64-NEXT:    store atomic i64 [[TMP32]], i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG63]]
// AARCH64-NEXT:    [[TMP33:%.*]] = load i64, i64* @ullv, align 8, !dbg [[DBG64:![0-9]+]]
// AARCH64-NEXT:    [[CONV12:%.*]] = uitofp i64 [[TMP33]] to fp128, !dbg [[DBG64]]
// AARCH64-NEXT:    [[TMP34:%.*]] = bitcast fp128 [[CONV12]] to i128, !dbg [[DBG65:![0-9]+]]
// AARCH64-NEXT:    store atomic i128 [[TMP34]], i128* bitcast (fp128* @ldx to i128*) monotonic, align 16, !dbg [[DBG65]]
// AARCH64-NEXT:    [[TMP35:%.*]] = load float, float* @fv, align 4, !dbg [[DBG66:![0-9]+]]
// AARCH64-NEXT:    [[CONV13:%.*]] = fptosi float [[TMP35]] to i32, !dbg [[DBG66]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG67:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG67]]
// AARCH64-NEXT:    store i32 [[CONV13]], i32* [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG67]]
// AARCH64-NEXT:    store i32 0, i32* [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG67]]
// AARCH64-NEXT:    [[TMP36:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP14]] to i8*, !dbg [[DBG67]]
// AARCH64-NEXT:    call void @__atomic_store(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP36]], i32 noundef 0), !dbg [[DBG67]]
// AARCH64-NEXT:    [[TMP37:%.*]] = load double, double* @dv, align 8, !dbg [[DBG68:![0-9]+]]
// AARCH64-NEXT:    [[CONV15:%.*]] = fptosi double [[TMP37]] to i16, !dbg [[DBG68]]
// AARCH64-NEXT:    store atomic i16 [[CONV15]], i16* @sx monotonic, align 2, !dbg [[DBG69:![0-9]+]]
// AARCH64-NEXT:    [[TMP38:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG70:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL16:%.*]] = fcmp une fp128 [[TMP38]], 0xL00000000000000000000000000000000, !dbg [[DBG70]]
// AARCH64-NEXT:    [[FROMBOOL17:%.*]] = zext i1 [[TOBOOL16]] to i8, !dbg [[DBG71:![0-9]+]]
// AARCH64-NEXT:    store atomic i8 [[FROMBOOL17]], i8* @bx monotonic, align 1, !dbg [[DBG71]]
// AARCH64-NEXT:    [[CIV_REAL18:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG72:![0-9]+]]
// AARCH64-NEXT:    [[CIV_IMAG19:%.*]] = load i32, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG72]]
// AARCH64-NEXT:    [[TOBOOL20:%.*]] = icmp ne i32 [[CIV_REAL18]], 0, !dbg [[DBG72]]
// AARCH64-NEXT:    [[TOBOOL21:%.*]] = icmp ne i32 [[CIV_IMAG19]], 0, !dbg [[DBG72]]
// AARCH64-NEXT:    [[TOBOOL22:%.*]] = or i1 [[TOBOOL20]], [[TOBOOL21]], !dbg [[DBG72]]
// AARCH64-NEXT:    [[FROMBOOL23:%.*]] = zext i1 [[TOBOOL22]] to i8, !dbg [[DBG73:![0-9]+]]
// AARCH64-NEXT:    store atomic i8 [[FROMBOOL23]], i8* @bx monotonic, align 1, !dbg [[DBG73]]
// AARCH64-NEXT:    [[CFV_REAL24:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG74:![0-9]+]]
// AARCH64-NEXT:    [[CONV25:%.*]] = fptoui float [[CFV_REAL24]] to i16, !dbg [[DBG74]]
// AARCH64-NEXT:    store atomic i16 [[CONV25]], i16* @usx monotonic, align 2, !dbg [[DBG75:![0-9]+]]
// AARCH64-NEXT:    [[CDV_REAL26:%.*]] = load double, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG76:![0-9]+]]
// AARCH64-NEXT:    [[CONV27:%.*]] = fptosi double [[CDV_REAL26]] to i64, !dbg [[DBG76]]
// AARCH64-NEXT:    store atomic i64 [[CONV27]], i64* @llx monotonic, align 8, !dbg [[DBG77:![0-9]+]]
// AARCH64-NEXT:    [[TMP39:%.*]] = load i16, i16* @sv, align 2, !dbg [[DBG78:![0-9]+]]
// AARCH64-NEXT:    [[TMP40:%.*]] = load i8, i8* @bv, align 1, !dbg [[DBG79:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL28:%.*]] = trunc i8 [[TMP40]] to i1, !dbg [[DBG79]]
// AARCH64-NEXT:    [[CONV29:%.*]] = zext i1 [[TOBOOL28]] to i32, !dbg [[DBG79]]
// AARCH64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i128, i128* bitcast (<4 x i32>* @int4x to i128*) monotonic, align 16, !dbg [[DBG80:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT:%.*]], !dbg [[DBG80]]
// AARCH64:       atomic_cont:
// AARCH64-NEXT:    [[TMP41:%.*]] = phi i128 [ [[ATOMIC_LOAD]], [[ENTRY:%.*]] ], [ [[TMP46:%.*]], [[ATOMIC_CONT]] ], !dbg [[DBG80]]
// AARCH64-NEXT:    [[TMP42:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP30]] to i128*, !dbg [[DBG80]]
// AARCH64-NEXT:    store i128 [[TMP41]], i128* [[TMP42]], align 16, !dbg [[DBG80]]
// AARCH64-NEXT:    [[TMP43:%.*]] = load <4 x i32>, <4 x i32>* [[ATOMIC_TEMP30]], align 16, !dbg [[DBG80]]
// AARCH64-NEXT:    [[VECINS:%.*]] = insertelement <4 x i32> [[TMP43]], i32 [[CONV29]], i16 [[TMP39]], !dbg [[DBG80]]
// AARCH64-NEXT:    store <4 x i32> [[VECINS]], <4 x i32>* [[ATOMIC_TEMP30]], align 16, !dbg [[DBG80]]
// AARCH64-NEXT:    [[TMP44:%.*]] = load i128, i128* [[TMP42]], align 16, !dbg [[DBG80]]
// AARCH64-NEXT:    [[TMP45:%.*]] = cmpxchg i128* bitcast (<4 x i32>* @int4x to i128*), i128 [[TMP41]], i128 [[TMP44]] monotonic monotonic, align 16, !dbg [[DBG80]]
// AARCH64-NEXT:    [[TMP46]] = extractvalue { i128, i1 } [[TMP45]], 0, !dbg [[DBG80]]
// AARCH64-NEXT:    [[TMP47:%.*]] = extractvalue { i128, i1 } [[TMP45]], 1, !dbg [[DBG80]]
// AARCH64-NEXT:    br i1 [[TMP47]], label [[ATOMIC_EXIT:%.*]], label [[ATOMIC_CONT]], !dbg [[DBG80]]
// AARCH64:       atomic_exit:
// AARCH64-NEXT:    [[TMP48:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG81:![0-9]+]]
// AARCH64-NEXT:    [[CONV31:%.*]] = fptosi fp128 [[TMP48]] to i32, !dbg [[DBG81]]
// AARCH64-NEXT:    [[ATOMIC_LOAD32:%.*]] = load atomic i32, i32* bitcast (i8* getelementptr (i8, i8* bitcast (%struct.BitFields* @bfx to i8*), i64 4) to i32*) monotonic, align 4, !dbg [[DBG82:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT33:%.*]], !dbg [[DBG82]]
// AARCH64:       atomic_cont33:
// AARCH64-NEXT:    [[TMP49:%.*]] = phi i32 [ [[ATOMIC_LOAD32]], [[ATOMIC_EXIT]] ], [ [[TMP52:%.*]], [[ATOMIC_CONT33]] ], !dbg [[DBG82]]
// AARCH64-NEXT:    store i32 [[TMP49]], i32* [[ATOMIC_TEMP34]], align 4, !dbg [[DBG82]]
// AARCH64-NEXT:    [[BF_LOAD:%.*]] = load i32, i32* [[ATOMIC_TEMP34]], align 4, !dbg [[DBG82]]
// AARCH64-NEXT:    [[BF_VALUE:%.*]] = and i32 [[CONV31]], 2147483647, !dbg [[DBG82]]
// AARCH64-NEXT:    [[BF_CLEAR:%.*]] = and i32 [[BF_LOAD]], -2147483648, !dbg [[DBG82]]
// AARCH64-NEXT:    [[BF_SET:%.*]] = or i32 [[BF_CLEAR]], [[BF_VALUE]], !dbg [[DBG82]]
// AARCH64-NEXT:    store i32 [[BF_SET]], i32* [[ATOMIC_TEMP34]], align 4, !dbg [[DBG82]]
// AARCH64-NEXT:    [[TMP50:%.*]] = load i32, i32* [[ATOMIC_TEMP34]], align 4, !dbg [[DBG82]]
// AARCH64-NEXT:    [[TMP51:%.*]] = cmpxchg i32* bitcast (i8* getelementptr (i8, i8* bitcast (%struct.BitFields* @bfx to i8*), i64 4) to i32*), i32 [[TMP49]], i32 [[TMP50]] monotonic monotonic, align 4, !dbg [[DBG82]]
// AARCH64-NEXT:    [[TMP52]] = extractvalue { i32, i1 } [[TMP51]], 0, !dbg [[DBG82]]
// AARCH64-NEXT:    [[TMP53:%.*]] = extractvalue { i32, i1 } [[TMP51]], 1, !dbg [[DBG82]]
// AARCH64-NEXT:    br i1 [[TMP53]], label [[ATOMIC_EXIT35:%.*]], label [[ATOMIC_CONT33]], !dbg [[DBG82]]
// AARCH64:       atomic_exit35:
// AARCH64-NEXT:    [[TMP54:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG83:![0-9]+]]
// AARCH64-NEXT:    [[CONV36:%.*]] = fptosi fp128 [[TMP54]] to i32, !dbg [[DBG83]]
// AARCH64-NEXT:    [[TMP55:%.*]] = bitcast i32* [[ATOMIC_TEMP37]] to i8*, !dbg [[DBG84:![0-9]+]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 4, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i64 4), i8* noundef [[TMP55]], i32 noundef 0), !dbg [[DBG84]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT38:%.*]], !dbg [[DBG84]]
// AARCH64:       atomic_cont38:
// AARCH64-NEXT:    [[TMP56:%.*]] = load i32, i32* [[ATOMIC_TEMP37]], align 1, !dbg [[DBG84]]
// AARCH64-NEXT:    store i32 [[TMP56]], i32* [[ATOMIC_TEMP39]], align 1, !dbg [[DBG84]]
// AARCH64-NEXT:    [[BF_LOAD40:%.*]] = load i32, i32* [[ATOMIC_TEMP39]], align 1, !dbg [[DBG84]]
// AARCH64-NEXT:    [[BF_VALUE41:%.*]] = and i32 [[CONV36]], 2147483647, !dbg [[DBG84]]
// AARCH64-NEXT:    [[BF_CLEAR42:%.*]] = and i32 [[BF_LOAD40]], -2147483648, !dbg [[DBG84]]
// AARCH64-NEXT:    [[BF_SET43:%.*]] = or i32 [[BF_CLEAR42]], [[BF_VALUE41]], !dbg [[DBG84]]
// AARCH64-NEXT:    store i32 [[BF_SET43]], i32* [[ATOMIC_TEMP39]], align 1, !dbg [[DBG84]]
// AARCH64-NEXT:    [[TMP57:%.*]] = bitcast i32* [[ATOMIC_TEMP37]] to i8*, !dbg [[DBG84]]
// AARCH64-NEXT:    [[TMP58:%.*]] = bitcast i32* [[ATOMIC_TEMP39]] to i8*, !dbg [[DBG84]]
// AARCH64-NEXT:    [[CALL:%.*]] = call i1 @__atomic_compare_exchange(i64 noundef 4, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i64 4), i8* noundef [[TMP57]], i8* noundef [[TMP58]], i32 noundef 0, i32 noundef 0), !dbg [[DBG84]]
// AARCH64-NEXT:    br i1 [[CALL]], label [[ATOMIC_EXIT44:%.*]], label [[ATOMIC_CONT38]], !dbg [[DBG84]]
// AARCH64:       atomic_exit44:
// AARCH64-NEXT:    [[TMP59:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG85:![0-9]+]]
// AARCH64-NEXT:    [[CONV45:%.*]] = fptosi fp128 [[TMP59]] to i32, !dbg [[DBG85]]
// AARCH64-NEXT:    [[ATOMIC_LOAD46:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS2:%.*]], %struct.BitFields2* @bfx2, i32 0, i32 0) monotonic, align 4, !dbg [[DBG86:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT47:%.*]], !dbg [[DBG86]]
// AARCH64:       atomic_cont47:
// AARCH64-NEXT:    [[TMP60:%.*]] = phi i32 [ [[ATOMIC_LOAD46]], [[ATOMIC_EXIT44]] ], [ [[TMP63:%.*]], [[ATOMIC_CONT47]] ], !dbg [[DBG86]]
// AARCH64-NEXT:    store i32 [[TMP60]], i32* [[ATOMIC_TEMP48]], align 4, !dbg [[DBG86]]
// AARCH64-NEXT:    [[BF_LOAD49:%.*]] = load i32, i32* [[ATOMIC_TEMP48]], align 4, !dbg [[DBG86]]
// AARCH64-NEXT:    [[BF_VALUE50:%.*]] = and i32 [[CONV45]], 1, !dbg [[DBG86]]
// AARCH64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_VALUE50]], 31, !dbg [[DBG86]]
// AARCH64-NEXT:    [[BF_CLEAR51:%.*]] = and i32 [[BF_LOAD49]], 2147483647, !dbg [[DBG86]]
// AARCH64-NEXT:    [[BF_SET52:%.*]] = or i32 [[BF_CLEAR51]], [[BF_SHL]], !dbg [[DBG86]]
// AARCH64-NEXT:    store i32 [[BF_SET52]], i32* [[ATOMIC_TEMP48]], align 4, !dbg [[DBG86]]
// AARCH64-NEXT:    [[TMP61:%.*]] = load i32, i32* [[ATOMIC_TEMP48]], align 4, !dbg [[DBG86]]
// AARCH64-NEXT:    [[TMP62:%.*]] = cmpxchg i32* getelementptr inbounds ([[STRUCT_BITFIELDS2]], %struct.BitFields2* @bfx2, i32 0, i32 0), i32 [[TMP60]], i32 [[TMP61]] monotonic monotonic, align 4, !dbg [[DBG86]]
// AARCH64-NEXT:    [[TMP63]] = extractvalue { i32, i1 } [[TMP62]], 0, !dbg [[DBG86]]
// AARCH64-NEXT:    [[TMP64:%.*]] = extractvalue { i32, i1 } [[TMP62]], 1, !dbg [[DBG86]]
// AARCH64-NEXT:    br i1 [[TMP64]], label [[ATOMIC_EXIT53:%.*]], label [[ATOMIC_CONT47]], !dbg [[DBG86]]
// AARCH64:       atomic_exit53:
// AARCH64-NEXT:    [[TMP65:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG87:![0-9]+]]
// AARCH64-NEXT:    [[CONV54:%.*]] = fptosi fp128 [[TMP65]] to i32, !dbg [[DBG87]]
// AARCH64-NEXT:    [[ATOMIC_LOAD55:%.*]] = load atomic i8, i8* getelementptr (i8, i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*), i64 3) monotonic, align 1, !dbg [[DBG88:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT56:%.*]], !dbg [[DBG88]]
// AARCH64:       atomic_cont56:
// AARCH64-NEXT:    [[TMP66:%.*]] = phi i8 [ [[ATOMIC_LOAD55]], [[ATOMIC_EXIT53]] ], [ [[TMP71:%.*]], [[ATOMIC_CONT56]] ], !dbg [[DBG88]]
// AARCH64-NEXT:    [[TMP67:%.*]] = bitcast i32* [[ATOMIC_TEMP57]] to i8*, !dbg [[DBG88]]
// AARCH64-NEXT:    store i8 [[TMP66]], i8* [[TMP67]], align 1, !dbg [[DBG88]]
// AARCH64-NEXT:    [[TMP68:%.*]] = trunc i32 [[CONV54]] to i8, !dbg [[DBG88]]
// AARCH64-NEXT:    [[BF_LOAD58:%.*]] = load i8, i8* [[TMP67]], align 1, !dbg [[DBG88]]
// AARCH64-NEXT:    [[BF_VALUE59:%.*]] = and i8 [[TMP68]], 1, !dbg [[DBG88]]
// AARCH64-NEXT:    [[BF_SHL60:%.*]] = shl i8 [[BF_VALUE59]], 7, !dbg [[DBG88]]
// AARCH64-NEXT:    [[BF_CLEAR61:%.*]] = and i8 [[BF_LOAD58]], 127, !dbg [[DBG88]]
// AARCH64-NEXT:    [[BF_SET62:%.*]] = or i8 [[BF_CLEAR61]], [[BF_SHL60]], !dbg [[DBG88]]
// AARCH64-NEXT:    store i8 [[BF_SET62]], i8* [[TMP67]], align 1, !dbg [[DBG88]]
// AARCH64-NEXT:    [[TMP69:%.*]] = load i8, i8* [[TMP67]], align 1, !dbg [[DBG88]]
// AARCH64-NEXT:    [[TMP70:%.*]] = cmpxchg i8* getelementptr (i8, i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*), i64 3), i8 [[TMP66]], i8 [[TMP69]] monotonic monotonic, align 1, !dbg [[DBG88]]
// AARCH64-NEXT:    [[TMP71]] = extractvalue { i8, i1 } [[TMP70]], 0, !dbg [[DBG88]]
// AARCH64-NEXT:    [[TMP72:%.*]] = extractvalue { i8, i1 } [[TMP70]], 1, !dbg [[DBG88]]
// AARCH64-NEXT:    br i1 [[TMP72]], label [[ATOMIC_EXIT63:%.*]], label [[ATOMIC_CONT56]], !dbg [[DBG88]]
// AARCH64:       atomic_exit63:
// AARCH64-NEXT:    [[TMP73:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG89:![0-9]+]]
// AARCH64-NEXT:    [[CONV64:%.*]] = fptosi fp128 [[TMP73]] to i32, !dbg [[DBG89]]
// AARCH64-NEXT:    [[ATOMIC_LOAD65:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS3:%.*]], %struct.BitFields3* @bfx3, i32 0, i32 0) monotonic, align 4, !dbg [[DBG90:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT66:%.*]], !dbg [[DBG90]]
// AARCH64:       atomic_cont66:
// AARCH64-NEXT:    [[TMP74:%.*]] = phi i32 [ [[ATOMIC_LOAD65]], [[ATOMIC_EXIT63]] ], [ [[TMP77:%.*]], [[ATOMIC_CONT66]] ], !dbg [[DBG90]]
// AARCH64-NEXT:    store i32 [[TMP74]], i32* [[ATOMIC_TEMP67]], align 4, !dbg [[DBG90]]
// AARCH64-NEXT:    [[BF_LOAD68:%.*]] = load i32, i32* [[ATOMIC_TEMP67]], align 4, !dbg [[DBG90]]
// AARCH64-NEXT:    [[BF_VALUE69:%.*]] = and i32 [[CONV64]], 16383, !dbg [[DBG90]]
// AARCH64-NEXT:    [[BF_SHL70:%.*]] = shl i32 [[BF_VALUE69]], 11, !dbg [[DBG90]]
// AARCH64-NEXT:    [[BF_CLEAR71:%.*]] = and i32 [[BF_LOAD68]], -33552385, !dbg [[DBG90]]
// AARCH64-NEXT:    [[BF_SET72:%.*]] = or i32 [[BF_CLEAR71]], [[BF_SHL70]], !dbg [[DBG90]]
// AARCH64-NEXT:    store i32 [[BF_SET72]], i32* [[ATOMIC_TEMP67]], align 4, !dbg [[DBG90]]
// AARCH64-NEXT:    [[TMP75:%.*]] = load i32, i32* [[ATOMIC_TEMP67]], align 4, !dbg [[DBG90]]
// AARCH64-NEXT:    [[TMP76:%.*]] = cmpxchg i32* getelementptr inbounds ([[STRUCT_BITFIELDS3]], %struct.BitFields3* @bfx3, i32 0, i32 0), i32 [[TMP74]], i32 [[TMP75]] monotonic monotonic, align 4, !dbg [[DBG90]]
// AARCH64-NEXT:    [[TMP77]] = extractvalue { i32, i1 } [[TMP76]], 0, !dbg [[DBG90]]
// AARCH64-NEXT:    [[TMP78:%.*]] = extractvalue { i32, i1 } [[TMP76]], 1, !dbg [[DBG90]]
// AARCH64-NEXT:    br i1 [[TMP78]], label [[ATOMIC_EXIT73:%.*]], label [[ATOMIC_CONT66]], !dbg [[DBG90]]
// AARCH64:       atomic_exit73:
// AARCH64-NEXT:    [[TMP79:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG91:![0-9]+]]
// AARCH64-NEXT:    [[CONV74:%.*]] = fptosi fp128 [[TMP79]] to i32, !dbg [[DBG91]]
// AARCH64-NEXT:    [[TMP80:%.*]] = bitcast i32* [[ATOMIC_TEMP75]] to i24*, !dbg [[DBG92:![0-9]+]]
// AARCH64-NEXT:    [[TMP81:%.*]] = bitcast i24* [[TMP80]] to i8*, !dbg [[DBG92]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 3, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i64 1), i8* noundef [[TMP81]], i32 noundef 0), !dbg [[DBG92]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT76:%.*]], !dbg [[DBG92]]
// AARCH64:       atomic_cont76:
// AARCH64-NEXT:    [[TMP82:%.*]] = bitcast i32* [[ATOMIC_TEMP77]] to i24*, !dbg [[DBG92]]
// AARCH64-NEXT:    [[TMP83:%.*]] = load i24, i24* [[TMP80]], align 1, !dbg [[DBG92]]
// AARCH64-NEXT:    store i24 [[TMP83]], i24* [[TMP82]], align 1, !dbg [[DBG92]]
// AARCH64-NEXT:    [[TMP84:%.*]] = trunc i32 [[CONV74]] to i24, !dbg [[DBG92]]
// AARCH64-NEXT:    [[BF_LOAD78:%.*]] = load i24, i24* [[TMP82]], align 1, !dbg [[DBG92]]
// AARCH64-NEXT:    [[BF_VALUE79:%.*]] = and i24 [[TMP84]], 16383, !dbg [[DBG92]]
// AARCH64-NEXT:    [[BF_SHL80:%.*]] = shl i24 [[BF_VALUE79]], 3, !dbg [[DBG92]]
// AARCH64-NEXT:    [[BF_CLEAR81:%.*]] = and i24 [[BF_LOAD78]], -131065, !dbg [[DBG92]]
// AARCH64-NEXT:    [[BF_SET82:%.*]] = or i24 [[BF_CLEAR81]], [[BF_SHL80]], !dbg [[DBG92]]
// AARCH64-NEXT:    store i24 [[BF_SET82]], i24* [[TMP82]], align 1, !dbg [[DBG92]]
// AARCH64-NEXT:    [[TMP85:%.*]] = bitcast i24* [[TMP80]] to i8*, !dbg [[DBG92]]
// AARCH64-NEXT:    [[TMP86:%.*]] = bitcast i24* [[TMP82]] to i8*, !dbg [[DBG92]]
// AARCH64-NEXT:    [[CALL83:%.*]] = call i1 @__atomic_compare_exchange(i64 noundef 3, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i64 1), i8* noundef [[TMP85]], i8* noundef [[TMP86]], i32 noundef 0, i32 noundef 0), !dbg [[DBG92]]
// AARCH64-NEXT:    br i1 [[CALL83]], label [[ATOMIC_EXIT84:%.*]], label [[ATOMIC_CONT76]], !dbg [[DBG92]]
// AARCH64:       atomic_exit84:
// AARCH64-NEXT:    [[TMP87:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG93:![0-9]+]]
// AARCH64-NEXT:    [[CONV85:%.*]] = fptosi fp128 [[TMP87]] to i32, !dbg [[DBG93]]
// AARCH64-NEXT:    [[ATOMIC_LOAD86:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG94:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT87:%.*]], !dbg [[DBG94]]
// AARCH64:       atomic_cont87:
// AARCH64-NEXT:    [[TMP88:%.*]] = phi i64 [ [[ATOMIC_LOAD86]], [[ATOMIC_EXIT84]] ], [ [[TMP92:%.*]], [[ATOMIC_CONT87]] ], !dbg [[DBG94]]
// AARCH64-NEXT:    store i64 [[TMP88]], i64* [[ATOMIC_TEMP88]], align 8, !dbg [[DBG94]]
// AARCH64-NEXT:    [[TMP89:%.*]] = zext i32 [[CONV85]] to i64, !dbg [[DBG94]]
// AARCH64-NEXT:    [[BF_LOAD89:%.*]] = load i64, i64* [[ATOMIC_TEMP88]], align 8, !dbg [[DBG94]]
// AARCH64-NEXT:    [[BF_VALUE90:%.*]] = and i64 [[TMP89]], 1, !dbg [[DBG94]]
// AARCH64-NEXT:    [[BF_SHL91:%.*]] = shl i64 [[BF_VALUE90]], 16, !dbg [[DBG94]]
// AARCH64-NEXT:    [[BF_CLEAR92:%.*]] = and i64 [[BF_LOAD89]], -65537, !dbg [[DBG94]]
// AARCH64-NEXT:    [[BF_SET93:%.*]] = or i64 [[BF_CLEAR92]], [[BF_SHL91]], !dbg [[DBG94]]
// AARCH64-NEXT:    store i64 [[BF_SET93]], i64* [[ATOMIC_TEMP88]], align 8, !dbg [[DBG94]]
// AARCH64-NEXT:    [[TMP90:%.*]] = load i64, i64* [[ATOMIC_TEMP88]], align 8, !dbg [[DBG94]]
// AARCH64-NEXT:    [[TMP91:%.*]] = cmpxchg i64* bitcast (%struct.BitFields4* @bfx4 to i64*), i64 [[TMP88]], i64 [[TMP90]] monotonic monotonic, align 8, !dbg [[DBG94]]
// AARCH64-NEXT:    [[TMP92]] = extractvalue { i64, i1 } [[TMP91]], 0, !dbg [[DBG94]]
// AARCH64-NEXT:    [[TMP93:%.*]] = extractvalue { i64, i1 } [[TMP91]], 1, !dbg [[DBG94]]
// AARCH64-NEXT:    br i1 [[TMP93]], label [[ATOMIC_EXIT94:%.*]], label [[ATOMIC_CONT87]], !dbg [[DBG94]]
// AARCH64:       atomic_exit94:
// AARCH64-NEXT:    [[TMP94:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG95:![0-9]+]]
// AARCH64-NEXT:    [[CONV95:%.*]] = fptosi fp128 [[TMP94]] to i32, !dbg [[DBG95]]
// AARCH64-NEXT:    [[ATOMIC_LOAD96:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) monotonic, align 1, !dbg [[DBG96:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT97:%.*]], !dbg [[DBG96]]
// AARCH64:       atomic_cont97:
// AARCH64-NEXT:    [[TMP95:%.*]] = phi i8 [ [[ATOMIC_LOAD96]], [[ATOMIC_EXIT94]] ], [ [[TMP100:%.*]], [[ATOMIC_CONT97]] ], !dbg [[DBG96]]
// AARCH64-NEXT:    [[TMP96:%.*]] = bitcast i32* [[ATOMIC_TEMP98]] to i8*, !dbg [[DBG96]]
// AARCH64-NEXT:    store i8 [[TMP95]], i8* [[TMP96]], align 1, !dbg [[DBG96]]
// AARCH64-NEXT:    [[TMP97:%.*]] = trunc i32 [[CONV95]] to i8, !dbg [[DBG96]]
// AARCH64-NEXT:    [[BF_LOAD99:%.*]] = load i8, i8* [[TMP96]], align 1, !dbg [[DBG96]]
// AARCH64-NEXT:    [[BF_VALUE100:%.*]] = and i8 [[TMP97]], 1, !dbg [[DBG96]]
// AARCH64-NEXT:    [[BF_CLEAR101:%.*]] = and i8 [[BF_LOAD99]], -2, !dbg [[DBG96]]
// AARCH64-NEXT:    [[BF_SET102:%.*]] = or i8 [[BF_CLEAR101]], [[BF_VALUE100]], !dbg [[DBG96]]
// AARCH64-NEXT:    store i8 [[BF_SET102]], i8* [[TMP96]], align 1, !dbg [[DBG96]]
// AARCH64-NEXT:    [[TMP98:%.*]] = load i8, i8* [[TMP96]], align 1, !dbg [[DBG96]]
// AARCH64-NEXT:    [[TMP99:%.*]] = cmpxchg i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2), i8 [[TMP95]], i8 [[TMP98]] monotonic monotonic, align 1, !dbg [[DBG96]]
// AARCH64-NEXT:    [[TMP100]] = extractvalue { i8, i1 } [[TMP99]], 0, !dbg [[DBG96]]
// AARCH64-NEXT:    [[TMP101:%.*]] = extractvalue { i8, i1 } [[TMP99]], 1, !dbg [[DBG96]]
// AARCH64-NEXT:    br i1 [[TMP101]], label [[ATOMIC_EXIT103:%.*]], label [[ATOMIC_CONT97]], !dbg [[DBG96]]
// AARCH64:       atomic_exit103:
// AARCH64-NEXT:    [[TMP102:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG97:![0-9]+]]
// AARCH64-NEXT:    [[CONV104:%.*]] = fptosi fp128 [[TMP102]] to i64, !dbg [[DBG97]]
// AARCH64-NEXT:    [[ATOMIC_LOAD105:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG98:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT106:%.*]], !dbg [[DBG98]]
// AARCH64:       atomic_cont106:
// AARCH64-NEXT:    [[TMP103:%.*]] = phi i64 [ [[ATOMIC_LOAD105]], [[ATOMIC_EXIT103]] ], [ [[TMP106:%.*]], [[ATOMIC_CONT106]] ], !dbg [[DBG98]]
// AARCH64-NEXT:    store i64 [[TMP103]], i64* [[ATOMIC_TEMP107]], align 8, !dbg [[DBG98]]
// AARCH64-NEXT:    [[BF_LOAD108:%.*]] = load i64, i64* [[ATOMIC_TEMP107]], align 8, !dbg [[DBG98]]
// AARCH64-NEXT:    [[BF_VALUE109:%.*]] = and i64 [[CONV104]], 127, !dbg [[DBG98]]
// AARCH64-NEXT:    [[BF_SHL110:%.*]] = shl i64 [[BF_VALUE109]], 17, !dbg [[DBG98]]
// AARCH64-NEXT:    [[BF_CLEAR111:%.*]] = and i64 [[BF_LOAD108]], -16646145, !dbg [[DBG98]]
// AARCH64-NEXT:    [[BF_SET112:%.*]] = or i64 [[BF_CLEAR111]], [[BF_SHL110]], !dbg [[DBG98]]
// AARCH64-NEXT:    store i64 [[BF_SET112]], i64* [[ATOMIC_TEMP107]], align 8, !dbg [[DBG98]]
// AARCH64-NEXT:    [[TMP104:%.*]] = load i64, i64* [[ATOMIC_TEMP107]], align 8, !dbg [[DBG98]]
// AARCH64-NEXT:    [[TMP105:%.*]] = cmpxchg i64* bitcast (%struct.BitFields4* @bfx4 to i64*), i64 [[TMP103]], i64 [[TMP104]] monotonic monotonic, align 8, !dbg [[DBG98]]
// AARCH64-NEXT:    [[TMP106]] = extractvalue { i64, i1 } [[TMP105]], 0, !dbg [[DBG98]]
// AARCH64-NEXT:    [[TMP107:%.*]] = extractvalue { i64, i1 } [[TMP105]], 1, !dbg [[DBG98]]
// AARCH64-NEXT:    br i1 [[TMP107]], label [[ATOMIC_EXIT113:%.*]], label [[ATOMIC_CONT106]], !dbg [[DBG98]]
// AARCH64:       atomic_exit113:
// AARCH64-NEXT:    [[TMP108:%.*]] = load fp128, fp128* @ldv, align 16, !dbg [[DBG99:![0-9]+]]
// AARCH64-NEXT:    [[CONV114:%.*]] = fptosi fp128 [[TMP108]] to i64, !dbg [[DBG99]]
// AARCH64-NEXT:    [[ATOMIC_LOAD115:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) monotonic, align 1, !dbg [[DBG100:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT116:%.*]], !dbg [[DBG100]]
// AARCH64:       atomic_cont116:
// AARCH64-NEXT:    [[TMP109:%.*]] = phi i8 [ [[ATOMIC_LOAD115]], [[ATOMIC_EXIT113]] ], [ [[TMP114:%.*]], [[ATOMIC_CONT116]] ], !dbg [[DBG100]]
// AARCH64-NEXT:    [[TMP110:%.*]] = bitcast i64* [[ATOMIC_TEMP117]] to i8*, !dbg [[DBG100]]
// AARCH64-NEXT:    store i8 [[TMP109]], i8* [[TMP110]], align 1, !dbg [[DBG100]]
// AARCH64-NEXT:    [[TMP111:%.*]] = trunc i64 [[CONV114]] to i8, !dbg [[DBG100]]
// AARCH64-NEXT:    [[BF_LOAD118:%.*]] = load i8, i8* [[TMP110]], align 1, !dbg [[DBG100]]
// AARCH64-NEXT:    [[BF_VALUE119:%.*]] = and i8 [[TMP111]], 127, !dbg [[DBG100]]
// AARCH64-NEXT:    [[BF_SHL120:%.*]] = shl i8 [[BF_VALUE119]], 1, !dbg [[DBG100]]
// AARCH64-NEXT:    [[BF_CLEAR121:%.*]] = and i8 [[BF_LOAD118]], 1, !dbg [[DBG100]]
// AARCH64-NEXT:    [[BF_SET122:%.*]] = or i8 [[BF_CLEAR121]], [[BF_SHL120]], !dbg [[DBG100]]
// AARCH64-NEXT:    store i8 [[BF_SET122]], i8* [[TMP110]], align 1, !dbg [[DBG100]]
// AARCH64-NEXT:    [[TMP112:%.*]] = load i8, i8* [[TMP110]], align 1, !dbg [[DBG100]]
// AARCH64-NEXT:    [[TMP113:%.*]] = cmpxchg i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2), i8 [[TMP109]], i8 [[TMP112]] monotonic monotonic, align 1, !dbg [[DBG100]]
// AARCH64-NEXT:    [[TMP114]] = extractvalue { i8, i1 } [[TMP113]], 0, !dbg [[DBG100]]
// AARCH64-NEXT:    [[TMP115:%.*]] = extractvalue { i8, i1 } [[TMP113]], 1, !dbg [[DBG100]]
// AARCH64-NEXT:    br i1 [[TMP115]], label [[ATOMIC_EXIT123:%.*]], label [[ATOMIC_CONT116]], !dbg [[DBG100]]
// AARCH64:       atomic_exit123:
// AARCH64-NEXT:    [[TMP116:%.*]] = load i64, i64* @ulv, align 8, !dbg [[DBG101:![0-9]+]]
// AARCH64-NEXT:    [[CONV124:%.*]] = uitofp i64 [[TMP116]] to float, !dbg [[DBG101]]
// AARCH64-NEXT:    [[ATOMIC_LOAD125:%.*]] = load atomic i64, i64* bitcast (<2 x float>* @float2x to i64*) monotonic, align 8, !dbg [[DBG102:![0-9]+]]
// AARCH64-NEXT:    br label [[ATOMIC_CONT126:%.*]], !dbg [[DBG102]]
// AARCH64:       atomic_cont126:
// AARCH64-NEXT:    [[TMP117:%.*]] = phi i64 [ [[ATOMIC_LOAD125]], [[ATOMIC_EXIT123]] ], [ [[TMP123:%.*]], [[ATOMIC_CONT126]] ], !dbg [[DBG102]]
// AARCH64-NEXT:    [[TMP118:%.*]] = bitcast <2 x float>* [[ATOMIC_TEMP127]] to i64*, !dbg [[DBG102]]
// AARCH64-NEXT:    store i64 [[TMP117]], i64* [[TMP118]], align 8, !dbg [[DBG102]]
// AARCH64-NEXT:    [[TMP119:%.*]] = load <2 x float>, <2 x float>* [[ATOMIC_TEMP127]], align 8, !dbg [[DBG102]]
// AARCH64-NEXT:    [[TMP120:%.*]] = insertelement <2 x float> [[TMP119]], float [[CONV124]], i64 0, !dbg [[DBG102]]
// AARCH64-NEXT:    store <2 x float> [[TMP120]], <2 x float>* [[ATOMIC_TEMP127]], align 8, !dbg [[DBG102]]
// AARCH64-NEXT:    [[TMP121:%.*]] = load i64, i64* [[TMP118]], align 8, !dbg [[DBG102]]
// AARCH64-NEXT:    [[TMP122:%.*]] = cmpxchg i64* bitcast (<2 x float>* @float2x to i64*), i64 [[TMP117]], i64 [[TMP121]] monotonic monotonic, align 8, !dbg [[DBG102]]
// AARCH64-NEXT:    [[TMP123]] = extractvalue { i64, i1 } [[TMP122]], 0, !dbg [[DBG102]]
// AARCH64-NEXT:    [[TMP124:%.*]] = extractvalue { i64, i1 } [[TMP122]], 1, !dbg [[DBG102]]
// AARCH64-NEXT:    br i1 [[TMP124]], label [[ATOMIC_EXIT128:%.*]], label [[ATOMIC_CONT126]], !dbg [[DBG102]]
// AARCH64:       atomic_exit128:
// AARCH64-NEXT:    ret i32 0, !dbg [[DBG103:![0-9]+]]
//
