// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics
template<typename T> T foo() { return T(); }

// LIN64-LABEL: @_Z3bari(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[I:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I1:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I3:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I4:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I5:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I6:%.*]] = alloca i32, align 4
// LIN64-NEXT:    store i32 [[N:%.*]], ptr [[N_ADDR]], align 4
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG9:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG10:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG10]]
// LIN64-NEXT:    store ptr [[TMP2]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG10]]
// LIN64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP1]], align 16, !dbg [[DBG10]]
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG10]]
// LIN64-NEXT:    store i32 0, ptr [[I]], align 4, !dbg [[DBG11:![0-9]+]]
// LIN64-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooIiET_v(), !dbg [[DBG12:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([9 x i8] c"TASKLOOP\00"), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub), "QUAL.OSS.LOOP.STEP"(ptr @compute_step), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[CALL]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]), !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    store i32 0, ptr [[I1]], align 4, !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    [[CALL2:%.*]] = call noundef i32 @_Z3fooIiET_v(), !dbg [[DBG16:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([13 x i8] c"TASKLOOP.FOR\00"), "QUAL.OSS.PRIVATE"(ptr [[I1]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I1]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.1), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.2), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.3), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[CALL2]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG17:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]), !dbg [[DBG18:![0-9]+]]
// LIN64-NEXT:    store i32 0, ptr [[I3]], align 4, !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG20:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([9 x i8] c"TASKLOOP\00"), "QUAL.OSS.PRIVATE"(ptr [[I3]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I3]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.4), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.5), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.6), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP5]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG21:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]), !dbg [[DBG22:![0-9]+]]
// LIN64-NEXT:    store i32 0, ptr [[I4]], align 4, !dbg [[DBG23:![0-9]+]]
// LIN64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG24:![0-9]+]]
// LIN64-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([13 x i8] c"TASKLOOP.FOR\00"), "QUAL.OSS.PRIVATE"(ptr [[I4]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I4]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.7), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.8), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.9), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP7]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG25:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]), !dbg [[DBG26:![0-9]+]]
// LIN64-NEXT:    store i32 0, ptr [[I5]], align 4, !dbg [[DBG27:![0-9]+]]
// LIN64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 1, !dbg [[DBG28:![0-9]+]]
// LIN64-NEXT:    [[TMP9:%.*]] = load i32, ptr [[ARRAYIDX]], align 4, !dbg [[DBG28]]
// LIN64-NEXT:    [[TMP10:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([9 x i8] c"TASKLOOP\00"), "QUAL.OSS.PRIVATE"(ptr [[I5]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I5]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.10), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.11), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.12), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP9]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG29:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP10]]), !dbg [[DBG30:![0-9]+]]
// LIN64-NEXT:    store i32 0, ptr [[I6]], align 4, !dbg [[DBG31:![0-9]+]]
// LIN64-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 1, !dbg [[DBG32:![0-9]+]]
// LIN64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[ARRAYIDX7]], align 4, !dbg [[DBG32]]
// LIN64-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([13 x i8] c"TASKLOOP.FOR\00"), "QUAL.OSS.PRIVATE"(ptr [[I6]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I6]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.13), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.14), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.15), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP11]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG33:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]), !dbg [[DBG34:![0-9]+]]
// LIN64-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG35:![0-9]+]]
// LIN64-NEXT:    call void @llvm.stackrestore(ptr [[TMP13]]), !dbg [[DBG35]]
// LIN64-NEXT:    ret void, !dbg [[DBG35]]
//
// PPC64-LABEL: @_Z3bari(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[I:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I1:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I3:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I4:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I5:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I6:%.*]] = alloca i32, align 4
// PPC64-NEXT:    store i32 [[N:%.*]], ptr [[N_ADDR]], align 4
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG9:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG10:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG10]]
// PPC64-NEXT:    store ptr [[TMP2]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG10]]
// PPC64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP1]], align 4, !dbg [[DBG10]]
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG10]]
// PPC64-NEXT:    store i32 0, ptr [[I]], align 4, !dbg [[DBG11:![0-9]+]]
// PPC64-NEXT:    [[CALL:%.*]] = call noundef signext i32 @_Z3fooIiET_v(), !dbg [[DBG12:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([9 x i8] c"TASKLOOP\00"), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub), "QUAL.OSS.LOOP.STEP"(ptr @compute_step), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[CALL]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]), !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    store i32 0, ptr [[I1]], align 4, !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    [[CALL2:%.*]] = call noundef signext i32 @_Z3fooIiET_v(), !dbg [[DBG16:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([13 x i8] c"TASKLOOP.FOR\00"), "QUAL.OSS.PRIVATE"(ptr [[I1]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I1]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.1), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.2), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.3), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[CALL2]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG17:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]), !dbg [[DBG18:![0-9]+]]
// PPC64-NEXT:    store i32 0, ptr [[I3]], align 4, !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG20:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([9 x i8] c"TASKLOOP\00"), "QUAL.OSS.PRIVATE"(ptr [[I3]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I3]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.4), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.5), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.6), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP5]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG21:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]), !dbg [[DBG22:![0-9]+]]
// PPC64-NEXT:    store i32 0, ptr [[I4]], align 4, !dbg [[DBG23:![0-9]+]]
// PPC64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG24:![0-9]+]]
// PPC64-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([13 x i8] c"TASKLOOP.FOR\00"), "QUAL.OSS.PRIVATE"(ptr [[I4]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I4]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.7), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.8), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.9), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP7]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG25:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]), !dbg [[DBG26:![0-9]+]]
// PPC64-NEXT:    store i32 0, ptr [[I5]], align 4, !dbg [[DBG27:![0-9]+]]
// PPC64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 1, !dbg [[DBG28:![0-9]+]]
// PPC64-NEXT:    [[TMP9:%.*]] = load i32, ptr [[ARRAYIDX]], align 4, !dbg [[DBG28]]
// PPC64-NEXT:    [[TMP10:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([9 x i8] c"TASKLOOP\00"), "QUAL.OSS.PRIVATE"(ptr [[I5]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I5]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.10), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.11), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.12), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP9]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG29:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP10]]), !dbg [[DBG30:![0-9]+]]
// PPC64-NEXT:    store i32 0, ptr [[I6]], align 4, !dbg [[DBG31:![0-9]+]]
// PPC64-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 1, !dbg [[DBG32:![0-9]+]]
// PPC64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[ARRAYIDX7]], align 4, !dbg [[DBG32]]
// PPC64-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([13 x i8] c"TASKLOOP.FOR\00"), "QUAL.OSS.PRIVATE"(ptr [[I6]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I6]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.13), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.14), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.15), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP11]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG33:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]), !dbg [[DBG34:![0-9]+]]
// PPC64-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG35:![0-9]+]]
// PPC64-NEXT:    call void @llvm.stackrestore(ptr [[TMP13]]), !dbg [[DBG35]]
// PPC64-NEXT:    ret void, !dbg [[DBG35]]
//
// AARCH64-LABEL: @_Z3bari(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[I:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I1:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I3:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I4:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I5:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I6:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    store i32 [[N:%.*]], ptr [[N_ADDR]], align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG14]]
// AARCH64-NEXT:    store ptr [[TMP2]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG14]]
// AARCH64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP1]], align 4, !dbg [[DBG14]]
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG14]]
// AARCH64-NEXT:    store i32 0, ptr [[I]], align 4, !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooIiET_v(), !dbg [[DBG16:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([9 x i8] c"TASKLOOP\00"), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub), "QUAL.OSS.LOOP.STEP"(ptr @compute_step), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[CALL]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG17:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]), !dbg [[DBG18:![0-9]+]]
// AARCH64-NEXT:    store i32 0, ptr [[I1]], align 4, !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    [[CALL2:%.*]] = call noundef i32 @_Z3fooIiET_v(), !dbg [[DBG20:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([13 x i8] c"TASKLOOP.FOR\00"), "QUAL.OSS.PRIVATE"(ptr [[I1]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I1]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.1), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.2), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.3), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[CALL2]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG21:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]), !dbg [[DBG22:![0-9]+]]
// AARCH64-NEXT:    store i32 0, ptr [[I3]], align 4, !dbg [[DBG23:![0-9]+]]
// AARCH64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG24:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([9 x i8] c"TASKLOOP\00"), "QUAL.OSS.PRIVATE"(ptr [[I3]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I3]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.4), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.5), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.6), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP5]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG25:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]), !dbg [[DBG26:![0-9]+]]
// AARCH64-NEXT:    store i32 0, ptr [[I4]], align 4, !dbg [[DBG27:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG28:![0-9]+]]
// AARCH64-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([13 x i8] c"TASKLOOP.FOR\00"), "QUAL.OSS.PRIVATE"(ptr [[I4]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I4]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.7), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.8), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.9), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP7]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG29:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]), !dbg [[DBG30:![0-9]+]]
// AARCH64-NEXT:    store i32 0, ptr [[I5]], align 4, !dbg [[DBG31:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 1, !dbg [[DBG32:![0-9]+]]
// AARCH64-NEXT:    [[TMP9:%.*]] = load i32, ptr [[ARRAYIDX]], align 4, !dbg [[DBG32]]
// AARCH64-NEXT:    [[TMP10:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([9 x i8] c"TASKLOOP\00"), "QUAL.OSS.PRIVATE"(ptr [[I5]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I5]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.10), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.11), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.12), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP9]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG33:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP10]]), !dbg [[DBG34:![0-9]+]]
// AARCH64-NEXT:    store i32 0, ptr [[I6]], align 4, !dbg [[DBG35:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 1, !dbg [[DBG36:![0-9]+]]
// AARCH64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[ARRAYIDX7]], align 4, !dbg [[DBG36]]
// AARCH64-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([13 x i8] c"TASKLOOP.FOR\00"), "QUAL.OSS.PRIVATE"(ptr [[I6]], i32 undef), "QUAL.OSS.LOOP.IND.VAR"(ptr [[I6]]), "QUAL.OSS.LOOP.LOWER.BOUND"(ptr @compute_lb.13), "QUAL.OSS.LOOP.UPPER.BOUND"(ptr @compute_ub.14), "QUAL.OSS.LOOP.STEP"(ptr @compute_step.15), "QUAL.OSS.LOOP.GRAINSIZE"(i32 [[TMP11]]), "QUAL.OSS.LOOP.TYPE"(i64 0, i64 1, i64 1, i64 1, i64 1) ], !dbg [[DBG37:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]), !dbg [[DBG38:![0-9]+]]
// AARCH64-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG39:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.stackrestore(ptr [[TMP13]]), !dbg [[DBG39]]
// AARCH64-NEXT:    ret void, !dbg [[DBG39]]
//
void bar(int n) {
    int vla[n];
    #pragma oss taskloop grainsize(foo<int>())
    for (int i = 0; i < 10; ++i) {}
    #pragma oss taskloop for grainsize(foo<int>())
    for (int i = 0; i < 10; ++i) {}
    #pragma oss taskloop grainsize(n)
    for (int i = 0; i < 10; ++i) {}
    #pragma oss taskloop for grainsize(n)
    for (int i = 0; i < 10; ++i) {}
    #pragma oss taskloop grainsize(vla[1])
    for (int i = 0; i < 10; ++i) {}
    #pragma oss taskloop for grainsize(vla[1])
    for (int i = 0; i < 10; ++i) {}
}


