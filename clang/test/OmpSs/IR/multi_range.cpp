// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs
// RUN: %clang_cc1 -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics

int v[10][20];
template<typename T>
void constants() {
  #pragma oss task in( { v[i][j], i = 0:10-1, j=0;20 } )
  {}
  #pragma oss task in( { v[i][j], i = 0:10-1:1, j=0;20:1 } )
  {}
}

template<typename T>
void nonconstants() {
  T lb1, ub1, step1;
  T lb2, ub2, step2;
  #pragma oss task in( { v[i][j], i = lb1:ub1, j=lb2;ub2 } )
  {}
  #pragma oss task in( { v[i][j], i = lb1:ub1:step1, j=lb2;ub2:step2 } )
  {}
}

void f() {
  constants<int>();
  nonconstants<int>();
  nonconstants<short>();
}

// LIN64-LABEL: define {{[^@]+}}@_Z1fv
// LIN64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    call void @_Z9constantsIiEvv(), !dbg [[DBG9:![0-9]+]]
// LIN64-NEXT:    call void @_Z12nonconstantsIiEvv(), !dbg [[DBG10:![0-9]+]]
// LIN64-NEXT:    call void @_Z12nonconstantsIsEvv(), !dbg [[DBG11:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG12:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@_Z9constantsIiEvv
// LIN64-SAME: () #[[ATTR0]] comdat !dbg [[DBG13:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[I:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[J:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I1:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[J2:%.*]] = alloca i32, align 4
// LIN64-NEXT:    store i32 0, ptr [[I]], align 4, !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    store i32 0, ptr [[J]], align 4, !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I]], ptr [[J]], ptr @compute_dep, ptr [[I]], ptr [[J]], ptr @v, [32 x i8] c"{ v[i][j], i = 0:10-1, j=0
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG17:![0-9]+]]
// LIN64-NEXT:    store i32 0, ptr [[I1]], align 4, !dbg [[DBG18:![0-9]+]]
// LIN64-NEXT:    store i32 0, ptr [[J2]], align 4, !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I1]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J2]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I1]], ptr [[J2]], ptr @compute_dep.2, ptr [[I1]], ptr [[J2]], ptr @v, [36 x i8] c"{ v[i][j], i = 0:10-1:1, j=0
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]), !dbg [[DBG21:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG22:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@_Z12nonconstantsIiEvv
// LIN64-SAME: () #[[ATTR0]] comdat !dbg [[DBG23:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[LB1:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[UB1:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[STEP1:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[LB2:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[UB2:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[STEP2:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[J:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I1:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[J2:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG24:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4, !dbg [[DBG25:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG26:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP1]], ptr [[J]], align 4, !dbg [[DBG27:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I]], ptr [[J]], ptr @compute_dep.4, ptr [[I]], ptr [[LB1]], ptr [[UB1]], ptr [[J]], ptr [[LB2]], ptr [[UB2]], ptr @v, [36 x i8] c"{ v[i][j], i = lb1:ub1, j=lb2
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG29:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG30:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP3]], ptr [[I1]], align 4, !dbg [[DBG31:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG32:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP4]], ptr [[J2]], align 4, !dbg [[DBG33:![0-9]+]]
// LIN64-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I1]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP2]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I1]], ptr [[J2]], ptr @compute_dep.6, ptr [[I1]], ptr [[LB1]], ptr [[UB1]], ptr [[STEP1]], ptr [[J2]], ptr [[LB2]], ptr [[UB2]], ptr [[STEP2]], ptr @v, [48 x i8] c"{ v[i][j], i = lb1:ub1:step1, j=lb2
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]), !dbg [[DBG35:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG36:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@_Z12nonconstantsIsEvv
// LIN64-SAME: () #[[ATTR0]] comdat !dbg [[DBG37:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[LB1:%.*]] = alloca i16, align 2
// LIN64-NEXT:    [[UB1:%.*]] = alloca i16, align 2
// LIN64-NEXT:    [[STEP1:%.*]] = alloca i16, align 2
// LIN64-NEXT:    [[LB2:%.*]] = alloca i16, align 2
// LIN64-NEXT:    [[UB2:%.*]] = alloca i16, align 2
// LIN64-NEXT:    [[STEP2:%.*]] = alloca i16, align 2
// LIN64-NEXT:    [[I:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[J:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I2:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[J4:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[TMP0:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG38:![0-9]+]]
// LIN64-NEXT:    [[CONV:%.*]] = sext i16 [[TMP0]] to i32, !dbg [[DBG38]]
// LIN64-NEXT:    store i32 [[CONV]], ptr [[I]], align 4, !dbg [[DBG39:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG40:![0-9]+]]
// LIN64-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP1]] to i32, !dbg [[DBG40]]
// LIN64-NEXT:    store i32 [[CONV1]], ptr [[J]], align 4, !dbg [[DBG41:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i16 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I]], ptr [[J]], ptr @compute_dep.8, ptr [[I]], ptr [[LB1]], ptr [[UB1]], ptr [[J]], ptr [[LB2]], ptr [[UB2]], ptr @v, [36 x i8] c"{ v[i][j], i = lb1:ub1, j=lb2
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG43:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG44:![0-9]+]]
// LIN64-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP3]] to i32, !dbg [[DBG44]]
// LIN64-NEXT:    store i32 [[CONV3]], ptr [[I2]], align 4, !dbg [[DBG45:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG46:![0-9]+]]
// LIN64-NEXT:    [[CONV5:%.*]] = sext i16 [[TMP4]] to i32, !dbg [[DBG46]]
// LIN64-NEXT:    store i32 [[CONV5]], ptr [[J4]], align 4, !dbg [[DBG47:![0-9]+]]
// LIN64-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I2]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J4]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP2]], i16 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I2]], ptr [[J4]], ptr @compute_dep.10, ptr [[I2]], ptr [[LB1]], ptr [[UB1]], ptr [[STEP1]], ptr [[J4]], ptr [[LB2]], ptr [[UB2]], ptr [[STEP2]], ptr @v, [48 x i8] c"{ v[i][j], i = lb1:ub1:step1, j=lb2
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]), !dbg [[DBG49:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG50:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep
// LIN64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], i64 [[TMP0:%.*]]) #[[ATTR2:[0-9]+]] !dbg [[DBG51:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 4
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// LIN64-NEXT:      i64 0, label [[TMP4:%.*]]
// LIN64-NEXT:      i64 1, label [[TMP10:%.*]]
// LIN64-NEXT:    ]
// LIN64:       1:
// LIN64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 4, !dbg [[DBG53:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP2]], !dbg [[DBG53]]
// LIN64:       3:
// LIN64-NEXT:    br label [[TMP1:%.*]]
// LIN64:       4:
// LIN64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG55:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store i32 0, ptr [[TMP6]], align 4
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i32 [[TMP5]], ptr [[TMP7]], align 4
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i32 9, ptr [[TMP8]], align 4
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i32 1, ptr [[TMP9]], align 4
// LIN64-NEXT:    br label [[TMP1]]
// LIN64:       10:
// LIN64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG56:![0-9]+]]
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i32 0, ptr [[TMP12]], align 4
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i32 [[TMP11]], ptr [[TMP13]], align 4
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i32 19, ptr [[TMP14]], align 4
// LIN64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 7
// LIN64-NEXT:    store i32 1, ptr [[TMP15]], align 4
// LIN64-NEXT:    br label [[TMP1]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.1
// LIN64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[V:%.*]]) #[[ATTR2]] !dbg [[DBG57:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG58:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG60:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// LIN64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG61:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// LIN64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// LIN64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8, !dbg [[DBG61]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP15]], !dbg [[DBG61]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.2
// LIN64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], i64 [[TMP0:%.*]]) #[[ATTR2]] !dbg [[DBG62:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 4
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// LIN64-NEXT:      i64 0, label [[TMP4:%.*]]
// LIN64-NEXT:      i64 1, label [[TMP10:%.*]]
// LIN64-NEXT:    ]
// LIN64:       1:
// LIN64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 4, !dbg [[DBG63:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP2]], !dbg [[DBG63]]
// LIN64:       3:
// LIN64-NEXT:    br label [[TMP1:%.*]]
// LIN64:       4:
// LIN64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG65:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store i32 0, ptr [[TMP6]], align 4
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i32 [[TMP5]], ptr [[TMP7]], align 4
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i32 9, ptr [[TMP8]], align 4
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i32 1, ptr [[TMP9]], align 4
// LIN64-NEXT:    br label [[TMP1]]
// LIN64:       10:
// LIN64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG66:![0-9]+]]
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i32 0, ptr [[TMP12]], align 4
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i32 [[TMP11]], ptr [[TMP13]], align 4
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i32 19, ptr [[TMP14]], align 4
// LIN64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 7
// LIN64-NEXT:    store i32 1, ptr [[TMP15]], align 4
// LIN64-NEXT:    br label [[TMP1]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.3
// LIN64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[V:%.*]]) #[[ATTR2]] !dbg [[DBG67:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_2:%.*]], align 8
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG68:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG70:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// LIN64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG71:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// LIN64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// LIN64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], align 8, !dbg [[DBG71]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_2]] [[TMP15]], !dbg [[DBG71]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.4
// LIN64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], i64 [[TMP0:%.*]]) #[[ATTR2]] !dbg [[DBG72:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_3:%.*]], align 4
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// LIN64-NEXT:      i64 0, label [[TMP4:%.*]]
// LIN64-NEXT:      i64 1, label [[TMP12:%.*]]
// LIN64-NEXT:    ]
// LIN64:       1:
// LIN64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], align 4, !dbg [[DBG73:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_3]] [[TMP2]], !dbg [[DBG73]]
// LIN64:       3:
// LIN64-NEXT:    br label [[TMP1:%.*]]
// LIN64:       4:
// LIN64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG75:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG76:![0-9]+]]
// LIN64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[UB1]], align 4, !dbg [[DBG77:![0-9]+]]
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store i32 [[TMP5]], ptr [[TMP8]], align 4
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i32 [[TMP6]], ptr [[TMP9]], align 4
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i32 [[TMP7]], ptr [[TMP10]], align 4
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i32 1, ptr [[TMP11]], align 4
// LIN64-NEXT:    br label [[TMP1]]
// LIN64:       12:
// LIN64-NEXT:    [[TMP13:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG78:![0-9]+]]
// LIN64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG79:![0-9]+]]
// LIN64-NEXT:    [[TMP15:%.*]] = load i32, ptr [[UB2]], align 4, !dbg [[DBG73]]
// LIN64-NEXT:    [[TMP16:%.*]] = add i32 [[TMP13]], [[TMP15]]
// LIN64-NEXT:    [[TMP17:%.*]] = add i32 [[TMP16]], -1
// LIN64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i32 [[TMP13]], ptr [[TMP18]], align 4
// LIN64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i32 [[TMP14]], ptr [[TMP19]], align 4
// LIN64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i32 [[TMP17]], ptr [[TMP20]], align 4
// LIN64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 7
// LIN64-NEXT:    store i32 1, ptr [[TMP21]], align 4
// LIN64-NEXT:    br label [[TMP1]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.5
// LIN64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[V:%.*]]) #[[ATTR2]] !dbg [[DBG80:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_4:%.*]], align 8
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG81:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG83:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// LIN64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG84:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// LIN64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// LIN64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], align 8, !dbg [[DBG84]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_4]] [[TMP15]], !dbg [[DBG84]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.6
// LIN64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[STEP1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], ptr [[STEP2:%.*]], i64 [[TMP0:%.*]]) #[[ATTR2]] !dbg [[DBG85:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_5:%.*]], align 4
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// LIN64-NEXT:      i64 0, label [[TMP4:%.*]]
// LIN64-NEXT:      i64 1, label [[TMP13:%.*]]
// LIN64-NEXT:    ]
// LIN64:       1:
// LIN64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], align 4, !dbg [[DBG86:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_5]] [[TMP2]], !dbg [[DBG86]]
// LIN64:       3:
// LIN64-NEXT:    br label [[TMP1:%.*]]
// LIN64:       4:
// LIN64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG88:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG89:![0-9]+]]
// LIN64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[UB1]], align 4, !dbg [[DBG90:![0-9]+]]
// LIN64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[STEP1]], align 4, !dbg [[DBG91:![0-9]+]]
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store i32 [[TMP5]], ptr [[TMP9]], align 4
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i32 [[TMP6]], ptr [[TMP10]], align 4
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i32 [[TMP7]], ptr [[TMP11]], align 4
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i32 [[TMP8]], ptr [[TMP12]], align 4
// LIN64-NEXT:    br label [[TMP1]]
// LIN64:       13:
// LIN64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG92:![0-9]+]]
// LIN64-NEXT:    [[TMP15:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG93:![0-9]+]]
// LIN64-NEXT:    [[TMP16:%.*]] = load i32, ptr [[UB2]], align 4, !dbg [[DBG94:![0-9]+]]
// LIN64-NEXT:    [[TMP17:%.*]] = add i32 [[TMP14]], [[TMP16]]
// LIN64-NEXT:    [[TMP18:%.*]] = add i32 [[TMP17]], -1
// LIN64-NEXT:    [[TMP19:%.*]] = load i32, ptr [[STEP2]], align 4, !dbg [[DBG86]]
// LIN64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i32 [[TMP14]], ptr [[TMP20]], align 4
// LIN64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i32 [[TMP15]], ptr [[TMP21]], align 4
// LIN64-NEXT:    [[TMP22:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i32 [[TMP18]], ptr [[TMP22]], align 4
// LIN64-NEXT:    [[TMP23:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 7
// LIN64-NEXT:    store i32 [[TMP19]], ptr [[TMP23]], align 4
// LIN64-NEXT:    br label [[TMP1]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.7
// LIN64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[STEP1:%.*]], ptr [[STEP2:%.*]], ptr [[V:%.*]]) #[[ATTR2]] !dbg [[DBG95:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_6:%.*]], align 8
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG96:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG98:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// LIN64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG99:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// LIN64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// LIN64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], align 8, !dbg [[DBG99]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_6]] [[TMP15]], !dbg [[DBG99]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.8
// LIN64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], i64 [[TMP0:%.*]]) #[[ATTR2]] !dbg [[DBG100:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_7:%.*]], align 4
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// LIN64-NEXT:      i64 0, label [[TMP4:%.*]]
// LIN64-NEXT:      i64 1, label [[TMP12:%.*]]
// LIN64-NEXT:    ]
// LIN64:       1:
// LIN64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], align 4, !dbg [[DBG101:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_7]] [[TMP2]], !dbg [[DBG101]]
// LIN64:       3:
// LIN64-NEXT:    br label [[TMP1:%.*]]
// LIN64:       4:
// LIN64-NEXT:    [[TMP5:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG103:![0-9]+]]
// LIN64-NEXT:    [[CONV:%.*]] = sext i16 [[TMP5]] to i32, !dbg [[DBG103]]
// LIN64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG104:![0-9]+]]
// LIN64-NEXT:    [[TMP7:%.*]] = load i16, ptr [[UB1]], align 2, !dbg [[DBG105:![0-9]+]]
// LIN64-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP7]] to i32, !dbg [[DBG105]]
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store i32 [[CONV]], ptr [[TMP8]], align 4
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i32 [[TMP6]], ptr [[TMP9]], align 4
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i32 [[CONV1]], ptr [[TMP10]], align 4
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i32 1, ptr [[TMP11]], align 4
// LIN64-NEXT:    br label [[TMP1]]
// LIN64:       12:
// LIN64-NEXT:    [[TMP13:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG106:![0-9]+]]
// LIN64-NEXT:    [[CONV2:%.*]] = sext i16 [[TMP13]] to i32, !dbg [[DBG106]]
// LIN64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG107:![0-9]+]]
// LIN64-NEXT:    [[TMP15:%.*]] = load i16, ptr [[UB2]], align 2, !dbg [[DBG101]]
// LIN64-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP15]] to i32, !dbg [[DBG101]]
// LIN64-NEXT:    [[TMP16:%.*]] = add i32 [[CONV2]], [[CONV3]]
// LIN64-NEXT:    [[TMP17:%.*]] = add i32 [[TMP16]], -1
// LIN64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i32 [[CONV2]], ptr [[TMP18]], align 4
// LIN64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i32 [[TMP14]], ptr [[TMP19]], align 4
// LIN64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i32 [[TMP17]], ptr [[TMP20]], align 4
// LIN64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 7
// LIN64-NEXT:    store i32 1, ptr [[TMP21]], align 4
// LIN64-NEXT:    br label [[TMP1]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.9
// LIN64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[V:%.*]]) #[[ATTR2]] !dbg [[DBG108:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_8:%.*]], align 8
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG109:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG111:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// LIN64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG112:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// LIN64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// LIN64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], align 8, !dbg [[DBG112]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_8]] [[TMP15]], !dbg [[DBG112]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.10
// LIN64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[STEP1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], ptr [[STEP2:%.*]], i64 [[TMP0:%.*]]) #[[ATTR2]] !dbg [[DBG113:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_9:%.*]], align 4
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// LIN64-NEXT:      i64 0, label [[TMP4:%.*]]
// LIN64-NEXT:      i64 1, label [[TMP13:%.*]]
// LIN64-NEXT:    ]
// LIN64:       1:
// LIN64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], align 4, !dbg [[DBG114:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_9]] [[TMP2]], !dbg [[DBG114]]
// LIN64:       3:
// LIN64-NEXT:    br label [[TMP1:%.*]]
// LIN64:       4:
// LIN64-NEXT:    [[TMP5:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG116:![0-9]+]]
// LIN64-NEXT:    [[CONV:%.*]] = sext i16 [[TMP5]] to i32, !dbg [[DBG116]]
// LIN64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG117:![0-9]+]]
// LIN64-NEXT:    [[TMP7:%.*]] = load i16, ptr [[UB1]], align 2, !dbg [[DBG118:![0-9]+]]
// LIN64-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP7]] to i32, !dbg [[DBG118]]
// LIN64-NEXT:    [[TMP8:%.*]] = load i16, ptr [[STEP1]], align 2, !dbg [[DBG119:![0-9]+]]
// LIN64-NEXT:    [[CONV2:%.*]] = sext i16 [[TMP8]] to i32, !dbg [[DBG119]]
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store i32 [[CONV]], ptr [[TMP9]], align 4
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i32 [[TMP6]], ptr [[TMP10]], align 4
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i32 [[CONV1]], ptr [[TMP11]], align 4
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i32 [[CONV2]], ptr [[TMP12]], align 4
// LIN64-NEXT:    br label [[TMP1]]
// LIN64:       13:
// LIN64-NEXT:    [[TMP14:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG120:![0-9]+]]
// LIN64-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP14]] to i32, !dbg [[DBG120]]
// LIN64-NEXT:    [[TMP15:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG121:![0-9]+]]
// LIN64-NEXT:    [[TMP16:%.*]] = load i16, ptr [[UB2]], align 2, !dbg [[DBG122:![0-9]+]]
// LIN64-NEXT:    [[CONV4:%.*]] = sext i16 [[TMP16]] to i32, !dbg [[DBG122]]
// LIN64-NEXT:    [[TMP17:%.*]] = add i32 [[CONV3]], [[CONV4]]
// LIN64-NEXT:    [[TMP18:%.*]] = add i32 [[TMP17]], -1
// LIN64-NEXT:    [[TMP19:%.*]] = load i16, ptr [[STEP2]], align 2, !dbg [[DBG114]]
// LIN64-NEXT:    [[CONV5:%.*]] = sext i16 [[TMP19]] to i32, !dbg [[DBG114]]
// LIN64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i32 [[CONV3]], ptr [[TMP20]], align 4
// LIN64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i32 [[TMP15]], ptr [[TMP21]], align 4
// LIN64-NEXT:    [[TMP22:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i32 [[TMP18]], ptr [[TMP22]], align 4
// LIN64-NEXT:    [[TMP23:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 7
// LIN64-NEXT:    store i32 [[CONV5]], ptr [[TMP23]], align 4
// LIN64-NEXT:    br label [[TMP1]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.11
// LIN64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[STEP1:%.*]], ptr [[STEP2:%.*]], ptr [[V:%.*]]) #[[ATTR2]] !dbg [[DBG123:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_10:%.*]], align 8
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// LIN64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// LIN64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG124:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG126:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// LIN64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG127:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// LIN64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// LIN64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], align 8, !dbg [[DBG127]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_10]] [[TMP15]], !dbg [[DBG127]]
//
//
// PPC64-LABEL: define {{[^@]+}}@_Z1fv
// PPC64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    call void @_Z9constantsIiEvv(), !dbg [[DBG9:![0-9]+]]
// PPC64-NEXT:    call void @_Z12nonconstantsIiEvv(), !dbg [[DBG10:![0-9]+]]
// PPC64-NEXT:    call void @_Z12nonconstantsIsEvv(), !dbg [[DBG11:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG12:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@_Z9constantsIiEvv
// PPC64-SAME: () #[[ATTR0]] comdat !dbg [[DBG13:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[I:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[J:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I1:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[J2:%.*]] = alloca i32, align 4
// PPC64-NEXT:    store i32 0, ptr [[I]], align 4, !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    store i32 0, ptr [[J]], align 4, !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I]], ptr [[J]], ptr @compute_dep, ptr [[I]], ptr [[J]], ptr @v, [32 x i8] c"{ v[i][j], i = 0:10-1, j=0
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG17:![0-9]+]]
// PPC64-NEXT:    store i32 0, ptr [[I1]], align 4, !dbg [[DBG18:![0-9]+]]
// PPC64-NEXT:    store i32 0, ptr [[J2]], align 4, !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I1]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J2]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I1]], ptr [[J2]], ptr @compute_dep.2, ptr [[I1]], ptr [[J2]], ptr @v, [36 x i8] c"{ v[i][j], i = 0:10-1:1, j=0
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]), !dbg [[DBG21:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG22:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@_Z12nonconstantsIiEvv
// PPC64-SAME: () #[[ATTR0]] comdat !dbg [[DBG23:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[LB1:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[UB1:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[STEP1:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[LB2:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[UB2:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[STEP2:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[J:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I1:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[J2:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG24:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4, !dbg [[DBG25:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG26:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP1]], ptr [[J]], align 4, !dbg [[DBG27:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I]], ptr [[J]], ptr @compute_dep.4, ptr [[I]], ptr [[LB1]], ptr [[UB1]], ptr [[J]], ptr [[LB2]], ptr [[UB2]], ptr @v, [36 x i8] c"{ v[i][j], i = lb1:ub1, j=lb2
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG29:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG30:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP3]], ptr [[I1]], align 4, !dbg [[DBG31:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG32:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP4]], ptr [[J2]], align 4, !dbg [[DBG33:![0-9]+]]
// PPC64-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I1]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP2]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I1]], ptr [[J2]], ptr @compute_dep.6, ptr [[I1]], ptr [[LB1]], ptr [[UB1]], ptr [[STEP1]], ptr [[J2]], ptr [[LB2]], ptr [[UB2]], ptr [[STEP2]], ptr @v, [48 x i8] c"{ v[i][j], i = lb1:ub1:step1, j=lb2
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]), !dbg [[DBG35:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG36:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@_Z12nonconstantsIsEvv
// PPC64-SAME: () #[[ATTR0]] comdat !dbg [[DBG37:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[LB1:%.*]] = alloca i16, align 2
// PPC64-NEXT:    [[UB1:%.*]] = alloca i16, align 2
// PPC64-NEXT:    [[STEP1:%.*]] = alloca i16, align 2
// PPC64-NEXT:    [[LB2:%.*]] = alloca i16, align 2
// PPC64-NEXT:    [[UB2:%.*]] = alloca i16, align 2
// PPC64-NEXT:    [[STEP2:%.*]] = alloca i16, align 2
// PPC64-NEXT:    [[I:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[J:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I2:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[J4:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[TMP0:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG38:![0-9]+]]
// PPC64-NEXT:    [[CONV:%.*]] = sext i16 [[TMP0]] to i32, !dbg [[DBG38]]
// PPC64-NEXT:    store i32 [[CONV]], ptr [[I]], align 4, !dbg [[DBG39:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG40:![0-9]+]]
// PPC64-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP1]] to i32, !dbg [[DBG40]]
// PPC64-NEXT:    store i32 [[CONV1]], ptr [[J]], align 4, !dbg [[DBG41:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i16 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I]], ptr [[J]], ptr @compute_dep.8, ptr [[I]], ptr [[LB1]], ptr [[UB1]], ptr [[J]], ptr [[LB2]], ptr [[UB2]], ptr @v, [36 x i8] c"{ v[i][j], i = lb1:ub1, j=lb2
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG43:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG44:![0-9]+]]
// PPC64-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP3]] to i32, !dbg [[DBG44]]
// PPC64-NEXT:    store i32 [[CONV3]], ptr [[I2]], align 4, !dbg [[DBG45:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG46:![0-9]+]]
// PPC64-NEXT:    [[CONV5:%.*]] = sext i16 [[TMP4]] to i32, !dbg [[DBG46]]
// PPC64-NEXT:    store i32 [[CONV5]], ptr [[J4]], align 4, !dbg [[DBG47:![0-9]+]]
// PPC64-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I2]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J4]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP2]], i16 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I2]], ptr [[J4]], ptr @compute_dep.10, ptr [[I2]], ptr [[LB1]], ptr [[UB1]], ptr [[STEP1]], ptr [[J4]], ptr [[LB2]], ptr [[UB2]], ptr [[STEP2]], ptr @v, [48 x i8] c"{ v[i][j], i = lb1:ub1:step1, j=lb2
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]), !dbg [[DBG49:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG50:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep
// PPC64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG51:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 4
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// PPC64-NEXT:      i64 0, label [[TMP4:%.*]]
// PPC64-NEXT:      i64 1, label [[TMP10:%.*]]
// PPC64-NEXT:    ]
// PPC64:       1:
// PPC64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 4, !dbg [[DBG53:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP2]], !dbg [[DBG53]]
// PPC64:       3:
// PPC64-NEXT:    br label [[TMP1:%.*]]
// PPC64:       4:
// PPC64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG55:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store i32 0, ptr [[TMP6]], align 4
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i32 [[TMP5]], ptr [[TMP7]], align 4
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i32 9, ptr [[TMP8]], align 4
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i32 1, ptr [[TMP9]], align 4
// PPC64-NEXT:    br label [[TMP1]]
// PPC64:       10:
// PPC64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG56:![0-9]+]]
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i32 0, ptr [[TMP12]], align 4
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i32 [[TMP11]], ptr [[TMP13]], align 4
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i32 19, ptr [[TMP14]], align 4
// PPC64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 7
// PPC64-NEXT:    store i32 1, ptr [[TMP15]], align 4
// PPC64-NEXT:    br label [[TMP1]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.1
// PPC64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[V:%.*]]) !dbg [[DBG57:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG58:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG60:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// PPC64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG61:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// PPC64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// PPC64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8, !dbg [[DBG61]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP15]], !dbg [[DBG61]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.2
// PPC64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG62:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 4
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// PPC64-NEXT:      i64 0, label [[TMP4:%.*]]
// PPC64-NEXT:      i64 1, label [[TMP10:%.*]]
// PPC64-NEXT:    ]
// PPC64:       1:
// PPC64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 4, !dbg [[DBG63:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP2]], !dbg [[DBG63]]
// PPC64:       3:
// PPC64-NEXT:    br label [[TMP1:%.*]]
// PPC64:       4:
// PPC64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG65:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store i32 0, ptr [[TMP6]], align 4
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i32 [[TMP5]], ptr [[TMP7]], align 4
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i32 9, ptr [[TMP8]], align 4
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i32 1, ptr [[TMP9]], align 4
// PPC64-NEXT:    br label [[TMP1]]
// PPC64:       10:
// PPC64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG66:![0-9]+]]
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i32 0, ptr [[TMP12]], align 4
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i32 [[TMP11]], ptr [[TMP13]], align 4
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i32 19, ptr [[TMP14]], align 4
// PPC64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 7
// PPC64-NEXT:    store i32 1, ptr [[TMP15]], align 4
// PPC64-NEXT:    br label [[TMP1]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.3
// PPC64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[V:%.*]]) !dbg [[DBG67:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_2:%.*]], align 8
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG68:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG70:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// PPC64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG71:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// PPC64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// PPC64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], align 8, !dbg [[DBG71]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_2]] [[TMP15]], !dbg [[DBG71]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.4
// PPC64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG72:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_3:%.*]], align 4
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// PPC64-NEXT:      i64 0, label [[TMP4:%.*]]
// PPC64-NEXT:      i64 1, label [[TMP12:%.*]]
// PPC64-NEXT:    ]
// PPC64:       1:
// PPC64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], align 4, !dbg [[DBG73:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_3]] [[TMP2]], !dbg [[DBG73]]
// PPC64:       3:
// PPC64-NEXT:    br label [[TMP1:%.*]]
// PPC64:       4:
// PPC64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG75:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG76:![0-9]+]]
// PPC64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[UB1]], align 4, !dbg [[DBG77:![0-9]+]]
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store i32 [[TMP5]], ptr [[TMP8]], align 4
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i32 [[TMP6]], ptr [[TMP9]], align 4
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i32 [[TMP7]], ptr [[TMP10]], align 4
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i32 1, ptr [[TMP11]], align 4
// PPC64-NEXT:    br label [[TMP1]]
// PPC64:       12:
// PPC64-NEXT:    [[TMP13:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG78:![0-9]+]]
// PPC64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG79:![0-9]+]]
// PPC64-NEXT:    [[TMP15:%.*]] = load i32, ptr [[UB2]], align 4, !dbg [[DBG73]]
// PPC64-NEXT:    [[TMP16:%.*]] = add i32 [[TMP13]], [[TMP15]]
// PPC64-NEXT:    [[TMP17:%.*]] = add i32 [[TMP16]], -1
// PPC64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i32 [[TMP13]], ptr [[TMP18]], align 4
// PPC64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i32 [[TMP14]], ptr [[TMP19]], align 4
// PPC64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i32 [[TMP17]], ptr [[TMP20]], align 4
// PPC64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 7
// PPC64-NEXT:    store i32 1, ptr [[TMP21]], align 4
// PPC64-NEXT:    br label [[TMP1]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.5
// PPC64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[V:%.*]]) !dbg [[DBG80:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_4:%.*]], align 8
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG81:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG83:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// PPC64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG84:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// PPC64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// PPC64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], align 8, !dbg [[DBG84]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_4]] [[TMP15]], !dbg [[DBG84]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.6
// PPC64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[STEP1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], ptr [[STEP2:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG85:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_5:%.*]], align 4
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// PPC64-NEXT:      i64 0, label [[TMP4:%.*]]
// PPC64-NEXT:      i64 1, label [[TMP13:%.*]]
// PPC64-NEXT:    ]
// PPC64:       1:
// PPC64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], align 4, !dbg [[DBG86:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_5]] [[TMP2]], !dbg [[DBG86]]
// PPC64:       3:
// PPC64-NEXT:    br label [[TMP1:%.*]]
// PPC64:       4:
// PPC64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG88:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG89:![0-9]+]]
// PPC64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[UB1]], align 4, !dbg [[DBG90:![0-9]+]]
// PPC64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[STEP1]], align 4, !dbg [[DBG91:![0-9]+]]
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store i32 [[TMP5]], ptr [[TMP9]], align 4
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i32 [[TMP6]], ptr [[TMP10]], align 4
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i32 [[TMP7]], ptr [[TMP11]], align 4
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i32 [[TMP8]], ptr [[TMP12]], align 4
// PPC64-NEXT:    br label [[TMP1]]
// PPC64:       13:
// PPC64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG92:![0-9]+]]
// PPC64-NEXT:    [[TMP15:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG93:![0-9]+]]
// PPC64-NEXT:    [[TMP16:%.*]] = load i32, ptr [[UB2]], align 4, !dbg [[DBG94:![0-9]+]]
// PPC64-NEXT:    [[TMP17:%.*]] = add i32 [[TMP14]], [[TMP16]]
// PPC64-NEXT:    [[TMP18:%.*]] = add i32 [[TMP17]], -1
// PPC64-NEXT:    [[TMP19:%.*]] = load i32, ptr [[STEP2]], align 4, !dbg [[DBG86]]
// PPC64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i32 [[TMP14]], ptr [[TMP20]], align 4
// PPC64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i32 [[TMP15]], ptr [[TMP21]], align 4
// PPC64-NEXT:    [[TMP22:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i32 [[TMP18]], ptr [[TMP22]], align 4
// PPC64-NEXT:    [[TMP23:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 7
// PPC64-NEXT:    store i32 [[TMP19]], ptr [[TMP23]], align 4
// PPC64-NEXT:    br label [[TMP1]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.7
// PPC64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[STEP1:%.*]], ptr [[STEP2:%.*]], ptr [[V:%.*]]) !dbg [[DBG95:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_6:%.*]], align 8
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG96:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG98:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// PPC64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG99:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// PPC64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// PPC64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], align 8, !dbg [[DBG99]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_6]] [[TMP15]], !dbg [[DBG99]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.8
// PPC64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG100:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_7:%.*]], align 4
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// PPC64-NEXT:      i64 0, label [[TMP4:%.*]]
// PPC64-NEXT:      i64 1, label [[TMP12:%.*]]
// PPC64-NEXT:    ]
// PPC64:       1:
// PPC64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], align 4, !dbg [[DBG101:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_7]] [[TMP2]], !dbg [[DBG101]]
// PPC64:       3:
// PPC64-NEXT:    br label [[TMP1:%.*]]
// PPC64:       4:
// PPC64-NEXT:    [[TMP5:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG103:![0-9]+]]
// PPC64-NEXT:    [[CONV:%.*]] = sext i16 [[TMP5]] to i32, !dbg [[DBG103]]
// PPC64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG104:![0-9]+]]
// PPC64-NEXT:    [[TMP7:%.*]] = load i16, ptr [[UB1]], align 2, !dbg [[DBG105:![0-9]+]]
// PPC64-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP7]] to i32, !dbg [[DBG105]]
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store i32 [[CONV]], ptr [[TMP8]], align 4
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i32 [[TMP6]], ptr [[TMP9]], align 4
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i32 [[CONV1]], ptr [[TMP10]], align 4
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i32 1, ptr [[TMP11]], align 4
// PPC64-NEXT:    br label [[TMP1]]
// PPC64:       12:
// PPC64-NEXT:    [[TMP13:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG106:![0-9]+]]
// PPC64-NEXT:    [[CONV2:%.*]] = sext i16 [[TMP13]] to i32, !dbg [[DBG106]]
// PPC64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG107:![0-9]+]]
// PPC64-NEXT:    [[TMP15:%.*]] = load i16, ptr [[UB2]], align 2, !dbg [[DBG101]]
// PPC64-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP15]] to i32, !dbg [[DBG101]]
// PPC64-NEXT:    [[TMP16:%.*]] = add i32 [[CONV2]], [[CONV3]]
// PPC64-NEXT:    [[TMP17:%.*]] = add i32 [[TMP16]], -1
// PPC64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i32 [[CONV2]], ptr [[TMP18]], align 4
// PPC64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i32 [[TMP14]], ptr [[TMP19]], align 4
// PPC64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i32 [[TMP17]], ptr [[TMP20]], align 4
// PPC64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 7
// PPC64-NEXT:    store i32 1, ptr [[TMP21]], align 4
// PPC64-NEXT:    br label [[TMP1]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.9
// PPC64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[V:%.*]]) !dbg [[DBG108:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_8:%.*]], align 8
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG109:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG111:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// PPC64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG112:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// PPC64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// PPC64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], align 8, !dbg [[DBG112]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_8]] [[TMP15]], !dbg [[DBG112]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.10
// PPC64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[STEP1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], ptr [[STEP2:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG113:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_9:%.*]], align 4
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// PPC64-NEXT:      i64 0, label [[TMP4:%.*]]
// PPC64-NEXT:      i64 1, label [[TMP13:%.*]]
// PPC64-NEXT:    ]
// PPC64:       1:
// PPC64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], align 4, !dbg [[DBG114:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_9]] [[TMP2]], !dbg [[DBG114]]
// PPC64:       3:
// PPC64-NEXT:    br label [[TMP1:%.*]]
// PPC64:       4:
// PPC64-NEXT:    [[TMP5:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG116:![0-9]+]]
// PPC64-NEXT:    [[CONV:%.*]] = sext i16 [[TMP5]] to i32, !dbg [[DBG116]]
// PPC64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG117:![0-9]+]]
// PPC64-NEXT:    [[TMP7:%.*]] = load i16, ptr [[UB1]], align 2, !dbg [[DBG118:![0-9]+]]
// PPC64-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP7]] to i32, !dbg [[DBG118]]
// PPC64-NEXT:    [[TMP8:%.*]] = load i16, ptr [[STEP1]], align 2, !dbg [[DBG119:![0-9]+]]
// PPC64-NEXT:    [[CONV2:%.*]] = sext i16 [[TMP8]] to i32, !dbg [[DBG119]]
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store i32 [[CONV]], ptr [[TMP9]], align 4
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i32 [[TMP6]], ptr [[TMP10]], align 4
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i32 [[CONV1]], ptr [[TMP11]], align 4
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i32 [[CONV2]], ptr [[TMP12]], align 4
// PPC64-NEXT:    br label [[TMP1]]
// PPC64:       13:
// PPC64-NEXT:    [[TMP14:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG120:![0-9]+]]
// PPC64-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP14]] to i32, !dbg [[DBG120]]
// PPC64-NEXT:    [[TMP15:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG121:![0-9]+]]
// PPC64-NEXT:    [[TMP16:%.*]] = load i16, ptr [[UB2]], align 2, !dbg [[DBG122:![0-9]+]]
// PPC64-NEXT:    [[CONV4:%.*]] = sext i16 [[TMP16]] to i32, !dbg [[DBG122]]
// PPC64-NEXT:    [[TMP17:%.*]] = add i32 [[CONV3]], [[CONV4]]
// PPC64-NEXT:    [[TMP18:%.*]] = add i32 [[TMP17]], -1
// PPC64-NEXT:    [[TMP19:%.*]] = load i16, ptr [[STEP2]], align 2, !dbg [[DBG114]]
// PPC64-NEXT:    [[CONV5:%.*]] = sext i16 [[TMP19]] to i32, !dbg [[DBG114]]
// PPC64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i32 [[CONV3]], ptr [[TMP20]], align 4
// PPC64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i32 [[TMP15]], ptr [[TMP21]], align 4
// PPC64-NEXT:    [[TMP22:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i32 [[TMP18]], ptr [[TMP22]], align 4
// PPC64-NEXT:    [[TMP23:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 7
// PPC64-NEXT:    store i32 [[CONV5]], ptr [[TMP23]], align 4
// PPC64-NEXT:    br label [[TMP1]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.11
// PPC64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[STEP1:%.*]], ptr [[STEP2:%.*]], ptr [[V:%.*]]) !dbg [[DBG123:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_10:%.*]], align 8
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// PPC64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// PPC64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG124:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG126:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// PPC64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG127:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// PPC64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// PPC64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], align 8, !dbg [[DBG127]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_10]] [[TMP15]], !dbg [[DBG127]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@_Z1fv
// AARCH64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    call void @_Z9constantsIiEvv(), !dbg [[DBG9:![0-9]+]]
// AARCH64-NEXT:    call void @_Z12nonconstantsIiEvv(), !dbg [[DBG10:![0-9]+]]
// AARCH64-NEXT:    call void @_Z12nonconstantsIsEvv(), !dbg [[DBG11:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG12:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@_Z9constantsIiEvv
// AARCH64-SAME: () #[[ATTR0]] comdat !dbg [[DBG13:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[I:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[J:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I1:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[J2:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    store i32 0, ptr [[I]], align 4, !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    store i32 0, ptr [[J]], align 4, !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I]], ptr [[J]], ptr @compute_dep, ptr [[I]], ptr [[J]], ptr @v, [32 x i8] c"{ v[i][j], i = 0:10-1, j=0
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG17:![0-9]+]]
// AARCH64-NEXT:    store i32 0, ptr [[I1]], align 4, !dbg [[DBG18:![0-9]+]]
// AARCH64-NEXT:    store i32 0, ptr [[J2]], align 4, !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I1]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J2]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I1]], ptr [[J2]], ptr @compute_dep.2, ptr [[I1]], ptr [[J2]], ptr @v, [36 x i8] c"{ v[i][j], i = 0:10-1:1, j=0
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]), !dbg [[DBG21:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG22:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@_Z12nonconstantsIiEvv
// AARCH64-SAME: () #[[ATTR0]] comdat !dbg [[DBG23:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[LB1:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[UB1:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[STEP1:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[LB2:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[UB2:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[STEP2:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[J:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I1:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[J2:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG24:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4, !dbg [[DBG25:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG26:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP1]], ptr [[J]], align 4, !dbg [[DBG27:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I]], ptr [[J]], ptr @compute_dep.4, ptr [[I]], ptr [[LB1]], ptr [[UB1]], ptr [[J]], ptr [[LB2]], ptr [[UB2]], ptr @v, [36 x i8] c"{ v[i][j], i = lb1:ub1, j=lb2
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG29:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG30:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP3]], ptr [[I1]], align 4, !dbg [[DBG31:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG32:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP4]], ptr [[J2]], align 4, !dbg [[DBG33:![0-9]+]]
// AARCH64-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I1]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP1]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP2]], i32 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I1]], ptr [[J2]], ptr @compute_dep.6, ptr [[I1]], ptr [[LB1]], ptr [[UB1]], ptr [[STEP1]], ptr [[J2]], ptr [[LB2]], ptr [[UB2]], ptr [[STEP2]], ptr @v, [48 x i8] c"{ v[i][j], i = lb1:ub1:step1, j=lb2
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]), !dbg [[DBG35:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG36:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@_Z12nonconstantsIsEvv
// AARCH64-SAME: () #[[ATTR0]] comdat !dbg [[DBG37:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[LB1:%.*]] = alloca i16, align 2
// AARCH64-NEXT:    [[UB1:%.*]] = alloca i16, align 2
// AARCH64-NEXT:    [[STEP1:%.*]] = alloca i16, align 2
// AARCH64-NEXT:    [[LB2:%.*]] = alloca i16, align 2
// AARCH64-NEXT:    [[UB2:%.*]] = alloca i16, align 2
// AARCH64-NEXT:    [[STEP2:%.*]] = alloca i16, align 2
// AARCH64-NEXT:    [[I:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[J:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I2:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[J4:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG38:![0-9]+]]
// AARCH64-NEXT:    [[CONV:%.*]] = sext i16 [[TMP0]] to i32, !dbg [[DBG38]]
// AARCH64-NEXT:    store i32 [[CONV]], ptr [[I]], align 4, !dbg [[DBG39:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG40:![0-9]+]]
// AARCH64-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP1]] to i32, !dbg [[DBG40]]
// AARCH64-NEXT:    store i32 [[CONV1]], ptr [[J]], align 4, !dbg [[DBG41:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i16 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I]], ptr [[J]], ptr @compute_dep.8, ptr [[I]], ptr [[LB1]], ptr [[UB1]], ptr [[J]], ptr [[LB2]], ptr [[UB2]], ptr @v, [36 x i8] c"{ v[i][j], i = lb1:ub1, j=lb2
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG43:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG44:![0-9]+]]
// AARCH64-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP3]] to i32, !dbg [[DBG44]]
// AARCH64-NEXT:    store i32 [[CONV3]], ptr [[I2]], align 4, !dbg [[DBG45:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG46:![0-9]+]]
// AARCH64-NEXT:    [[CONV5:%.*]] = sext i16 [[TMP4]] to i32, !dbg [[DBG46]]
// AARCH64-NEXT:    store i32 [[CONV5]], ptr [[J4]], align 4, !dbg [[DBG47:![0-9]+]]
// AARCH64-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @v, [10 x [20 x i32]] undef), "QUAL.OSS.PRIVATE"(ptr [[I2]], i32 undef), "QUAL.OSS.PRIVATE"(ptr [[J4]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP1]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[LB2]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[UB2]], i16 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[STEP2]], i16 undef), "QUAL.OSS.MULTIDEP.RANGE.IN"(ptr [[I2]], ptr [[J4]], ptr @compute_dep.10, ptr [[I2]], ptr [[LB1]], ptr [[UB1]], ptr [[STEP1]], ptr [[J4]], ptr [[LB2]], ptr [[UB2]], ptr [[STEP2]], ptr @v, [48 x i8] c"{ v[i][j], i = lb1:ub1:step1, j=lb2
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]), !dbg [[DBG49:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG50:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG51:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 4
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// AARCH64-NEXT:      i64 0, label [[TMP4:%.*]]
// AARCH64-NEXT:      i64 1, label [[TMP10:%.*]]
// AARCH64-NEXT:    ]
// AARCH64:       1:
// AARCH64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 4, !dbg [[DBG53:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP2]], !dbg [[DBG53]]
// AARCH64:       3:
// AARCH64-NEXT:    br label [[TMP1:%.*]]
// AARCH64:       4:
// AARCH64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG55:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store i32 0, ptr [[TMP6]], align 4
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i32 [[TMP5]], ptr [[TMP7]], align 4
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i32 9, ptr [[TMP8]], align 4
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i32 1, ptr [[TMP9]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
// AARCH64:       10:
// AARCH64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG56:![0-9]+]]
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i32 0, ptr [[TMP12]], align 4
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i32 [[TMP11]], ptr [[TMP13]], align 4
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i32 19, ptr [[TMP14]], align 4
// AARCH64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 7
// AARCH64-NEXT:    store i32 1, ptr [[TMP15]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.1
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[V:%.*]]) !dbg [[DBG57:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG58:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG60:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// AARCH64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG61:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// AARCH64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// AARCH64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8, !dbg [[DBG61]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP15]], !dbg [[DBG61]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.2
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG62:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 4
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// AARCH64-NEXT:      i64 0, label [[TMP4:%.*]]
// AARCH64-NEXT:      i64 1, label [[TMP10:%.*]]
// AARCH64-NEXT:    ]
// AARCH64:       1:
// AARCH64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 4, !dbg [[DBG63:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP2]], !dbg [[DBG63]]
// AARCH64:       3:
// AARCH64-NEXT:    br label [[TMP1:%.*]]
// AARCH64:       4:
// AARCH64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG65:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store i32 0, ptr [[TMP6]], align 4
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i32 [[TMP5]], ptr [[TMP7]], align 4
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i32 9, ptr [[TMP8]], align 4
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i32 1, ptr [[TMP9]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
// AARCH64:       10:
// AARCH64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG66:![0-9]+]]
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i32 0, ptr [[TMP12]], align 4
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i32 [[TMP11]], ptr [[TMP13]], align 4
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i32 19, ptr [[TMP14]], align 4
// AARCH64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 7
// AARCH64-NEXT:    store i32 1, ptr [[TMP15]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.3
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[V:%.*]]) !dbg [[DBG67:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_2:%.*]], align 8
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG68:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG70:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// AARCH64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG71:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// AARCH64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// AARCH64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], align 8, !dbg [[DBG71]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_2]] [[TMP15]], !dbg [[DBG71]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.4
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG72:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_3:%.*]], align 4
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// AARCH64-NEXT:      i64 0, label [[TMP4:%.*]]
// AARCH64-NEXT:      i64 1, label [[TMP12:%.*]]
// AARCH64-NEXT:    ]
// AARCH64:       1:
// AARCH64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], align 4, !dbg [[DBG73:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_3]] [[TMP2]], !dbg [[DBG73]]
// AARCH64:       3:
// AARCH64-NEXT:    br label [[TMP1:%.*]]
// AARCH64:       4:
// AARCH64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG75:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG76:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[UB1]], align 4, !dbg [[DBG77:![0-9]+]]
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store i32 [[TMP5]], ptr [[TMP8]], align 4
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i32 [[TMP6]], ptr [[TMP9]], align 4
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i32 [[TMP7]], ptr [[TMP10]], align 4
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i32 1, ptr [[TMP11]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
// AARCH64:       12:
// AARCH64-NEXT:    [[TMP13:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG78:![0-9]+]]
// AARCH64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG79:![0-9]+]]
// AARCH64-NEXT:    [[TMP15:%.*]] = load i32, ptr [[UB2]], align 4, !dbg [[DBG73]]
// AARCH64-NEXT:    [[TMP16:%.*]] = add i32 [[TMP13]], [[TMP15]]
// AARCH64-NEXT:    [[TMP17:%.*]] = add i32 [[TMP16]], -1
// AARCH64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i32 [[TMP13]], ptr [[TMP18]], align 4
// AARCH64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i32 [[TMP14]], ptr [[TMP19]], align 4
// AARCH64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i32 [[TMP17]], ptr [[TMP20]], align 4
// AARCH64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 7
// AARCH64-NEXT:    store i32 1, ptr [[TMP21]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.5
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[V:%.*]]) !dbg [[DBG80:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_4:%.*]], align 8
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG81:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG83:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// AARCH64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG84:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// AARCH64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// AARCH64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], align 8, !dbg [[DBG84]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_4]] [[TMP15]], !dbg [[DBG84]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.6
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[STEP1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], ptr [[STEP2:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG85:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_5:%.*]], align 4
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// AARCH64-NEXT:      i64 0, label [[TMP4:%.*]]
// AARCH64-NEXT:      i64 1, label [[TMP13:%.*]]
// AARCH64-NEXT:    ]
// AARCH64:       1:
// AARCH64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], align 4, !dbg [[DBG86:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_5]] [[TMP2]], !dbg [[DBG86]]
// AARCH64:       3:
// AARCH64-NEXT:    br label [[TMP1:%.*]]
// AARCH64:       4:
// AARCH64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[LB1]], align 4, !dbg [[DBG88:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG89:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[UB1]], align 4, !dbg [[DBG90:![0-9]+]]
// AARCH64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[STEP1]], align 4, !dbg [[DBG91:![0-9]+]]
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store i32 [[TMP5]], ptr [[TMP9]], align 4
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i32 [[TMP6]], ptr [[TMP10]], align 4
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i32 [[TMP7]], ptr [[TMP11]], align 4
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i32 [[TMP8]], ptr [[TMP12]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
// AARCH64:       13:
// AARCH64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[LB2]], align 4, !dbg [[DBG92:![0-9]+]]
// AARCH64-NEXT:    [[TMP15:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG93:![0-9]+]]
// AARCH64-NEXT:    [[TMP16:%.*]] = load i32, ptr [[UB2]], align 4, !dbg [[DBG94:![0-9]+]]
// AARCH64-NEXT:    [[TMP17:%.*]] = add i32 [[TMP14]], [[TMP16]]
// AARCH64-NEXT:    [[TMP18:%.*]] = add i32 [[TMP17]], -1
// AARCH64-NEXT:    [[TMP19:%.*]] = load i32, ptr [[STEP2]], align 4, !dbg [[DBG86]]
// AARCH64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i32 [[TMP14]], ptr [[TMP20]], align 4
// AARCH64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i32 [[TMP15]], ptr [[TMP21]], align 4
// AARCH64-NEXT:    [[TMP22:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i32 [[TMP18]], ptr [[TMP22]], align 4
// AARCH64-NEXT:    [[TMP23:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 7
// AARCH64-NEXT:    store i32 [[TMP19]], ptr [[TMP23]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.7
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[STEP1:%.*]], ptr [[STEP2:%.*]], ptr [[V:%.*]]) !dbg [[DBG95:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_6:%.*]], align 8
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG96:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG98:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// AARCH64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG99:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// AARCH64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// AARCH64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], align 8, !dbg [[DBG99]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_6]] [[TMP15]], !dbg [[DBG99]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.8
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG100:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_7:%.*]], align 4
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// AARCH64-NEXT:      i64 0, label [[TMP4:%.*]]
// AARCH64-NEXT:      i64 1, label [[TMP12:%.*]]
// AARCH64-NEXT:    ]
// AARCH64:       1:
// AARCH64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], align 4, !dbg [[DBG101:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_7]] [[TMP2]], !dbg [[DBG101]]
// AARCH64:       3:
// AARCH64-NEXT:    br label [[TMP1:%.*]]
// AARCH64:       4:
// AARCH64-NEXT:    [[TMP5:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG103:![0-9]+]]
// AARCH64-NEXT:    [[CONV:%.*]] = sext i16 [[TMP5]] to i32, !dbg [[DBG103]]
// AARCH64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG104:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = load i16, ptr [[UB1]], align 2, !dbg [[DBG105:![0-9]+]]
// AARCH64-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP7]] to i32, !dbg [[DBG105]]
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store i32 [[CONV]], ptr [[TMP8]], align 4
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i32 [[TMP6]], ptr [[TMP9]], align 4
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i32 [[CONV1]], ptr [[TMP10]], align 4
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i32 1, ptr [[TMP11]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
// AARCH64:       12:
// AARCH64-NEXT:    [[TMP13:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG106:![0-9]+]]
// AARCH64-NEXT:    [[CONV2:%.*]] = sext i16 [[TMP13]] to i32, !dbg [[DBG106]]
// AARCH64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG107:![0-9]+]]
// AARCH64-NEXT:    [[TMP15:%.*]] = load i16, ptr [[UB2]], align 2, !dbg [[DBG101]]
// AARCH64-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP15]] to i32, !dbg [[DBG101]]
// AARCH64-NEXT:    [[TMP16:%.*]] = add i32 [[CONV2]], [[CONV3]]
// AARCH64-NEXT:    [[TMP17:%.*]] = add i32 [[TMP16]], -1
// AARCH64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i32 [[CONV2]], ptr [[TMP18]], align 4
// AARCH64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i32 [[TMP14]], ptr [[TMP19]], align 4
// AARCH64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i32 [[TMP17]], ptr [[TMP20]], align 4
// AARCH64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 7
// AARCH64-NEXT:    store i32 1, ptr [[TMP21]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.9
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[V:%.*]]) !dbg [[DBG108:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_8:%.*]], align 8
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG109:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG111:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// AARCH64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG112:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// AARCH64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// AARCH64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], align 8, !dbg [[DBG112]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_8]] [[TMP15]], !dbg [[DBG112]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.10
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[LB1:%.*]], ptr [[UB1:%.*]], ptr [[STEP1:%.*]], ptr [[J:%.*]], ptr [[LB2:%.*]], ptr [[UB2:%.*]], ptr [[STEP2:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG113:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_9:%.*]], align 4
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    switch i64 [[TMP0]], label [[TMP3:%.*]] [
// AARCH64-NEXT:      i64 0, label [[TMP4:%.*]]
// AARCH64-NEXT:      i64 1, label [[TMP13:%.*]]
// AARCH64-NEXT:    ]
// AARCH64:       1:
// AARCH64-NEXT:    [[TMP2:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], align 4, !dbg [[DBG114:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_9]] [[TMP2]], !dbg [[DBG114]]
// AARCH64:       3:
// AARCH64-NEXT:    br label [[TMP1:%.*]]
// AARCH64:       4:
// AARCH64-NEXT:    [[TMP5:%.*]] = load i16, ptr [[LB1]], align 2, !dbg [[DBG116:![0-9]+]]
// AARCH64-NEXT:    [[CONV:%.*]] = sext i16 [[TMP5]] to i32, !dbg [[DBG116]]
// AARCH64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG117:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = load i16, ptr [[UB1]], align 2, !dbg [[DBG118:![0-9]+]]
// AARCH64-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP7]] to i32, !dbg [[DBG118]]
// AARCH64-NEXT:    [[TMP8:%.*]] = load i16, ptr [[STEP1]], align 2, !dbg [[DBG119:![0-9]+]]
// AARCH64-NEXT:    [[CONV2:%.*]] = sext i16 [[TMP8]] to i32, !dbg [[DBG119]]
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store i32 [[CONV]], ptr [[TMP9]], align 4
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i32 [[TMP6]], ptr [[TMP10]], align 4
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i32 [[CONV1]], ptr [[TMP11]], align 4
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i32 [[CONV2]], ptr [[TMP12]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
// AARCH64:       13:
// AARCH64-NEXT:    [[TMP14:%.*]] = load i16, ptr [[LB2]], align 2, !dbg [[DBG120:![0-9]+]]
// AARCH64-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP14]] to i32, !dbg [[DBG120]]
// AARCH64-NEXT:    [[TMP15:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG121:![0-9]+]]
// AARCH64-NEXT:    [[TMP16:%.*]] = load i16, ptr [[UB2]], align 2, !dbg [[DBG122:![0-9]+]]
// AARCH64-NEXT:    [[CONV4:%.*]] = sext i16 [[TMP16]] to i32, !dbg [[DBG122]]
// AARCH64-NEXT:    [[TMP17:%.*]] = add i32 [[CONV3]], [[CONV4]]
// AARCH64-NEXT:    [[TMP18:%.*]] = add i32 [[TMP17]], -1
// AARCH64-NEXT:    [[TMP19:%.*]] = load i16, ptr [[STEP2]], align 2, !dbg [[DBG114]]
// AARCH64-NEXT:    [[CONV5:%.*]] = sext i16 [[TMP19]] to i32, !dbg [[DBG114]]
// AARCH64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i32 [[CONV3]], ptr [[TMP20]], align 4
// AARCH64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i32 [[TMP15]], ptr [[TMP21]], align 4
// AARCH64-NEXT:    [[TMP22:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i32 [[TMP18]], ptr [[TMP22]], align 4
// AARCH64-NEXT:    [[TMP23:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 7
// AARCH64-NEXT:    store i32 [[CONV5]], ptr [[TMP23]], align 4
// AARCH64-NEXT:    br label [[TMP1]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.11
// AARCH64-SAME: (ptr [[I:%.*]], ptr [[J:%.*]], ptr [[LB1:%.*]], ptr [[LB2:%.*]], ptr [[UB1:%.*]], ptr [[UB2:%.*]], ptr [[STEP1:%.*]], ptr [[STEP2:%.*]], ptr [[V:%.*]]) !dbg [[DBG123:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_10:%.*]], align 8
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[J_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[LB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[UB2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[STEP1_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[STEP2_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[I]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[J]], ptr [[J_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB1]], ptr [[LB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[LB2]], ptr [[LB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB1]], ptr [[UB1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[UB2]], ptr [[UB2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[STEP1]], ptr [[STEP1_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[STEP2]], ptr [[STEP2_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[V]], ptr [[V_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[J]], align 4, !dbg [[DBG124:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4, !dbg [[DBG126:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// AARCH64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[V]], i64 0, i64 0, !dbg [[DBG127:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// AARCH64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 80, ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 10, ptr [[TMP12]], align 8
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP13]], align 8
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[TMP14]], align 8
// AARCH64-NEXT:    [[TMP15:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], align 8, !dbg [[DBG127]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_10]] [[TMP15]], !dbg [[DBG127]]
//
