// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs
// RUN: %clang_cc1 -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics

#pragma oss task in(*a)
void foo(int size, int (*a)[size]) {
}

int main() {
    int n;
    int mat[n][n];
    foo(n - 1, mat);
}




#pragma oss task out([size/77]p)
void foo1(int size, int *p) {
}

void bar() {
    int n = 10;
    int *v;
    foo1(n/55, v);
    foo1(n/99, v);
}

// checks we do not reuse VLASize between task outline calls

// LIN64-LABEL: define {{[^@]+}}@foo
// LIN64-SAME: (i32 noundef [[SIZE:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// LIN64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4, !dbg [[DBG9:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64
// LIN64-NEXT:    ret void, !dbg [[DBG10:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@main
// LIN64-SAME: () #[[ATTR0]] !dbg [[DBG11:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[CALL_ARG1:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG12:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = zext i32 [[TMP2]] to i64, !dbg [[DBG13]]
// LIN64-NEXT:    [[TMP4:%.*]] = call ptr @llvm.stacksave.p0(), !dbg [[DBG13]]
// LIN64-NEXT:    store ptr [[TMP4]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG13]]
// LIN64-NEXT:    [[TMP5:%.*]] = mul nuw i64 [[TMP1]], [[TMP3]], !dbg [[DBG13]]
// LIN64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP5]], align 16, !dbg [[DBG13]]
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG13]]
// LIN64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG13]]
// LIN64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP6]], 1, !dbg [[DBG16:![0-9]+]]
// LIN64-NEXT:    store i32 [[SUB]], ptr [[CALL_ARG]], align 4, !dbg [[DBG16]]
// LIN64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG17:![0-9]+]]
// LIN64-NEXT:    [[TMP8:%.*]] = zext i32 [[TMP7]] to i64, !dbg [[DBG18:![0-9]+]]
// LIN64-NEXT:    store ptr [[VLA]], ptr [[CALL_ARG1]], align 8, !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG1]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[CALL_ARG1]], [3 x i8] c"*a\00", ptr @compute_dep, ptr [[CALL_ARG1]], i64 [[TMP8]]), "QUAL.OSS.CAPTURED"(i64 [[TMP8]]), "QUAL.OSS.DECL.SOURCE"([21 x i8] c"task_function1.c:7:9\00") ], !dbg [[DBG18]]
// LIN64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG16]]
// LIN64-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[CALL_ARG1]], align 8, !dbg [[DBG19]]
// LIN64-NEXT:    call void @foo(i32 noundef [[TMP10]], ptr noundef [[TMP11]]), !dbg [[DBG18]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]), !dbg [[DBG18]]
// LIN64-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG20:![0-9]+]]
// LIN64-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP12]]), !dbg [[DBG20]]
// LIN64-NEXT:    ret i32 0, !dbg [[DBG20]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep
// LIN64-SAME: (ptr [[A:%.*]], i64 [[TMP0:%.*]]) #[[ATTR3:[0-9]+]] !dbg [[DBG21:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 8
// LIN64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[A]], align 8, !dbg [[DBG22:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = mul i64 [[TMP0]], 4
// LIN64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP0]], 4
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP1]], ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 [[TMP2]], ptr [[TMP5]], align 8
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP6]], align 8
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP3]], ptr [[TMP7]], align 8
// LIN64-NEXT:    [[TMP8:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 8, !dbg [[DBG22]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP8]], !dbg [[DBG22]]
//
//
// LIN64-LABEL: define {{[^@]+}}@foo1
// LIN64-SAME: (i32 noundef [[SIZE:%.*]], ptr noundef [[P:%.*]]) #[[ATTR0]] !dbg [[DBG24:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// LIN64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// LIN64-NEXT:    ret void, !dbg [[DBG25:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@bar
// LIN64-SAME: () #[[ATTR0]] !dbg [[DBG26:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[V:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[CALL_ARG1:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[CALL_ARG2:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[CALL_ARG4:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store i32 10, ptr [[N]], align 4, !dbg [[DBG27:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG28:![0-9]+]]
// LIN64-NEXT:    [[DIV:%.*]] = sdiv i32 [[TMP0]], 55, !dbg [[DBG29:![0-9]+]]
// LIN64-NEXT:    store i32 [[DIV]], ptr [[CALL_ARG]], align 4, !dbg [[DBG29]]
// LIN64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[V]], align 8, !dbg [[DBG30:![0-9]+]]
// LIN64-NEXT:    store ptr [[TMP1]], ptr [[CALL_ARG1]], align 8, !dbg [[DBG30]]
// LIN64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG1]], ptr undef), "QUAL.OSS.DEP.OUT"(ptr [[CALL_ARG1]], [11 x i8] c"[size/77]p\00", ptr @compute_dep.1, ptr [[CALL_ARG1]], ptr [[CALL_ARG]]), "QUAL.OSS.DECL.SOURCE"([22 x i8] c"task_function1.c:20:9\00") ], !dbg [[DBG31:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG29]]
// LIN64-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[CALL_ARG1]], align 8, !dbg [[DBG30]]
// LIN64-NEXT:    call void @foo1(i32 noundef [[TMP3]], ptr noundef [[TMP4]]), !dbg [[DBG31]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG31]]
// LIN64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG32:![0-9]+]]
// LIN64-NEXT:    [[DIV3:%.*]] = sdiv i32 [[TMP5]], 99, !dbg [[DBG33:![0-9]+]]
// LIN64-NEXT:    store i32 [[DIV3]], ptr [[CALL_ARG2]], align 4, !dbg [[DBG33]]
// LIN64-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[V]], align 8, !dbg [[DBG34:![0-9]+]]
// LIN64-NEXT:    store ptr [[TMP6]], ptr [[CALL_ARG4]], align 8, !dbg [[DBG34]]
// LIN64-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG4]], ptr undef), "QUAL.OSS.DEP.OUT"(ptr [[CALL_ARG4]], [11 x i8] c"[size/77]p\00", ptr @compute_dep.2, ptr [[CALL_ARG4]], ptr [[CALL_ARG2]]), "QUAL.OSS.DECL.SOURCE"([22 x i8] c"task_function1.c:20:9\00") ], !dbg [[DBG35:![0-9]+]]
// LIN64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[CALL_ARG2]], align 4, !dbg [[DBG33]]
// LIN64-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[CALL_ARG4]], align 8, !dbg [[DBG34]]
// LIN64-NEXT:    call void @foo1(i32 noundef [[TMP8]], ptr noundef [[TMP9]]), !dbg [[DBG35]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]), !dbg [[DBG35]]
// LIN64-NEXT:    ret void, !dbg [[DBG36:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.1
// LIN64-SAME: (ptr [[P:%.*]], ptr [[SIZE:%.*]]) #[[ATTR3]] !dbg [[DBG37:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// LIN64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[SIZE_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// LIN64-NEXT:    store ptr [[SIZE]], ptr [[SIZE_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG38:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SIZE]], align 4, !dbg [[DBG40:![0-9]+]]
// LIN64-NEXT:    [[DIV:%.*]] = sdiv i32 [[TMP1]], 77, !dbg [[DBG41:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = zext i32 [[DIV]] to i64
// LIN64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP5]], align 8
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 [[TMP3]], ptr [[TMP6]], align 8
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8, !dbg [[DBG42:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP9]], !dbg [[DBG42]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.2
// LIN64-SAME: (ptr [[P:%.*]], ptr [[SIZE:%.*]]) #[[ATTR3]] !dbg [[DBG43:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 8
// LIN64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[SIZE_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// LIN64-NEXT:    store ptr [[SIZE]], ptr [[SIZE_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG44:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SIZE]], align 4, !dbg [[DBG46:![0-9]+]]
// LIN64-NEXT:    [[DIV:%.*]] = sdiv i32 [[TMP1]], 77, !dbg [[DBG47:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = zext i32 [[DIV]] to i64
// LIN64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP5]], align 8
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 [[TMP3]], ptr [[TMP6]], align 8
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 8, !dbg [[DBG48:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP9]], !dbg [[DBG48]]
//
//
// PPC64-LABEL: define {{[^@]+}}@foo
// PPC64-SAME: (i32 noundef signext [[SIZE:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// PPC64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4, !dbg [[DBG9:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64
// PPC64-NEXT:    ret void, !dbg [[DBG10:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@main
// PPC64-SAME: () #[[ATTR0]] !dbg [[DBG11:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[CALL_ARG1:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG12:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = zext i32 [[TMP2]] to i64, !dbg [[DBG13]]
// PPC64-NEXT:    [[TMP4:%.*]] = call ptr @llvm.stacksave.p0(), !dbg [[DBG13]]
// PPC64-NEXT:    store ptr [[TMP4]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG13]]
// PPC64-NEXT:    [[TMP5:%.*]] = mul nuw i64 [[TMP1]], [[TMP3]], !dbg [[DBG13]]
// PPC64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP5]], align 4, !dbg [[DBG13]]
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG13]]
// PPC64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG13]]
// PPC64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP6]], 1, !dbg [[DBG16:![0-9]+]]
// PPC64-NEXT:    store i32 [[SUB]], ptr [[CALL_ARG]], align 4, !dbg [[DBG16]]
// PPC64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG17:![0-9]+]]
// PPC64-NEXT:    [[TMP8:%.*]] = zext i32 [[TMP7]] to i64, !dbg [[DBG18:![0-9]+]]
// PPC64-NEXT:    store ptr [[VLA]], ptr [[CALL_ARG1]], align 8, !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG1]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[CALL_ARG1]], [3 x i8] c"*a\00", ptr @compute_dep, ptr [[CALL_ARG1]], i64 [[TMP8]]), "QUAL.OSS.CAPTURED"(i64 [[TMP8]]), "QUAL.OSS.DECL.SOURCE"([21 x i8] c"task_function1.c:7:9\00") ], !dbg [[DBG18]]
// PPC64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG16]]
// PPC64-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[CALL_ARG1]], align 8, !dbg [[DBG19]]
// PPC64-NEXT:    call void @foo(i32 noundef signext [[TMP10]], ptr noundef [[TMP11]]), !dbg [[DBG18]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]), !dbg [[DBG18]]
// PPC64-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG20:![0-9]+]]
// PPC64-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP12]]), !dbg [[DBG20]]
// PPC64-NEXT:    ret i32 0, !dbg [[DBG20]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep
// PPC64-SAME: (ptr [[A:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG21:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 8
// PPC64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[A]], align 8, !dbg [[DBG22:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = mul i64 [[TMP0]], 4
// PPC64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP0]], 4
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP1]], ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 [[TMP2]], ptr [[TMP5]], align 8
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP6]], align 8
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP3]], ptr [[TMP7]], align 8
// PPC64-NEXT:    [[TMP8:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 8, !dbg [[DBG22]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP8]], !dbg [[DBG22]]
//
//
// PPC64-LABEL: define {{[^@]+}}@foo1
// PPC64-SAME: (i32 noundef signext [[SIZE:%.*]], ptr noundef [[P:%.*]]) #[[ATTR0]] !dbg [[DBG24:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// PPC64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// PPC64-NEXT:    ret void, !dbg [[DBG25:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@bar
// PPC64-SAME: () #[[ATTR0]] !dbg [[DBG26:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[V:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[CALL_ARG1:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[CALL_ARG2:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[CALL_ARG4:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store i32 10, ptr [[N]], align 4, !dbg [[DBG27:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG28:![0-9]+]]
// PPC64-NEXT:    [[DIV:%.*]] = sdiv i32 [[TMP0]], 55, !dbg [[DBG29:![0-9]+]]
// PPC64-NEXT:    store i32 [[DIV]], ptr [[CALL_ARG]], align 4, !dbg [[DBG29]]
// PPC64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[V]], align 8, !dbg [[DBG30:![0-9]+]]
// PPC64-NEXT:    store ptr [[TMP1]], ptr [[CALL_ARG1]], align 8, !dbg [[DBG30]]
// PPC64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG1]], ptr undef), "QUAL.OSS.DEP.OUT"(ptr [[CALL_ARG1]], [11 x i8] c"[size/77]p\00", ptr @compute_dep.1, ptr [[CALL_ARG1]], ptr [[CALL_ARG]]), "QUAL.OSS.DECL.SOURCE"([22 x i8] c"task_function1.c:20:9\00") ], !dbg [[DBG31:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG29]]
// PPC64-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[CALL_ARG1]], align 8, !dbg [[DBG30]]
// PPC64-NEXT:    call void @foo1(i32 noundef signext [[TMP3]], ptr noundef [[TMP4]]), !dbg [[DBG31]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG31]]
// PPC64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG32:![0-9]+]]
// PPC64-NEXT:    [[DIV3:%.*]] = sdiv i32 [[TMP5]], 99, !dbg [[DBG33:![0-9]+]]
// PPC64-NEXT:    store i32 [[DIV3]], ptr [[CALL_ARG2]], align 4, !dbg [[DBG33]]
// PPC64-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[V]], align 8, !dbg [[DBG34:![0-9]+]]
// PPC64-NEXT:    store ptr [[TMP6]], ptr [[CALL_ARG4]], align 8, !dbg [[DBG34]]
// PPC64-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG4]], ptr undef), "QUAL.OSS.DEP.OUT"(ptr [[CALL_ARG4]], [11 x i8] c"[size/77]p\00", ptr @compute_dep.2, ptr [[CALL_ARG4]], ptr [[CALL_ARG2]]), "QUAL.OSS.DECL.SOURCE"([22 x i8] c"task_function1.c:20:9\00") ], !dbg [[DBG35:![0-9]+]]
// PPC64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[CALL_ARG2]], align 4, !dbg [[DBG33]]
// PPC64-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[CALL_ARG4]], align 8, !dbg [[DBG34]]
// PPC64-NEXT:    call void @foo1(i32 noundef signext [[TMP8]], ptr noundef [[TMP9]]), !dbg [[DBG35]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]), !dbg [[DBG35]]
// PPC64-NEXT:    ret void, !dbg [[DBG36:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.1
// PPC64-SAME: (ptr [[P:%.*]], ptr [[SIZE:%.*]]) !dbg [[DBG37:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// PPC64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[SIZE_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// PPC64-NEXT:    store ptr [[SIZE]], ptr [[SIZE_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG38:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SIZE]], align 4, !dbg [[DBG40:![0-9]+]]
// PPC64-NEXT:    [[DIV:%.*]] = sdiv i32 [[TMP1]], 77, !dbg [[DBG41:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = zext i32 [[DIV]] to i64
// PPC64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP5]], align 8
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 [[TMP3]], ptr [[TMP6]], align 8
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8, !dbg [[DBG42:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP9]], !dbg [[DBG42]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.2
// PPC64-SAME: (ptr [[P:%.*]], ptr [[SIZE:%.*]]) !dbg [[DBG43:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 8
// PPC64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[SIZE_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// PPC64-NEXT:    store ptr [[SIZE]], ptr [[SIZE_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG44:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SIZE]], align 4, !dbg [[DBG46:![0-9]+]]
// PPC64-NEXT:    [[DIV:%.*]] = sdiv i32 [[TMP1]], 77, !dbg [[DBG47:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = zext i32 [[DIV]] to i64
// PPC64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP5]], align 8
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 [[TMP3]], ptr [[TMP6]], align 8
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 8, !dbg [[DBG48:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP9]], !dbg [[DBG48]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@foo
// AARCH64-SAME: (i32 noundef [[SIZE:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// AARCH64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4, !dbg [[DBG9:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64
// AARCH64-NEXT:    ret void, !dbg [[DBG10:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@main
// AARCH64-SAME: () #[[ATTR0]] !dbg [[DBG11:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[CALL_ARG1:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG12:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = zext i32 [[TMP2]] to i64, !dbg [[DBG13]]
// AARCH64-NEXT:    [[TMP4:%.*]] = call ptr @llvm.stacksave.p0(), !dbg [[DBG13]]
// AARCH64-NEXT:    store ptr [[TMP4]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG13]]
// AARCH64-NEXT:    [[TMP5:%.*]] = mul nuw i64 [[TMP1]], [[TMP3]], !dbg [[DBG13]]
// AARCH64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP5]], align 4, !dbg [[DBG13]]
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG13]]
// AARCH64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG13]]
// AARCH64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP6]], 1, !dbg [[DBG16:![0-9]+]]
// AARCH64-NEXT:    store i32 [[SUB]], ptr [[CALL_ARG]], align 4, !dbg [[DBG16]]
// AARCH64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG17:![0-9]+]]
// AARCH64-NEXT:    [[TMP8:%.*]] = zext i32 [[TMP7]] to i64, !dbg [[DBG18:![0-9]+]]
// AARCH64-NEXT:    store ptr [[VLA]], ptr [[CALL_ARG1]], align 8, !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG1]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[CALL_ARG1]], [3 x i8] c"*a\00", ptr @compute_dep, ptr [[CALL_ARG1]], i64 [[TMP8]]), "QUAL.OSS.CAPTURED"(i64 [[TMP8]]), "QUAL.OSS.DECL.SOURCE"([21 x i8] c"task_function1.c:7:9\00") ], !dbg [[DBG18]]
// AARCH64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG16]]
// AARCH64-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[CALL_ARG1]], align 8, !dbg [[DBG19]]
// AARCH64-NEXT:    call void @foo(i32 noundef [[TMP10]], ptr noundef [[TMP11]]), !dbg [[DBG18]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]), !dbg [[DBG18]]
// AARCH64-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG20:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP12]]), !dbg [[DBG20]]
// AARCH64-NEXT:    ret i32 0, !dbg [[DBG20]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep
// AARCH64-SAME: (ptr [[A:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG21:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 8
// AARCH64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[A]], align 8, !dbg [[DBG22:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = mul i64 [[TMP0]], 4
// AARCH64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP0]], 4
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP1]], ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 [[TMP2]], ptr [[TMP5]], align 8
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP6]], align 8
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP3]], ptr [[TMP7]], align 8
// AARCH64-NEXT:    [[TMP8:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 8, !dbg [[DBG22]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP8]], !dbg [[DBG22]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@foo1
// AARCH64-SAME: (i32 noundef [[SIZE:%.*]], ptr noundef [[P:%.*]]) #[[ATTR0]] !dbg [[DBG24:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// AARCH64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// AARCH64-NEXT:    ret void, !dbg [[DBG25:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@bar
// AARCH64-SAME: () #[[ATTR0]] !dbg [[DBG26:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[V:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[CALL_ARG1:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[CALL_ARG2:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[CALL_ARG4:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store i32 10, ptr [[N]], align 4, !dbg [[DBG27:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG28:![0-9]+]]
// AARCH64-NEXT:    [[DIV:%.*]] = sdiv i32 [[TMP0]], 55, !dbg [[DBG29:![0-9]+]]
// AARCH64-NEXT:    store i32 [[DIV]], ptr [[CALL_ARG]], align 4, !dbg [[DBG29]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[V]], align 8, !dbg [[DBG30:![0-9]+]]
// AARCH64-NEXT:    store ptr [[TMP1]], ptr [[CALL_ARG1]], align 8, !dbg [[DBG30]]
// AARCH64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG1]], ptr undef), "QUAL.OSS.DEP.OUT"(ptr [[CALL_ARG1]], [11 x i8] c"[size/77]p\00", ptr @compute_dep.1, ptr [[CALL_ARG1]], ptr [[CALL_ARG]]), "QUAL.OSS.DECL.SOURCE"([22 x i8] c"task_function1.c:20:9\00") ], !dbg [[DBG31:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG29]]
// AARCH64-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[CALL_ARG1]], align 8, !dbg [[DBG30]]
// AARCH64-NEXT:    call void @foo1(i32 noundef [[TMP3]], ptr noundef [[TMP4]]), !dbg [[DBG31]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG31]]
// AARCH64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG32:![0-9]+]]
// AARCH64-NEXT:    [[DIV3:%.*]] = sdiv i32 [[TMP5]], 99, !dbg [[DBG33:![0-9]+]]
// AARCH64-NEXT:    store i32 [[DIV3]], ptr [[CALL_ARG2]], align 4, !dbg [[DBG33]]
// AARCH64-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[V]], align 8, !dbg [[DBG34:![0-9]+]]
// AARCH64-NEXT:    store ptr [[TMP6]], ptr [[CALL_ARG4]], align 8, !dbg [[DBG34]]
// AARCH64-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG2]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG4]], ptr undef), "QUAL.OSS.DEP.OUT"(ptr [[CALL_ARG4]], [11 x i8] c"[size/77]p\00", ptr @compute_dep.2, ptr [[CALL_ARG4]], ptr [[CALL_ARG2]]), "QUAL.OSS.DECL.SOURCE"([22 x i8] c"task_function1.c:20:9\00") ], !dbg [[DBG35:![0-9]+]]
// AARCH64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[CALL_ARG2]], align 4, !dbg [[DBG33]]
// AARCH64-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[CALL_ARG4]], align 8, !dbg [[DBG34]]
// AARCH64-NEXT:    call void @foo1(i32 noundef [[TMP8]], ptr noundef [[TMP9]]), !dbg [[DBG35]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]), !dbg [[DBG35]]
// AARCH64-NEXT:    ret void, !dbg [[DBG36:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.1
// AARCH64-SAME: (ptr [[P:%.*]], ptr [[SIZE:%.*]]) !dbg [[DBG37:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// AARCH64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[SIZE_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[SIZE]], ptr [[SIZE_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG38:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SIZE]], align 4, !dbg [[DBG40:![0-9]+]]
// AARCH64-NEXT:    [[DIV:%.*]] = sdiv i32 [[TMP1]], 77, !dbg [[DBG41:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = zext i32 [[DIV]] to i64
// AARCH64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP5]], align 8
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 [[TMP3]], ptr [[TMP6]], align 8
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8, !dbg [[DBG42:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP9]], !dbg [[DBG42]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.2
// AARCH64-SAME: (ptr [[P:%.*]], ptr [[SIZE:%.*]]) !dbg [[DBG43:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 8
// AARCH64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[SIZE_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[SIZE]], ptr [[SIZE_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG44:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SIZE]], align 4, !dbg [[DBG46:![0-9]+]]
// AARCH64-NEXT:    [[DIV:%.*]] = sdiv i32 [[TMP1]], 77, !dbg [[DBG47:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = zext i32 [[DIV]] to i64
// AARCH64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP5]], align 8
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 [[TMP3]], ptr [[TMP6]], align 8
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 8, !dbg [[DBG48:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP9]], !dbg [[DBG48]]
//
