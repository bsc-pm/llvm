// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -no-opaque-pointers -verify -disable-llvm-passes -fompss-2 -x c -S -emit-llvm %s -o - | FileCheck %s
// expected-no-diagnostics
#ifndef HEADER
#define HEADER

_Bool bv, bx;
char cv, cx;
unsigned char ucv, ucx;
short sv, sx;
unsigned short usv, usx;
int iv, ix;
unsigned int uiv, uix;
long lv, lx;
unsigned long ulv, ulx;
long long llv, llx;
unsigned long long ullv, ullx;
float fv, fx;
double dv, dx;
long double ldv, ldx;
_Complex int civ, cix;
_Complex float cfv, cfx;
_Complex double cdv, cdx;

typedef int int4 __attribute__((__vector_size__(16)));
int4 int4x;

struct BitFields {
  int : 32;
  int a : 31;
} bfx;

struct BitFields_packed {
  int : 32;
  int a : 31;
} __attribute__ ((__packed__)) bfx_packed;

struct BitFields2 {
  int : 31;
  int a : 1;
} bfx2;

struct BitFields2_packed {
  int : 31;
  int a : 1;
} __attribute__ ((__packed__)) bfx2_packed;

struct BitFields3 {
  int : 11;
  int a : 14;
} bfx3;

struct BitFields3_packed {
  int : 11;
  int a : 14;
} __attribute__ ((__packed__)) bfx3_packed;

struct BitFields4 {
  short : 16;
  int a: 1;
  long b : 7;
} bfx4;

struct BitFields4_packed {
  short : 16;
  int a: 1;
  long b : 7;
} __attribute__ ((__packed__)) bfx4_packed;

typedef float float2 __attribute__((ext_vector_type(2)));
float2 float2x;

// Register "0" is currently an invalid register for global register variables.
// Use "esp" instead of "0".
// register int rix __asm__("0");
register int rix __asm__("esp");

// CHECK-LABEL: @main(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca x86_fp80, align 16
// CHECK-NEXT:    [[ATOMIC_TEMP13:%.*]] = alloca { i32, i32 }, align 4
// CHECK-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// CHECK-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// CHECK-NEXT:    [[ATOMIC_TEMP28:%.*]] = alloca { i32, i32 }, align 4
// CHECK-NEXT:    [[ATOMIC_TEMP34:%.*]] = alloca x86_fp80, align 16
// CHECK-NEXT:    [[ATOMIC_TEMP36:%.*]] = alloca { i32, i32 }, align 4
// CHECK-NEXT:    [[ATOMIC_TEMP50:%.*]] = alloca <4 x i32>, align 16
// CHECK-NEXT:    [[ATOMIC_TEMP54:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[ATOMIC_TEMP56:%.*]] = alloca i32, align 1
// CHECK-NEXT:    [[ATOMIC_TEMP62:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[ATOMIC_TEMP67:%.*]] = alloca i32, align 1
// CHECK-NEXT:    [[ATOMIC_TEMP72:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[ATOMIC_TEMP77:%.*]] = alloca i32, align 1
// CHECK-NEXT:    [[ATOMIC_TEMP84:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[ATOMIC_TEMP91:%.*]] = alloca i32, align 1
// CHECK-NEXT:    [[ATOMIC_TEMP98:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[ATOMIC_TEMP104:%.*]] = alloca i64, align 1
// CHECK-NEXT:    [[ATOMIC_TEMP110:%.*]] = alloca <2 x float>, align 8
// CHECK-NEXT:    store i32 0, i32* [[RETVAL]], align 4
// CHECK-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// CHECK-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG10]]
// CHECK-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG10]]
// CHECK-NEXT:    store i8 [[FROMBOOL]], i8* @bv, align 1, !dbg [[DBG10]]
// CHECK-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, i8* @cx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// CHECK-NEXT:    store i8 [[ATOMIC_LOAD1]], i8* @cv, align 1, !dbg [[DBG11]]
// CHECK-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, i8* @ucx monotonic, align 1, !dbg [[DBG12:![0-9]+]]
// CHECK-NEXT:    store i8 [[ATOMIC_LOAD2]], i8* @ucv, align 1, !dbg [[DBG12]]
// CHECK-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, i16* @sx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// CHECK-NEXT:    store i16 [[ATOMIC_LOAD3]], i16* @sv, align 2, !dbg [[DBG13]]
// CHECK-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, i16* @usx monotonic, align 2, !dbg [[DBG14:![0-9]+]]
// CHECK-NEXT:    store i16 [[ATOMIC_LOAD4]], i16* @usv, align 2, !dbg [[DBG14]]
// CHECK-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, i32* @ix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// CHECK-NEXT:    store i32 [[ATOMIC_LOAD5]], i32* @iv, align 4, !dbg [[DBG15]]
// CHECK-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, i32* @uix monotonic, align 4, !dbg [[DBG16:![0-9]+]]
// CHECK-NEXT:    store i32 [[ATOMIC_LOAD6]], i32* @uiv, align 4, !dbg [[DBG16]]
// CHECK-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, i64* @lx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// CHECK-NEXT:    store i64 [[ATOMIC_LOAD7]], i64* @lv, align 8, !dbg [[DBG17]]
// CHECK-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// CHECK-NEXT:    store i64 [[ATOMIC_LOAD8]], i64* @ulv, align 8, !dbg [[DBG18]]
// CHECK-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, i64* @llx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// CHECK-NEXT:    store i64 [[ATOMIC_LOAD9]], i64* @llv, align 8, !dbg [[DBG19]]
// CHECK-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, i64* @ullx monotonic, align 8, !dbg [[DBG20:![0-9]+]]
// CHECK-NEXT:    store i64 [[ATOMIC_LOAD10]], i64* @ullv, align 8, !dbg [[DBG20]]
// CHECK-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic i32, i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG21:![0-9]+]]
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32 [[ATOMIC_LOAD11]] to float, !dbg [[DBG21]]
// CHECK-NEXT:    store float [[TMP0]], float* @fv, align 4, !dbg [[DBG21]]
// CHECK-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic i64, i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG22:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast i64 [[ATOMIC_LOAD12]] to double, !dbg [[DBG22]]
// CHECK-NEXT:    store double [[TMP1]], double* @dv, align 8, !dbg [[DBG22]]
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast x86_fp80* [[ATOMIC_TEMP]] to i8*, !dbg [[DBG23:![0-9]+]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (x86_fp80* @ldx to i8*), i8* noundef [[TMP2]], i32 noundef 0), !dbg [[DBG23]]
// CHECK-NEXT:    [[TMP3:%.*]] = load x86_fp80, x86_fp80* [[ATOMIC_TEMP]], align 16, !dbg [[DBG23]]
// CHECK-NEXT:    store x86_fp80 [[TMP3]], x86_fp80* @ldv, align 16, !dbg [[DBG23]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP13]] to i8*, !dbg [[DBG24:![0-9]+]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP4]], i32 noundef 0), !dbg [[DBG24]]
// CHECK-NEXT:    [[ATOMIC_TEMP13_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP13]], i32 0, i32 0, !dbg [[DBG24]]
// CHECK-NEXT:    [[ATOMIC_TEMP13_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP13_REALP]], align 4, !dbg [[DBG24]]
// CHECK-NEXT:    [[ATOMIC_TEMP13_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP13]], i32 0, i32 1, !dbg [[DBG24]]
// CHECK-NEXT:    [[ATOMIC_TEMP13_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP13_IMAGP]], align 4, !dbg [[DBG24]]
// CHECK-NEXT:    store i32 [[ATOMIC_TEMP13_REAL]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG24]]
// CHECK-NEXT:    store i32 [[ATOMIC_TEMP13_IMAG]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG24]]
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast { float, float }* [[ATOMIC_TEMP14]] to i8*, !dbg [[DBG25:![0-9]+]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ float, float }* @cfx to i8*), i8* noundef [[TMP5]], i32 noundef 0), !dbg [[DBG25]]
// CHECK-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG25]]
// CHECK-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, float* [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG25]]
// CHECK-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG25]]
// CHECK-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, float* [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG25]]
// CHECK-NEXT:    store float [[ATOMIC_TEMP14_REAL]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG25]]
// CHECK-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG25]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast { double, double }* [[ATOMIC_TEMP15]] to i8*, !dbg [[DBG26:![0-9]+]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast ({ double, double }* @cdx to i8*), i8* noundef [[TMP6]], i32 noundef 5), !dbg [[DBG26]]
// CHECK-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG26]]
// CHECK-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, double* [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG26]]
// CHECK-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG26]]
// CHECK-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, double* [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG26]]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    store double [[ATOMIC_TEMP15_REAL]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG26]]
// CHECK-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG26]]
// CHECK-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG27:![0-9]+]]
// CHECK-NEXT:    [[TOBOOL17:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG27]]
// CHECK-NEXT:    [[FROMBOOL18:%.*]] = zext i1 [[TOBOOL17]] to i8, !dbg [[DBG27]]
// CHECK-NEXT:    store i8 [[FROMBOOL18]], i8* @bv, align 1, !dbg [[DBG27]]
// CHECK-NEXT:    [[ATOMIC_LOAD19:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG28:![0-9]+]]
// CHECK-NEXT:    [[TOBOOL20:%.*]] = trunc i8 [[ATOMIC_LOAD19]] to i1, !dbg [[DBG28]]
// CHECK-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL20]] to i8, !dbg [[DBG28]]
// CHECK-NEXT:    store i8 [[CONV]], i8* @cv, align 1, !dbg [[DBG28]]
// CHECK-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i8, i8* @cx seq_cst, align 1, !dbg [[DBG29:![0-9]+]]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    store i8 [[ATOMIC_LOAD21]], i8* @ucv, align 1, !dbg [[DBG29]]
// CHECK-NEXT:    [[ATOMIC_LOAD22:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// CHECK-NEXT:    [[CONV23:%.*]] = trunc i64 [[ATOMIC_LOAD22]] to i16, !dbg [[DBG30]]
// CHECK-NEXT:    store i16 [[CONV23]], i16* @sv, align 2, !dbg [[DBG30]]
// CHECK-NEXT:    [[ATOMIC_LOAD24:%.*]] = load atomic i64, i64* @lx monotonic, align 8, !dbg [[DBG31:![0-9]+]]
// CHECK-NEXT:    [[CONV25:%.*]] = trunc i64 [[ATOMIC_LOAD24]] to i16, !dbg [[DBG31]]
// CHECK-NEXT:    store i16 [[CONV25]], i16* @usv, align 2, !dbg [[DBG31]]
// CHECK-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, i32* @uix seq_cst, align 4, !dbg [[DBG32:![0-9]+]]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    store i32 [[ATOMIC_LOAD26]], i32* @iv, align 4, !dbg [[DBG32]]
// CHECK-NEXT:    [[ATOMIC_LOAD27:%.*]] = load atomic i32, i32* @ix monotonic, align 4, !dbg [[DBG33:![0-9]+]]
// CHECK-NEXT:    store i32 [[ATOMIC_LOAD27]], i32* @uiv, align 4, !dbg [[DBG33]]
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP28]] to i8*, !dbg [[DBG34:![0-9]+]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP7]], i32 noundef 0), !dbg [[DBG34]]
// CHECK-NEXT:    [[ATOMIC_TEMP28_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP28]], i32 0, i32 0, !dbg [[DBG34]]
// CHECK-NEXT:    [[ATOMIC_TEMP28_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP28_REALP]], align 4, !dbg [[DBG34]]
// CHECK-NEXT:    [[ATOMIC_TEMP28_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP28]], i32 0, i32 1, !dbg [[DBG34]]
// CHECK-NEXT:    [[ATOMIC_TEMP28_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP28_IMAGP]], align 4, !dbg [[DBG34]]
// CHECK-NEXT:    [[CONV29:%.*]] = sext i32 [[ATOMIC_TEMP28_REAL]] to i64, !dbg [[DBG34]]
// CHECK-NEXT:    store i64 [[CONV29]], i64* @lv, align 8, !dbg [[DBG34]]
// CHECK-NEXT:    [[ATOMIC_LOAD30:%.*]] = load atomic i32, i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG35:![0-9]+]]
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i32 [[ATOMIC_LOAD30]] to float, !dbg [[DBG35]]
// CHECK-NEXT:    [[CONV31:%.*]] = fptoui float [[TMP8]] to i64, !dbg [[DBG35]]
// CHECK-NEXT:    store i64 [[CONV31]], i64* @ulv, align 8, !dbg [[DBG35]]
// CHECK-NEXT:    [[ATOMIC_LOAD32:%.*]] = load atomic i64, i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG36:![0-9]+]]
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i64 [[ATOMIC_LOAD32]] to double, !dbg [[DBG36]]
// CHECK-NEXT:    [[CONV33:%.*]] = fptosi double [[TMP9]] to i64, !dbg [[DBG36]]
// CHECK-NEXT:    store i64 [[CONV33]], i64* @llv, align 8, !dbg [[DBG36]]
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast x86_fp80* [[ATOMIC_TEMP34]] to i8*, !dbg [[DBG37:![0-9]+]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (x86_fp80* @ldx to i8*), i8* noundef [[TMP10]], i32 noundef 0), !dbg [[DBG37]]
// CHECK-NEXT:    [[TMP11:%.*]] = load x86_fp80, x86_fp80* [[ATOMIC_TEMP34]], align 16, !dbg [[DBG37]]
// CHECK-NEXT:    [[CONV35:%.*]] = fptoui x86_fp80 [[TMP11]] to i64, !dbg [[DBG37]]
// CHECK-NEXT:    store i64 [[CONV35]], i64* @ullv, align 8, !dbg [[DBG37]]
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP36]] to i8*, !dbg [[DBG38:![0-9]+]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP12]], i32 noundef 0), !dbg [[DBG38]]
// CHECK-NEXT:    [[ATOMIC_TEMP36_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP36]], i32 0, i32 0, !dbg [[DBG38]]
// CHECK-NEXT:    [[ATOMIC_TEMP36_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP36_REALP]], align 4, !dbg [[DBG38]]
// CHECK-NEXT:    [[ATOMIC_TEMP36_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP36]], i32 0, i32 1, !dbg [[DBG38]]
// CHECK-NEXT:    [[ATOMIC_TEMP36_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP36_IMAGP]], align 4, !dbg [[DBG38]]
// CHECK-NEXT:    [[CONV37:%.*]] = sitofp i32 [[ATOMIC_TEMP36_REAL]] to float, !dbg [[DBG38]]
// CHECK-NEXT:    store float [[CONV37]], float* @fv, align 4, !dbg [[DBG38]]
// CHECK-NEXT:    [[ATOMIC_LOAD38:%.*]] = load atomic i16, i16* @sx monotonic, align 2, !dbg [[DBG39:![0-9]+]]
// CHECK-NEXT:    [[CONV39:%.*]] = sitofp i16 [[ATOMIC_LOAD38]] to double, !dbg [[DBG39]]
// CHECK-NEXT:    store double [[CONV39]], double* @dv, align 8, !dbg [[DBG39]]
// CHECK-NEXT:    [[ATOMIC_LOAD40:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// CHECK-NEXT:    [[TOBOOL41:%.*]] = trunc i8 [[ATOMIC_LOAD40]] to i1, !dbg [[DBG40]]
// CHECK-NEXT:    [[CONV42:%.*]] = uitofp i1 [[TOBOOL41]] to x86_fp80, !dbg [[DBG40]]
// CHECK-NEXT:    store x86_fp80 [[CONV42]], x86_fp80* @ldv, align 16, !dbg [[DBG40]]
// CHECK-NEXT:    [[ATOMIC_LOAD43:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG41:![0-9]+]]
// CHECK-NEXT:    [[TOBOOL44:%.*]] = trunc i8 [[ATOMIC_LOAD43]] to i1, !dbg [[DBG41]]
// CHECK-NEXT:    [[CONV45:%.*]] = zext i1 [[TOBOOL44]] to i32, !dbg [[DBG41]]
// CHECK-NEXT:    store i32 [[CONV45]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG41]]
// CHECK-NEXT:    store i32 0, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG41]]
// CHECK-NEXT:    [[ATOMIC_LOAD46:%.*]] = load atomic i16, i16* @usx monotonic, align 2, !dbg [[DBG42:![0-9]+]]
// CHECK-NEXT:    [[CONV47:%.*]] = uitofp i16 [[ATOMIC_LOAD46]] to float, !dbg [[DBG42]]
// CHECK-NEXT:    store float [[CONV47]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG42]]
// CHECK-NEXT:    store float 0.000000e+00, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG42]]
// CHECK-NEXT:    [[ATOMIC_LOAD48:%.*]] = load atomic i64, i64* @llx monotonic, align 8, !dbg [[DBG43:![0-9]+]]
// CHECK-NEXT:    [[CONV49:%.*]] = sitofp i64 [[ATOMIC_LOAD48]] to double, !dbg [[DBG43]]
// CHECK-NEXT:    store double [[CONV49]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG43]]
// CHECK-NEXT:    store double 0.000000e+00, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG43]]
// CHECK-NEXT:    [[TMP13:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP50]] to i8*, !dbg [[DBG44:![0-9]+]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (<4 x i32>* @int4x to i8*), i8* noundef [[TMP13]], i32 noundef 0), !dbg [[DBG44]]
// CHECK-NEXT:    [[TMP14:%.*]] = load <4 x i32>, <4 x i32>* [[ATOMIC_TEMP50]], align 16, !dbg [[DBG44]]
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP14]], i32 0, !dbg [[DBG44]]
// CHECK-NEXT:    [[TOBOOL51:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG44]]
// CHECK-NEXT:    [[FROMBOOL52:%.*]] = zext i1 [[TOBOOL51]] to i8, !dbg [[DBG44]]
// CHECK-NEXT:    store i8 [[FROMBOOL52]], i8* @bv, align 1, !dbg [[DBG44]]
// CHECK-NEXT:    [[ATOMIC_LOAD53:%.*]] = load atomic i32, i32* bitcast (i8* getelementptr (i8, i8* bitcast (%struct.BitFields* @bfx to i8*), i64 4) to i32*) monotonic, align 4, !dbg [[DBG45:![0-9]+]]
// CHECK-NEXT:    store i32 [[ATOMIC_LOAD53]], i32* [[ATOMIC_TEMP54]], align 4, !dbg [[DBG45]]
// CHECK-NEXT:    [[BF_LOAD:%.*]] = load i32, i32* [[ATOMIC_TEMP54]], align 4, !dbg [[DBG45]]
// CHECK-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD]], 1, !dbg [[DBG45]]
// CHECK-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_SHL]], 1, !dbg [[DBG45]]
// CHECK-NEXT:    [[CONV55:%.*]] = sitofp i32 [[BF_ASHR]] to x86_fp80, !dbg [[DBG45]]
// CHECK-NEXT:    store x86_fp80 [[CONV55]], x86_fp80* @ldv, align 16, !dbg [[DBG45]]
// CHECK-NEXT:    [[TMP15:%.*]] = bitcast i32* [[ATOMIC_TEMP56]] to i8*, !dbg [[DBG46:![0-9]+]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 4, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i64 4), i8* noundef [[TMP15]], i32 noundef 0), !dbg [[DBG46]]
// CHECK-NEXT:    [[BF_LOAD57:%.*]] = load i32, i32* [[ATOMIC_TEMP56]], align 1, !dbg [[DBG46]]
// CHECK-NEXT:    [[BF_SHL58:%.*]] = shl i32 [[BF_LOAD57]], 1, !dbg [[DBG46]]
// CHECK-NEXT:    [[BF_ASHR59:%.*]] = ashr i32 [[BF_SHL58]], 1, !dbg [[DBG46]]
// CHECK-NEXT:    [[CONV60:%.*]] = sitofp i32 [[BF_ASHR59]] to x86_fp80, !dbg [[DBG46]]
// CHECK-NEXT:    store x86_fp80 [[CONV60]], x86_fp80* @ldv, align 16, !dbg [[DBG46]]
// CHECK-NEXT:    [[ATOMIC_LOAD61:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS2:%.*]], %struct.BitFields2* @bfx2, i32 0, i32 0) monotonic, align 4, !dbg [[DBG47:![0-9]+]]
// CHECK-NEXT:    store i32 [[ATOMIC_LOAD61]], i32* [[ATOMIC_TEMP62]], align 4, !dbg [[DBG47]]
// CHECK-NEXT:    [[BF_LOAD63:%.*]] = load i32, i32* [[ATOMIC_TEMP62]], align 4, !dbg [[DBG47]]
// CHECK-NEXT:    [[BF_ASHR64:%.*]] = ashr i32 [[BF_LOAD63]], 31, !dbg [[DBG47]]
// CHECK-NEXT:    [[CONV65:%.*]] = sitofp i32 [[BF_ASHR64]] to x86_fp80, !dbg [[DBG47]]
// CHECK-NEXT:    store x86_fp80 [[CONV65]], x86_fp80* @ldv, align 16, !dbg [[DBG47]]
// CHECK-NEXT:    [[ATOMIC_LOAD66:%.*]] = load atomic i8, i8* getelementptr (i8, i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*), i64 3) monotonic, align 1, !dbg [[DBG48:![0-9]+]]
// CHECK-NEXT:    [[TMP16:%.*]] = bitcast i32* [[ATOMIC_TEMP67]] to i8*, !dbg [[DBG48]]
// CHECK-NEXT:    store i8 [[ATOMIC_LOAD66]], i8* [[TMP16]], align 1, !dbg [[DBG48]]
// CHECK-NEXT:    [[BF_LOAD68:%.*]] = load i8, i8* [[TMP16]], align 1, !dbg [[DBG48]]
// CHECK-NEXT:    [[BF_ASHR69:%.*]] = ashr i8 [[BF_LOAD68]], 7, !dbg [[DBG48]]
// CHECK-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR69]] to i32, !dbg [[DBG48]]
// CHECK-NEXT:    [[CONV70:%.*]] = sitofp i32 [[BF_CAST]] to x86_fp80, !dbg [[DBG48]]
// CHECK-NEXT:    store x86_fp80 [[CONV70]], x86_fp80* @ldv, align 16, !dbg [[DBG48]]
// CHECK-NEXT:    [[ATOMIC_LOAD71:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS3:%.*]], %struct.BitFields3* @bfx3, i32 0, i32 0) monotonic, align 4, !dbg [[DBG49:![0-9]+]]
// CHECK-NEXT:    store i32 [[ATOMIC_LOAD71]], i32* [[ATOMIC_TEMP72]], align 4, !dbg [[DBG49]]
// CHECK-NEXT:    [[BF_LOAD73:%.*]] = load i32, i32* [[ATOMIC_TEMP72]], align 4, !dbg [[DBG49]]
// CHECK-NEXT:    [[BF_SHL74:%.*]] = shl i32 [[BF_LOAD73]], 7, !dbg [[DBG49]]
// CHECK-NEXT:    [[BF_ASHR75:%.*]] = ashr i32 [[BF_SHL74]], 18, !dbg [[DBG49]]
// CHECK-NEXT:    [[CONV76:%.*]] = sitofp i32 [[BF_ASHR75]] to x86_fp80, !dbg [[DBG49]]
// CHECK-NEXT:    store x86_fp80 [[CONV76]], x86_fp80* @ldv, align 16, !dbg [[DBG49]]
// CHECK-NEXT:    [[TMP17:%.*]] = bitcast i32* [[ATOMIC_TEMP77]] to i24*, !dbg [[DBG50:![0-9]+]]
// CHECK-NEXT:    [[TMP18:%.*]] = bitcast i24* [[TMP17]] to i8*, !dbg [[DBG50]]
// CHECK-NEXT:    call void @__atomic_load(i64 noundef 3, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i64 1), i8* noundef [[TMP18]], i32 noundef 0), !dbg [[DBG50]]
// CHECK-NEXT:    [[BF_LOAD78:%.*]] = load i24, i24* [[TMP17]], align 1, !dbg [[DBG50]]
// CHECK-NEXT:    [[BF_SHL79:%.*]] = shl i24 [[BF_LOAD78]], 7, !dbg [[DBG50]]
// CHECK-NEXT:    [[BF_ASHR80:%.*]] = ashr i24 [[BF_SHL79]], 10, !dbg [[DBG50]]
// CHECK-NEXT:    [[BF_CAST81:%.*]] = sext i24 [[BF_ASHR80]] to i32, !dbg [[DBG50]]
// CHECK-NEXT:    [[CONV82:%.*]] = sitofp i32 [[BF_CAST81]] to x86_fp80, !dbg [[DBG50]]
// CHECK-NEXT:    store x86_fp80 [[CONV82]], x86_fp80* @ldv, align 16, !dbg [[DBG50]]
// CHECK-NEXT:    [[ATOMIC_LOAD83:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG51:![0-9]+]]
// CHECK-NEXT:    store i64 [[ATOMIC_LOAD83]], i64* [[ATOMIC_TEMP84]], align 8, !dbg [[DBG51]]
// CHECK-NEXT:    [[BF_LOAD85:%.*]] = load i64, i64* [[ATOMIC_TEMP84]], align 8, !dbg [[DBG51]]
// CHECK-NEXT:    [[BF_SHL86:%.*]] = shl i64 [[BF_LOAD85]], 47, !dbg [[DBG51]]
// CHECK-NEXT:    [[BF_ASHR87:%.*]] = ashr i64 [[BF_SHL86]], 63, !dbg [[DBG51]]
// CHECK-NEXT:    [[BF_CAST88:%.*]] = trunc i64 [[BF_ASHR87]] to i32, !dbg [[DBG51]]
// CHECK-NEXT:    [[CONV89:%.*]] = sitofp i32 [[BF_CAST88]] to x86_fp80, !dbg [[DBG51]]
// CHECK-NEXT:    store x86_fp80 [[CONV89]], x86_fp80* @ldv, align 16, !dbg [[DBG51]]
// CHECK-NEXT:    [[ATOMIC_LOAD90:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) monotonic, align 1, !dbg [[DBG52:![0-9]+]]
// CHECK-NEXT:    [[TMP19:%.*]] = bitcast i32* [[ATOMIC_TEMP91]] to i8*, !dbg [[DBG52]]
// CHECK-NEXT:    store i8 [[ATOMIC_LOAD90]], i8* [[TMP19]], align 1, !dbg [[DBG52]]
// CHECK-NEXT:    [[BF_LOAD92:%.*]] = load i8, i8* [[TMP19]], align 1, !dbg [[DBG52]]
// CHECK-NEXT:    [[BF_SHL93:%.*]] = shl i8 [[BF_LOAD92]], 7, !dbg [[DBG52]]
// CHECK-NEXT:    [[BF_ASHR94:%.*]] = ashr i8 [[BF_SHL93]], 7, !dbg [[DBG52]]
// CHECK-NEXT:    [[BF_CAST95:%.*]] = sext i8 [[BF_ASHR94]] to i32, !dbg [[DBG52]]
// CHECK-NEXT:    [[CONV96:%.*]] = sitofp i32 [[BF_CAST95]] to x86_fp80, !dbg [[DBG52]]
// CHECK-NEXT:    store x86_fp80 [[CONV96]], x86_fp80* @ldv, align 16, !dbg [[DBG52]]
// CHECK-NEXT:    [[ATOMIC_LOAD97:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG53:![0-9]+]]
// CHECK-NEXT:    store i64 [[ATOMIC_LOAD97]], i64* [[ATOMIC_TEMP98]], align 8, !dbg [[DBG53]]
// CHECK-NEXT:    [[BF_LOAD99:%.*]] = load i64, i64* [[ATOMIC_TEMP98]], align 8, !dbg [[DBG53]]
// CHECK-NEXT:    [[BF_SHL100:%.*]] = shl i64 [[BF_LOAD99]], 40, !dbg [[DBG53]]
// CHECK-NEXT:    [[BF_ASHR101:%.*]] = ashr i64 [[BF_SHL100]], 57, !dbg [[DBG53]]
// CHECK-NEXT:    [[CONV102:%.*]] = sitofp i64 [[BF_ASHR101]] to x86_fp80, !dbg [[DBG53]]
// CHECK-NEXT:    store x86_fp80 [[CONV102]], x86_fp80* @ldv, align 16, !dbg [[DBG53]]
// CHECK-NEXT:    [[ATOMIC_LOAD103:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) acquire, align 1, !dbg [[DBG54:![0-9]+]]
// CHECK-NEXT:    [[TMP20:%.*]] = bitcast i64* [[ATOMIC_TEMP104]] to i8*, !dbg [[DBG54]]
// CHECK-NEXT:    store i8 [[ATOMIC_LOAD103]], i8* [[TMP20]], align 1, !dbg [[DBG54]]
// CHECK-NEXT:    [[BF_LOAD105:%.*]] = load i8, i8* [[TMP20]], align 1, !dbg [[DBG54]]
// CHECK-NEXT:    [[BF_ASHR106:%.*]] = ashr i8 [[BF_LOAD105]], 1, !dbg [[DBG54]]
// CHECK-NEXT:    [[BF_CAST107:%.*]] = sext i8 [[BF_ASHR106]] to i64, !dbg [[DBG54]]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    [[CONV108:%.*]] = sitofp i64 [[BF_CAST107]] to x86_fp80, !dbg [[DBG54]]
// CHECK-NEXT:    store x86_fp80 [[CONV108]], x86_fp80* @ldv, align 16, !dbg [[DBG54]]
// CHECK-NEXT:    [[ATOMIC_LOAD109:%.*]] = load atomic i64, i64* bitcast (<2 x float>* @float2x to i64*) monotonic, align 8, !dbg [[DBG55:![0-9]+]]
// CHECK-NEXT:    [[TMP21:%.*]] = bitcast <2 x float>* [[ATOMIC_TEMP110]] to i64*, !dbg [[DBG55]]
// CHECK-NEXT:    store i64 [[ATOMIC_LOAD109]], i64* [[TMP21]], align 8, !dbg [[DBG55]]
// CHECK-NEXT:    [[TMP22:%.*]] = load <2 x float>, <2 x float>* [[ATOMIC_TEMP110]], align 8, !dbg [[DBG55]]
// CHECK-NEXT:    [[TMP23:%.*]] = extractelement <2 x float> [[TMP22]], i64 0, !dbg [[DBG55]]
// CHECK-NEXT:    [[CONV111:%.*]] = fptoui float [[TMP23]] to i64, !dbg [[DBG55]]
// CHECK-NEXT:    store i64 [[CONV111]], i64* @ulv, align 8, !dbg [[DBG55]]
// CHECK-NEXT:    [[TMP24:%.*]] = call i32 @llvm.read_register.i32(metadata [[META2:![0-9]+]]), !dbg [[DBG56:![0-9]+]]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    [[CONV112:%.*]] = sitofp i32 [[TMP24]] to double, !dbg [[DBG56]]
// CHECK-NEXT:    store double [[CONV112]], double* @dv, align 8, !dbg [[DBG56]]
// CHECK-NEXT:    ret i32 0, !dbg [[DBG57:![0-9]+]]
//
int main(void) {
#pragma oss atomic read
  bv = bx;
#pragma oss atomic read
  cv = cx;
#pragma oss atomic read
  ucv = ucx;
#pragma oss atomic read
  sv = sx;
#pragma oss atomic read
  usv = usx;
#pragma oss atomic read
  iv = ix;
#pragma oss atomic read
  uiv = uix;
#pragma oss atomic read
  lv = lx;
#pragma oss atomic read
  ulv = ulx;
#pragma oss atomic read
  llv = llx;
#pragma oss atomic read
  ullv = ullx;
#pragma oss atomic read
  fv = fx;
#pragma oss atomic read
  dv = dx;
#pragma oss atomic read
  ldv = ldx;
#pragma oss atomic read
  civ = cix;
#pragma oss atomic read
  cfv = cfx;
#pragma oss atomic seq_cst read
  cdv = cdx;
#pragma oss atomic read
  bv = ulx;
#pragma oss atomic read
  cv = bx;
#pragma oss atomic read seq_cst
  ucv = cx;
#pragma oss atomic read
  sv = ulx;
#pragma oss atomic read
  usv = lx;
#pragma oss atomic seq_cst, read
  iv = uix;
#pragma oss atomic read
  uiv = ix;
#pragma oss atomic read
  lv = cix;
#pragma oss atomic read
  ulv = fx;
#pragma oss atomic read
  llv = dx;
#pragma oss atomic read
  ullv = ldx;
#pragma oss atomic read
  fv = cix;
#pragma oss atomic read
  dv = sx;
#pragma oss atomic read
  ldv = bx;
#pragma oss atomic read
  civ = bx;
#pragma oss atomic read
  cfv = usx;
#pragma oss atomic read
  cdv = llx;
#pragma oss atomic read
  bv = int4x[0];
#pragma oss atomic read
  ldv = bfx.a;
#pragma oss atomic read
  ldv = bfx_packed.a;
#pragma oss atomic read
  ldv = bfx2.a;
#pragma oss atomic read
  ldv = bfx2_packed.a;
#pragma oss atomic read
  ldv = bfx3.a;
#pragma oss atomic read
  ldv = bfx3_packed.a;
#pragma oss atomic read
  ldv = bfx4.a;
#pragma oss atomic relaxed read
  ldv = bfx4_packed.a;
#pragma oss atomic read relaxed
  ldv = bfx4.b;
#pragma oss atomic read acquire
  ldv = bfx4_packed.b;
#pragma oss atomic read
  ulv = float2x.x;
#pragma oss atomic read seq_cst
  dv = rix;
  return 0;
}

#endif
