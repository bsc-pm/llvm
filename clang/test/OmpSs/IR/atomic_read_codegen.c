// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs
// RUN: %clang_cc1 -x c -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -x c -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -x c -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics
#ifndef HEADER
#define HEADER

_Bool bv, bx;
char cv, cx;
unsigned char ucv, ucx;
short sv, sx;
unsigned short usv, usx;
int iv, ix;
unsigned int uiv, uix;
long lv, lx;
unsigned long ulv, ulx;
long long llv, llx;
unsigned long long ullv, ullx;
float fv, fx;
double dv, dx;
long double ldv, ldx;
_Complex int civ, cix;
_Complex float cfv, cfx;
_Complex double cdv, cdx;

typedef int int4 __attribute__((__vector_size__(16)));
int4 int4x;

struct BitFields {
  int : 32;
  int a : 31;
} bfx;

struct BitFields_packed {
  int : 32;
  int a : 31;
} __attribute__ ((__packed__)) bfx_packed;

struct BitFields2 {
  int : 31;
  int a : 1;
} bfx2;

struct BitFields2_packed {
  int : 31;
  int a : 1;
} __attribute__ ((__packed__)) bfx2_packed;

struct BitFields3 {
  int : 11;
  int a : 14;
} bfx3;

struct BitFields3_packed {
  int : 11;
  int a : 14;
} __attribute__ ((__packed__)) bfx3_packed;

struct BitFields4 {
  short : 16;
  int a: 1;
  long b : 7;
} bfx4;

struct BitFields4_packed {
  short : 16;
  int a: 1;
  long b : 7;
} __attribute__ ((__packed__)) bfx4_packed;

typedef float float2 __attribute__((ext_vector_type(2)));
float2 float2x;

#if defined(__x86_64__)
// Register "0" is currently an invalid register for global register variables.
// Use "esp" instead of "0".
// register int rix __asm__("0");
register int rix __asm__("esp");
#endif

int main(void) {
#pragma oss atomic read
  bv = bx;
#pragma oss atomic read
  cv = cx;
#pragma oss atomic read
  ucv = ucx;
#pragma oss atomic read
  sv = sx;
#pragma oss atomic read
  usv = usx;
#pragma oss atomic read
  iv = ix;
#pragma oss atomic read
  uiv = uix;
#pragma oss atomic read
  lv = lx;
#pragma oss atomic read
  ulv = ulx;
#pragma oss atomic read
  llv = llx;
#pragma oss atomic read
  ullv = ullx;
#pragma oss atomic read
  fv = fx;
#pragma oss atomic read
  dv = dx;
#pragma oss atomic read
  ldv = ldx;
#pragma oss atomic read
  civ = cix;
#pragma oss atomic read
  cfv = cfx;
#pragma oss atomic seq_cst read
  cdv = cdx;
#pragma oss atomic read
  bv = ulx;
#pragma oss atomic read
  cv = bx;
#pragma oss atomic read seq_cst
  ucv = cx;
#pragma oss atomic read
  sv = ulx;
#pragma oss atomic read
  usv = lx;
#pragma oss atomic seq_cst, read
  iv = uix;
#pragma oss atomic read
  uiv = ix;
#pragma oss atomic read
  lv = cix;
#pragma oss atomic read
  ulv = fx;
#pragma oss atomic read
  llv = dx;
#pragma oss atomic read
  ullv = ldx;
#pragma oss atomic read
  fv = cix;
#pragma oss atomic read
  dv = sx;
#pragma oss atomic read
  ldv = bx;
#pragma oss atomic read
  civ = bx;
#pragma oss atomic read
  cfv = usx;
#pragma oss atomic read
  cdv = llx;
#pragma oss atomic read
  bv = int4x[0];
#pragma oss atomic read
  ldv = bfx.a;
#pragma oss atomic read
  ldv = bfx_packed.a;
#pragma oss atomic read
  ldv = bfx2.a;
#pragma oss atomic read
  ldv = bfx2_packed.a;
#pragma oss atomic read
  ldv = bfx3.a;
#pragma oss atomic read
  ldv = bfx3_packed.a;
#pragma oss atomic read
  ldv = bfx4.a;
#pragma oss atomic relaxed read
  ldv = bfx4_packed.a;
#pragma oss atomic read relaxed
  ldv = bfx4.b;
#pragma oss atomic read acquire
  ldv = bfx4_packed.b;
#pragma oss atomic read
  ulv = float2x.x;
#if defined(__x86_64__)
#pragma oss atomic read seq_cst
  dv = rix;
#endif
  return 0;
}

#endif
// LIN64-LABEL: define {{[^@]+}}@main
// LIN64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG6:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca x86_fp80, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP13:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP27:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP33:%.*]] = alloca x86_fp80, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP35:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP49:%.*]] = alloca <4 x i32>, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP53:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP55:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP61:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP66:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP71:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP76:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP83:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP90:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP97:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP103:%.*]] = alloca i64, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP109:%.*]] = alloca <2 x float>, align 8
// LIN64-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// LIN64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// LIN64-NEXT:    [[LOADEDV:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG10]]
// LIN64-NEXT:    [[STOREDV:%.*]] = zext i1 [[LOADEDV]] to i8, !dbg [[DBG10]]
// LIN64-NEXT:    store i8 [[STOREDV]], ptr @bv, align 1, !dbg [[DBG10]]
// LIN64-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, ptr @cx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD1]], ptr @cv, align 1, !dbg [[DBG11]]
// LIN64-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, ptr @ucx monotonic, align 1, !dbg [[DBG12:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD2]], ptr @ucv, align 1, !dbg [[DBG12]]
// LIN64-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    store i16 [[ATOMIC_LOAD3]], ptr @sv, align 2, !dbg [[DBG13]]
// LIN64-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    store i16 [[ATOMIC_LOAD4]], ptr @usv, align 2, !dbg [[DBG14]]
// LIN64-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD5]], ptr @iv, align 4, !dbg [[DBG15]]
// LIN64-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, ptr @uix monotonic, align 4, !dbg [[DBG16:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD6]], ptr @uiv, align 4, !dbg [[DBG16]]
// LIN64-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD7]], ptr @lv, align 8, !dbg [[DBG17]]
// LIN64-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD8]], ptr @ulv, align 8, !dbg [[DBG18]]
// LIN64-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD9]], ptr @llv, align 8, !dbg [[DBG19]]
// LIN64-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, ptr @ullx monotonic, align 8, !dbg [[DBG20:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD10]], ptr @ullv, align 8, !dbg [[DBG20]]
// LIN64-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG21:![0-9]+]]
// LIN64-NEXT:    store float [[ATOMIC_LOAD11]], ptr @fv, align 4, !dbg [[DBG21]]
// LIN64-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG22:![0-9]+]]
// LIN64-NEXT:    store double [[ATOMIC_LOAD12]], ptr @dv, align 8, !dbg [[DBG22]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @ldx, ptr noundef [[ATOMIC_TEMP]], i32 noundef 0), !dbg [[DBG23:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = load x86_fp80, ptr [[ATOMIC_TEMP]], align 16, !dbg [[DBG23]]
// LIN64-NEXT:    store x86_fp80 [[TMP0]], ptr @ldv, align 16, !dbg [[DBG23]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP13]], i32 noundef 0), !dbg [[DBG24:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_REALP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP13]], i32 0, i32 0, !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP13_REALP]], align 4, !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_IMAGP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP13]], i32 0, i32 1, !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP13_IMAGP]], align 4, !dbg [[DBG24]]
// LIN64-NEXT:    store i32 [[ATOMIC_TEMP13_REAL]], ptr @civ, align 4, !dbg [[DBG24]]
// LIN64-NEXT:    store i32 [[ATOMIC_TEMP13_IMAG]], ptr getelementptr inbounds nuw ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG24]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cfx, ptr noundef [[ATOMIC_TEMP14]], i32 noundef 0), !dbg [[DBG25:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds nuw { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, ptr [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds nuw { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, ptr [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG25]]
// LIN64-NEXT:    store float [[ATOMIC_TEMP14_REAL]], ptr @cfv, align 4, !dbg [[DBG25]]
// LIN64-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], ptr getelementptr inbounds nuw ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG25]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @cdx, ptr noundef [[ATOMIC_TEMP15]], i32 noundef 5), !dbg [[DBG26:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, ptr [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, ptr [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG26]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    store double [[ATOMIC_TEMP15_REAL]], ptr @cdv, align 8, !dbg [[DBG26]]
// LIN64-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], ptr getelementptr inbounds nuw ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG27:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG27]]
// LIN64-NEXT:    [[STOREDV17:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG27]]
// LIN64-NEXT:    store i8 [[STOREDV17]], ptr @bv, align 1, !dbg [[DBG27]]
// LIN64-NEXT:    [[ATOMIC_LOAD18:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG28:![0-9]+]]
// LIN64-NEXT:    [[LOADEDV19:%.*]] = trunc i8 [[ATOMIC_LOAD18]] to i1, !dbg [[DBG28]]
// LIN64-NEXT:    [[CONV:%.*]] = zext i1 [[LOADEDV19]] to i8, !dbg [[DBG28]]
// LIN64-NEXT:    store i8 [[CONV]], ptr @cv, align 1, !dbg [[DBG28]]
// LIN64-NEXT:    [[ATOMIC_LOAD20:%.*]] = load atomic i8, ptr @cx seq_cst, align 1, !dbg [[DBG29:![0-9]+]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD20]], ptr @ucv, align 1, !dbg [[DBG29]]
// LIN64-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// LIN64-NEXT:    [[CONV22:%.*]] = trunc i64 [[ATOMIC_LOAD21]] to i16, !dbg [[DBG30]]
// LIN64-NEXT:    store i16 [[CONV22]], ptr @sv, align 2, !dbg [[DBG30]]
// LIN64-NEXT:    [[ATOMIC_LOAD23:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG31:![0-9]+]]
// LIN64-NEXT:    [[CONV24:%.*]] = trunc i64 [[ATOMIC_LOAD23]] to i16, !dbg [[DBG31]]
// LIN64-NEXT:    store i16 [[CONV24]], ptr @usv, align 2, !dbg [[DBG31]]
// LIN64-NEXT:    [[ATOMIC_LOAD25:%.*]] = load atomic i32, ptr @uix seq_cst, align 4, !dbg [[DBG32:![0-9]+]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD25]], ptr @iv, align 4, !dbg [[DBG32]]
// LIN64-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG33:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD26]], ptr @uiv, align 4, !dbg [[DBG33]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP27]], i32 noundef 0), !dbg [[DBG34:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP27_REALP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP27]], i32 0, i32 0, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP27_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP27_REALP]], align 4, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP27_IMAGP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP27]], i32 0, i32 1, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP27_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP27_IMAGP]], align 4, !dbg [[DBG34]]
// LIN64-NEXT:    [[CONV28:%.*]] = sext i32 [[ATOMIC_TEMP27_REAL]] to i64, !dbg [[DBG34]]
// LIN64-NEXT:    store i64 [[CONV28]], ptr @lv, align 8, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_LOAD29:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG35:![0-9]+]]
// LIN64-NEXT:    [[CONV30:%.*]] = fptoui float [[ATOMIC_LOAD29]] to i64, !dbg [[DBG35]]
// LIN64-NEXT:    store i64 [[CONV30]], ptr @ulv, align 8, !dbg [[DBG35]]
// LIN64-NEXT:    [[ATOMIC_LOAD31:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG36:![0-9]+]]
// LIN64-NEXT:    [[CONV32:%.*]] = fptosi double [[ATOMIC_LOAD31]] to i64, !dbg [[DBG36]]
// LIN64-NEXT:    store i64 [[CONV32]], ptr @llv, align 8, !dbg [[DBG36]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @ldx, ptr noundef [[ATOMIC_TEMP33]], i32 noundef 0), !dbg [[DBG37:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load x86_fp80, ptr [[ATOMIC_TEMP33]], align 16, !dbg [[DBG37]]
// LIN64-NEXT:    [[CONV34:%.*]] = fptoui x86_fp80 [[TMP1]] to i64, !dbg [[DBG37]]
// LIN64-NEXT:    store i64 [[CONV34]], ptr @ullv, align 8, !dbg [[DBG37]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP35]], i32 noundef 0), !dbg [[DBG38:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP35_REALP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP35]], i32 0, i32 0, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP35_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP35_REALP]], align 4, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP35_IMAGP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP35]], i32 0, i32 1, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP35_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP35_IMAGP]], align 4, !dbg [[DBG38]]
// LIN64-NEXT:    [[CONV36:%.*]] = sitofp i32 [[ATOMIC_TEMP35_REAL]] to float, !dbg [[DBG38]]
// LIN64-NEXT:    store float [[CONV36]], ptr @fv, align 4, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_LOAD37:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG39:![0-9]+]]
// LIN64-NEXT:    [[CONV38:%.*]] = sitofp i16 [[ATOMIC_LOAD37]] to double, !dbg [[DBG39]]
// LIN64-NEXT:    store double [[CONV38]], ptr @dv, align 8, !dbg [[DBG39]]
// LIN64-NEXT:    [[ATOMIC_LOAD39:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// LIN64-NEXT:    [[LOADEDV40:%.*]] = trunc i8 [[ATOMIC_LOAD39]] to i1, !dbg [[DBG40]]
// LIN64-NEXT:    [[CONV41:%.*]] = uitofp i1 [[LOADEDV40]] to x86_fp80, !dbg [[DBG40]]
// LIN64-NEXT:    store x86_fp80 [[CONV41]], ptr @ldv, align 16, !dbg [[DBG40]]
// LIN64-NEXT:    [[ATOMIC_LOAD42:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG41:![0-9]+]]
// LIN64-NEXT:    [[LOADEDV43:%.*]] = trunc i8 [[ATOMIC_LOAD42]] to i1, !dbg [[DBG41]]
// LIN64-NEXT:    [[CONV44:%.*]] = zext i1 [[LOADEDV43]] to i32, !dbg [[DBG41]]
// LIN64-NEXT:    store i32 [[CONV44]], ptr @civ, align 4, !dbg [[DBG41]]
// LIN64-NEXT:    store i32 0, ptr getelementptr inbounds nuw ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG41]]
// LIN64-NEXT:    [[ATOMIC_LOAD45:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG42:![0-9]+]]
// LIN64-NEXT:    [[CONV46:%.*]] = uitofp i16 [[ATOMIC_LOAD45]] to float, !dbg [[DBG42]]
// LIN64-NEXT:    store float [[CONV46]], ptr @cfv, align 4, !dbg [[DBG42]]
// LIN64-NEXT:    store float 0.000000e+00, ptr getelementptr inbounds nuw ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG42]]
// LIN64-NEXT:    [[ATOMIC_LOAD47:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG43:![0-9]+]]
// LIN64-NEXT:    [[CONV48:%.*]] = sitofp i64 [[ATOMIC_LOAD47]] to double, !dbg [[DBG43]]
// LIN64-NEXT:    store double [[CONV48]], ptr @cdv, align 8, !dbg [[DBG43]]
// LIN64-NEXT:    store double 0.000000e+00, ptr getelementptr inbounds nuw ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG43]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @int4x, ptr noundef [[ATOMIC_TEMP49]], i32 noundef 0), !dbg [[DBG44:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ATOMIC_TEMP49]], align 16, !dbg [[DBG44]]
// LIN64-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP2]], i32 0, !dbg [[DBG44]]
// LIN64-NEXT:    [[TOBOOL50:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG44]]
// LIN64-NEXT:    [[STOREDV51:%.*]] = zext i1 [[TOBOOL50]] to i8, !dbg [[DBG44]]
// LIN64-NEXT:    store i8 [[STOREDV51]], ptr @bv, align 1, !dbg [[DBG44]]
// LIN64-NEXT:    [[ATOMIC_LOAD52:%.*]] = load atomic i32, ptr getelementptr (i8, ptr @bfx, i64 4) monotonic, align 4, !dbg [[DBG45:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD52]], ptr [[ATOMIC_TEMP53]], align 4, !dbg [[DBG45]]
// LIN64-NEXT:    [[BF_LOAD:%.*]] = load i32, ptr [[ATOMIC_TEMP53]], align 4, !dbg [[DBG45]]
// LIN64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD]], 1, !dbg [[DBG45]]
// LIN64-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_SHL]], 1, !dbg [[DBG45]]
// LIN64-NEXT:    [[CONV54:%.*]] = sitofp i32 [[BF_ASHR]] to x86_fp80, !dbg [[DBG45]]
// LIN64-NEXT:    store x86_fp80 [[CONV54]], ptr @ldv, align 16, !dbg [[DBG45]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 4, ptr noundef getelementptr (i8, ptr @bfx_packed, i64 4), ptr noundef [[ATOMIC_TEMP55]], i32 noundef 0), !dbg [[DBG46:![0-9]+]]
// LIN64-NEXT:    [[BF_LOAD56:%.*]] = load i32, ptr [[ATOMIC_TEMP55]], align 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[BF_SHL57:%.*]] = shl i32 [[BF_LOAD56]], 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[BF_ASHR58:%.*]] = ashr i32 [[BF_SHL57]], 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[CONV59:%.*]] = sitofp i32 [[BF_ASHR58]] to x86_fp80, !dbg [[DBG46]]
// LIN64-NEXT:    store x86_fp80 [[CONV59]], ptr @ldv, align 16, !dbg [[DBG46]]
// LIN64-NEXT:    [[ATOMIC_LOAD60:%.*]] = load atomic i32, ptr @bfx2 monotonic, align 4, !dbg [[DBG47:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD60]], ptr [[ATOMIC_TEMP61]], align 4, !dbg [[DBG47]]
// LIN64-NEXT:    [[BF_LOAD62:%.*]] = load i32, ptr [[ATOMIC_TEMP61]], align 4, !dbg [[DBG47]]
// LIN64-NEXT:    [[BF_ASHR63:%.*]] = ashr i32 [[BF_LOAD62]], 31, !dbg [[DBG47]]
// LIN64-NEXT:    [[CONV64:%.*]] = sitofp i32 [[BF_ASHR63]] to x86_fp80, !dbg [[DBG47]]
// LIN64-NEXT:    store x86_fp80 [[CONV64]], ptr @ldv, align 16, !dbg [[DBG47]]
// LIN64-NEXT:    [[ATOMIC_LOAD65:%.*]] = load atomic i8, ptr getelementptr (i8, ptr @bfx2_packed, i64 3) monotonic, align 1, !dbg [[DBG48:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD65]], ptr [[ATOMIC_TEMP66]], align 1, !dbg [[DBG48]]
// LIN64-NEXT:    [[BF_LOAD67:%.*]] = load i8, ptr [[ATOMIC_TEMP66]], align 1, !dbg [[DBG48]]
// LIN64-NEXT:    [[BF_ASHR68:%.*]] = ashr i8 [[BF_LOAD67]], 7, !dbg [[DBG48]]
// LIN64-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR68]] to i32, !dbg [[DBG48]]
// LIN64-NEXT:    [[CONV69:%.*]] = sitofp i32 [[BF_CAST]] to x86_fp80, !dbg [[DBG48]]
// LIN64-NEXT:    store x86_fp80 [[CONV69]], ptr @ldv, align 16, !dbg [[DBG48]]
// LIN64-NEXT:    [[ATOMIC_LOAD70:%.*]] = load atomic i32, ptr @bfx3 monotonic, align 4, !dbg [[DBG49:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD70]], ptr [[ATOMIC_TEMP71]], align 4, !dbg [[DBG49]]
// LIN64-NEXT:    [[BF_LOAD72:%.*]] = load i32, ptr [[ATOMIC_TEMP71]], align 4, !dbg [[DBG49]]
// LIN64-NEXT:    [[BF_SHL73:%.*]] = shl i32 [[BF_LOAD72]], 7, !dbg [[DBG49]]
// LIN64-NEXT:    [[BF_ASHR74:%.*]] = ashr i32 [[BF_SHL73]], 18, !dbg [[DBG49]]
// LIN64-NEXT:    [[CONV75:%.*]] = sitofp i32 [[BF_ASHR74]] to x86_fp80, !dbg [[DBG49]]
// LIN64-NEXT:    store x86_fp80 [[CONV75]], ptr @ldv, align 16, !dbg [[DBG49]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 3, ptr noundef getelementptr (i8, ptr @bfx3_packed, i64 1), ptr noundef [[ATOMIC_TEMP76]], i32 noundef 0), !dbg [[DBG50:![0-9]+]]
// LIN64-NEXT:    [[BF_LOAD77:%.*]] = load i24, ptr [[ATOMIC_TEMP76]], align 1, !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_SHL78:%.*]] = shl i24 [[BF_LOAD77]], 7, !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_ASHR79:%.*]] = ashr i24 [[BF_SHL78]], 10, !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_CAST80:%.*]] = sext i24 [[BF_ASHR79]] to i32, !dbg [[DBG50]]
// LIN64-NEXT:    [[CONV81:%.*]] = sitofp i32 [[BF_CAST80]] to x86_fp80, !dbg [[DBG50]]
// LIN64-NEXT:    store x86_fp80 [[CONV81]], ptr @ldv, align 16, !dbg [[DBG50]]
// LIN64-NEXT:    [[ATOMIC_LOAD82:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG51:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD82]], ptr [[ATOMIC_TEMP83]], align 8, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_LOAD84:%.*]] = load i64, ptr [[ATOMIC_TEMP83]], align 8, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_SHL85:%.*]] = shl i64 [[BF_LOAD84]], 47, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_ASHR86:%.*]] = ashr i64 [[BF_SHL85]], 63, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_CAST87:%.*]] = trunc i64 [[BF_ASHR86]] to i32, !dbg [[DBG51]]
// LIN64-NEXT:    [[CONV88:%.*]] = sitofp i32 [[BF_CAST87]] to x86_fp80, !dbg [[DBG51]]
// LIN64-NEXT:    store x86_fp80 [[CONV88]], ptr @ldv, align 16, !dbg [[DBG51]]
// LIN64-NEXT:    [[ATOMIC_LOAD89:%.*]] = load atomic i8, ptr getelementptr inbounds nuw ([[STRUCT_BITFIELDS4_PACKED:%.*]], ptr @bfx4_packed, i32 0, i32 1) monotonic, align 1, !dbg [[DBG52:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD89]], ptr [[ATOMIC_TEMP90]], align 1, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_LOAD91:%.*]] = load i8, ptr [[ATOMIC_TEMP90]], align 1, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_SHL92:%.*]] = shl i8 [[BF_LOAD91]], 7, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_ASHR93:%.*]] = ashr i8 [[BF_SHL92]], 7, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_CAST94:%.*]] = sext i8 [[BF_ASHR93]] to i32, !dbg [[DBG52]]
// LIN64-NEXT:    [[CONV95:%.*]] = sitofp i32 [[BF_CAST94]] to x86_fp80, !dbg [[DBG52]]
// LIN64-NEXT:    store x86_fp80 [[CONV95]], ptr @ldv, align 16, !dbg [[DBG52]]
// LIN64-NEXT:    [[ATOMIC_LOAD96:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG53:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD96]], ptr [[ATOMIC_TEMP97]], align 8, !dbg [[DBG53]]
// LIN64-NEXT:    [[BF_LOAD98:%.*]] = load i64, ptr [[ATOMIC_TEMP97]], align 8, !dbg [[DBG53]]
// LIN64-NEXT:    [[BF_SHL99:%.*]] = shl i64 [[BF_LOAD98]], 40, !dbg [[DBG53]]
// LIN64-NEXT:    [[BF_ASHR100:%.*]] = ashr i64 [[BF_SHL99]], 57, !dbg [[DBG53]]
// LIN64-NEXT:    [[CONV101:%.*]] = sitofp i64 [[BF_ASHR100]] to x86_fp80, !dbg [[DBG53]]
// LIN64-NEXT:    store x86_fp80 [[CONV101]], ptr @ldv, align 16, !dbg [[DBG53]]
// LIN64-NEXT:    [[ATOMIC_LOAD102:%.*]] = load atomic i8, ptr getelementptr inbounds nuw ([[STRUCT_BITFIELDS4_PACKED]], ptr @bfx4_packed, i32 0, i32 1) acquire, align 1, !dbg [[DBG54:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD102]], ptr [[ATOMIC_TEMP103]], align 1, !dbg [[DBG54]]
// LIN64-NEXT:    [[BF_LOAD104:%.*]] = load i8, ptr [[ATOMIC_TEMP103]], align 1, !dbg [[DBG54]]
// LIN64-NEXT:    [[BF_ASHR105:%.*]] = ashr i8 [[BF_LOAD104]], 1, !dbg [[DBG54]]
// LIN64-NEXT:    [[BF_CAST106:%.*]] = sext i8 [[BF_ASHR105]] to i64, !dbg [[DBG54]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    [[CONV107:%.*]] = sitofp i64 [[BF_CAST106]] to x86_fp80, !dbg [[DBG54]]
// LIN64-NEXT:    store x86_fp80 [[CONV107]], ptr @ldv, align 16, !dbg [[DBG54]]
// LIN64-NEXT:    [[ATOMIC_LOAD108:%.*]] = load atomic i64, ptr @float2x monotonic, align 8, !dbg [[DBG55:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD108]], ptr [[ATOMIC_TEMP109]], align 8, !dbg [[DBG55]]
// LIN64-NEXT:    [[TMP3:%.*]] = load <2 x float>, ptr [[ATOMIC_TEMP109]], align 8, !dbg [[DBG55]]
// LIN64-NEXT:    [[TMP4:%.*]] = extractelement <2 x float> [[TMP3]], i64 0, !dbg [[DBG55]]
// LIN64-NEXT:    [[CONV110:%.*]] = fptoui float [[TMP4]] to i64, !dbg [[DBG55]]
// LIN64-NEXT:    store i64 [[CONV110]], ptr @ulv, align 8, !dbg [[DBG55]]
// LIN64-NEXT:    [[TMP5:%.*]] = call i32 @llvm.read_register.i32(metadata [[META2:![0-9]+]]), !dbg [[DBG56:![0-9]+]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    [[CONV111:%.*]] = sitofp i32 [[TMP5]] to double, !dbg [[DBG56]]
// LIN64-NEXT:    store double [[CONV111]], ptr @dv, align 8, !dbg [[DBG56]]
// LIN64-NEXT:    ret i32 0, !dbg [[DBG57:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@main
// PPC64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG6:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca ppc_fp128, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP13:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP27:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP33:%.*]] = alloca ppc_fp128, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP35:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP49:%.*]] = alloca <4 x i32>, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP53:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP55:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP60:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP65:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP71:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP76:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP83:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP90:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP96:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP102:%.*]] = alloca i64, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP109:%.*]] = alloca <2 x float>, align 8
// PPC64-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// PPC64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// PPC64-NEXT:    [[LOADEDV:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG10]]
// PPC64-NEXT:    [[STOREDV:%.*]] = zext i1 [[LOADEDV]] to i8, !dbg [[DBG10]]
// PPC64-NEXT:    store i8 [[STOREDV]], ptr @bv, align 1, !dbg [[DBG10]]
// PPC64-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, ptr @cx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD1]], ptr @cv, align 1, !dbg [[DBG11]]
// PPC64-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, ptr @ucx monotonic, align 1, !dbg [[DBG12:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD2]], ptr @ucv, align 1, !dbg [[DBG12]]
// PPC64-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    store i16 [[ATOMIC_LOAD3]], ptr @sv, align 2, !dbg [[DBG13]]
// PPC64-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    store i16 [[ATOMIC_LOAD4]], ptr @usv, align 2, !dbg [[DBG14]]
// PPC64-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD5]], ptr @iv, align 4, !dbg [[DBG15]]
// PPC64-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, ptr @uix monotonic, align 4, !dbg [[DBG16:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD6]], ptr @uiv, align 4, !dbg [[DBG16]]
// PPC64-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD7]], ptr @lv, align 8, !dbg [[DBG17]]
// PPC64-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD8]], ptr @ulv, align 8, !dbg [[DBG18]]
// PPC64-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD9]], ptr @llv, align 8, !dbg [[DBG19]]
// PPC64-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, ptr @ullx monotonic, align 8, !dbg [[DBG20:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD10]], ptr @ullv, align 8, !dbg [[DBG20]]
// PPC64-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG21:![0-9]+]]
// PPC64-NEXT:    store float [[ATOMIC_LOAD11]], ptr @fv, align 4, !dbg [[DBG21]]
// PPC64-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG22:![0-9]+]]
// PPC64-NEXT:    store double [[ATOMIC_LOAD12]], ptr @dv, align 8, !dbg [[DBG22]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @ldx, ptr noundef [[ATOMIC_TEMP]], i32 noundef signext 0), !dbg [[DBG23:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = load ppc_fp128, ptr [[ATOMIC_TEMP]], align 16, !dbg [[DBG23]]
// PPC64-NEXT:    store ppc_fp128 [[TMP0]], ptr @ldv, align 16, !dbg [[DBG23]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP13]], i32 noundef signext 0), !dbg [[DBG24:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_REALP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP13]], i32 0, i32 0, !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP13_REALP]], align 4, !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_IMAGP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP13]], i32 0, i32 1, !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP13_IMAGP]], align 4, !dbg [[DBG24]]
// PPC64-NEXT:    store i32 [[ATOMIC_TEMP13_REAL]], ptr @civ, align 4, !dbg [[DBG24]]
// PPC64-NEXT:    store i32 [[ATOMIC_TEMP13_IMAG]], ptr getelementptr inbounds nuw ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG24]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cfx, ptr noundef [[ATOMIC_TEMP14]], i32 noundef signext 0), !dbg [[DBG25:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds nuw { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, ptr [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds nuw { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, ptr [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG25]]
// PPC64-NEXT:    store float [[ATOMIC_TEMP14_REAL]], ptr @cfv, align 4, !dbg [[DBG25]]
// PPC64-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], ptr getelementptr inbounds nuw ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG25]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @cdx, ptr noundef [[ATOMIC_TEMP15]], i32 noundef signext 5), !dbg [[DBG26:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG26]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, ptr [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG26]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG26]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, ptr [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG26]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    store double [[ATOMIC_TEMP15_REAL]], ptr @cdv, align 8, !dbg [[DBG26]]
// PPC64-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], ptr getelementptr inbounds nuw ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG26]]
// PPC64-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG27:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG27]]
// PPC64-NEXT:    [[STOREDV17:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG27]]
// PPC64-NEXT:    store i8 [[STOREDV17]], ptr @bv, align 1, !dbg [[DBG27]]
// PPC64-NEXT:    [[ATOMIC_LOAD18:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG28:![0-9]+]]
// PPC64-NEXT:    [[LOADEDV19:%.*]] = trunc i8 [[ATOMIC_LOAD18]] to i1, !dbg [[DBG28]]
// PPC64-NEXT:    [[CONV:%.*]] = zext i1 [[LOADEDV19]] to i8, !dbg [[DBG28]]
// PPC64-NEXT:    store i8 [[CONV]], ptr @cv, align 1, !dbg [[DBG28]]
// PPC64-NEXT:    [[ATOMIC_LOAD20:%.*]] = load atomic i8, ptr @cx seq_cst, align 1, !dbg [[DBG29:![0-9]+]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD20]], ptr @ucv, align 1, !dbg [[DBG29]]
// PPC64-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// PPC64-NEXT:    [[CONV22:%.*]] = trunc i64 [[ATOMIC_LOAD21]] to i16, !dbg [[DBG30]]
// PPC64-NEXT:    store i16 [[CONV22]], ptr @sv, align 2, !dbg [[DBG30]]
// PPC64-NEXT:    [[ATOMIC_LOAD23:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG31:![0-9]+]]
// PPC64-NEXT:    [[CONV24:%.*]] = trunc i64 [[ATOMIC_LOAD23]] to i16, !dbg [[DBG31]]
// PPC64-NEXT:    store i16 [[CONV24]], ptr @usv, align 2, !dbg [[DBG31]]
// PPC64-NEXT:    [[ATOMIC_LOAD25:%.*]] = load atomic i32, ptr @uix seq_cst, align 4, !dbg [[DBG32:![0-9]+]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD25]], ptr @iv, align 4, !dbg [[DBG32]]
// PPC64-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG33:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD26]], ptr @uiv, align 4, !dbg [[DBG33]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP27]], i32 noundef signext 0), !dbg [[DBG34:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP27_REALP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP27]], i32 0, i32 0, !dbg [[DBG34]]
// PPC64-NEXT:    [[ATOMIC_TEMP27_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP27_REALP]], align 4, !dbg [[DBG34]]
// PPC64-NEXT:    [[ATOMIC_TEMP27_IMAGP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP27]], i32 0, i32 1, !dbg [[DBG34]]
// PPC64-NEXT:    [[ATOMIC_TEMP27_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP27_IMAGP]], align 4, !dbg [[DBG34]]
// PPC64-NEXT:    [[CONV28:%.*]] = sext i32 [[ATOMIC_TEMP27_REAL]] to i64, !dbg [[DBG34]]
// PPC64-NEXT:    store i64 [[CONV28]], ptr @lv, align 8, !dbg [[DBG34]]
// PPC64-NEXT:    [[ATOMIC_LOAD29:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG35:![0-9]+]]
// PPC64-NEXT:    [[CONV30:%.*]] = fptoui float [[ATOMIC_LOAD29]] to i64, !dbg [[DBG35]]
// PPC64-NEXT:    store i64 [[CONV30]], ptr @ulv, align 8, !dbg [[DBG35]]
// PPC64-NEXT:    [[ATOMIC_LOAD31:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG36:![0-9]+]]
// PPC64-NEXT:    [[CONV32:%.*]] = fptosi double [[ATOMIC_LOAD31]] to i64, !dbg [[DBG36]]
// PPC64-NEXT:    store i64 [[CONV32]], ptr @llv, align 8, !dbg [[DBG36]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @ldx, ptr noundef [[ATOMIC_TEMP33]], i32 noundef signext 0), !dbg [[DBG37:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load ppc_fp128, ptr [[ATOMIC_TEMP33]], align 16, !dbg [[DBG37]]
// PPC64-NEXT:    [[CONV34:%.*]] = fptoui ppc_fp128 [[TMP1]] to i64, !dbg [[DBG37]]
// PPC64-NEXT:    store i64 [[CONV34]], ptr @ullv, align 8, !dbg [[DBG37]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP35]], i32 noundef signext 0), !dbg [[DBG38:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP35_REALP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP35]], i32 0, i32 0, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_TEMP35_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP35_REALP]], align 4, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_TEMP35_IMAGP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP35]], i32 0, i32 1, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_TEMP35_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP35_IMAGP]], align 4, !dbg [[DBG38]]
// PPC64-NEXT:    [[CONV36:%.*]] = sitofp i32 [[ATOMIC_TEMP35_REAL]] to float, !dbg [[DBG38]]
// PPC64-NEXT:    store float [[CONV36]], ptr @fv, align 4, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_LOAD37:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG39:![0-9]+]]
// PPC64-NEXT:    [[CONV38:%.*]] = sitofp i16 [[ATOMIC_LOAD37]] to double, !dbg [[DBG39]]
// PPC64-NEXT:    store double [[CONV38]], ptr @dv, align 8, !dbg [[DBG39]]
// PPC64-NEXT:    [[ATOMIC_LOAD39:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// PPC64-NEXT:    [[LOADEDV40:%.*]] = trunc i8 [[ATOMIC_LOAD39]] to i1, !dbg [[DBG40]]
// PPC64-NEXT:    [[CONV41:%.*]] = uitofp i1 [[LOADEDV40]] to ppc_fp128, !dbg [[DBG40]]
// PPC64-NEXT:    store ppc_fp128 [[CONV41]], ptr @ldv, align 16, !dbg [[DBG40]]
// PPC64-NEXT:    [[ATOMIC_LOAD42:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG41:![0-9]+]]
// PPC64-NEXT:    [[LOADEDV43:%.*]] = trunc i8 [[ATOMIC_LOAD42]] to i1, !dbg [[DBG41]]
// PPC64-NEXT:    [[CONV44:%.*]] = zext i1 [[LOADEDV43]] to i32, !dbg [[DBG41]]
// PPC64-NEXT:    store i32 [[CONV44]], ptr @civ, align 4, !dbg [[DBG41]]
// PPC64-NEXT:    store i32 0, ptr getelementptr inbounds nuw ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG41]]
// PPC64-NEXT:    [[ATOMIC_LOAD45:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG42:![0-9]+]]
// PPC64-NEXT:    [[CONV46:%.*]] = uitofp i16 [[ATOMIC_LOAD45]] to float, !dbg [[DBG42]]
// PPC64-NEXT:    store float [[CONV46]], ptr @cfv, align 4, !dbg [[DBG42]]
// PPC64-NEXT:    store float 0.000000e+00, ptr getelementptr inbounds nuw ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG42]]
// PPC64-NEXT:    [[ATOMIC_LOAD47:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG43:![0-9]+]]
// PPC64-NEXT:    [[CONV48:%.*]] = sitofp i64 [[ATOMIC_LOAD47]] to double, !dbg [[DBG43]]
// PPC64-NEXT:    store double [[CONV48]], ptr @cdv, align 8, !dbg [[DBG43]]
// PPC64-NEXT:    store double 0.000000e+00, ptr getelementptr inbounds nuw ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG43]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @int4x, ptr noundef [[ATOMIC_TEMP49]], i32 noundef signext 0), !dbg [[DBG44:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ATOMIC_TEMP49]], align 16, !dbg [[DBG44]]
// PPC64-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP2]], i32 0, !dbg [[DBG44]]
// PPC64-NEXT:    [[TOBOOL50:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG44]]
// PPC64-NEXT:    [[STOREDV51:%.*]] = zext i1 [[TOBOOL50]] to i8, !dbg [[DBG44]]
// PPC64-NEXT:    store i8 [[STOREDV51]], ptr @bv, align 1, !dbg [[DBG44]]
// PPC64-NEXT:    [[ATOMIC_LOAD52:%.*]] = load atomic i32, ptr @bfx monotonic, align 4, !dbg [[DBG45:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD52]], ptr [[ATOMIC_TEMP53]], align 4, !dbg [[DBG45]]
// PPC64-NEXT:    [[BF_LOAD:%.*]] = load i32, ptr [[ATOMIC_TEMP53]], align 4, !dbg [[DBG45]]
// PPC64-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_LOAD]], 1, !dbg [[DBG45]]
// PPC64-NEXT:    [[CONV54:%.*]] = sitofp i32 [[BF_ASHR]] to ppc_fp128, !dbg [[DBG45]]
// PPC64-NEXT:    store ppc_fp128 [[CONV54]], ptr @ldv, align 16, !dbg [[DBG45]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 4, ptr noundef @bfx_packed, ptr noundef [[ATOMIC_TEMP55]], i32 noundef signext 0), !dbg [[DBG46:![0-9]+]]
// PPC64-NEXT:    [[BF_LOAD56:%.*]] = load i32, ptr [[ATOMIC_TEMP55]], align 1, !dbg [[DBG46]]
// PPC64-NEXT:    [[BF_ASHR57:%.*]] = ashr i32 [[BF_LOAD56]], 1, !dbg [[DBG46]]
// PPC64-NEXT:    [[CONV58:%.*]] = sitofp i32 [[BF_ASHR57]] to ppc_fp128, !dbg [[DBG46]]
// PPC64-NEXT:    store ppc_fp128 [[CONV58]], ptr @ldv, align 16, !dbg [[DBG46]]
// PPC64-NEXT:    [[ATOMIC_LOAD59:%.*]] = load atomic i32, ptr @bfx2 monotonic, align 4, !dbg [[DBG47:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD59]], ptr [[ATOMIC_TEMP60]], align 4, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_LOAD61:%.*]] = load i32, ptr [[ATOMIC_TEMP60]], align 4, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD61]], 31, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_ASHR62:%.*]] = ashr i32 [[BF_SHL]], 31, !dbg [[DBG47]]
// PPC64-NEXT:    [[CONV63:%.*]] = sitofp i32 [[BF_ASHR62]] to ppc_fp128, !dbg [[DBG47]]
// PPC64-NEXT:    store ppc_fp128 [[CONV63]], ptr @ldv, align 16, !dbg [[DBG47]]
// PPC64-NEXT:    [[ATOMIC_LOAD64:%.*]] = load atomic i8, ptr @bfx2_packed monotonic, align 1, !dbg [[DBG48:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD64]], ptr [[ATOMIC_TEMP65]], align 1, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_LOAD66:%.*]] = load i8, ptr [[ATOMIC_TEMP65]], align 1, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_SHL67:%.*]] = shl i8 [[BF_LOAD66]], 7, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_ASHR68:%.*]] = ashr i8 [[BF_SHL67]], 7, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR68]] to i32, !dbg [[DBG48]]
// PPC64-NEXT:    [[CONV69:%.*]] = sitofp i32 [[BF_CAST]] to ppc_fp128, !dbg [[DBG48]]
// PPC64-NEXT:    store ppc_fp128 [[CONV69]], ptr @ldv, align 16, !dbg [[DBG48]]
// PPC64-NEXT:    [[ATOMIC_LOAD70:%.*]] = load atomic i32, ptr @bfx3 monotonic, align 4, !dbg [[DBG49:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD70]], ptr [[ATOMIC_TEMP71]], align 4, !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_LOAD72:%.*]] = load i32, ptr [[ATOMIC_TEMP71]], align 4, !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_SHL73:%.*]] = shl i32 [[BF_LOAD72]], 11, !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_ASHR74:%.*]] = ashr i32 [[BF_SHL73]], 18, !dbg [[DBG49]]
// PPC64-NEXT:    [[CONV75:%.*]] = sitofp i32 [[BF_ASHR74]] to ppc_fp128, !dbg [[DBG49]]
// PPC64-NEXT:    store ppc_fp128 [[CONV75]], ptr @ldv, align 16, !dbg [[DBG49]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 3, ptr noundef @bfx3_packed, ptr noundef [[ATOMIC_TEMP76]], i32 noundef signext 0), !dbg [[DBG50:![0-9]+]]
// PPC64-NEXT:    [[BF_LOAD77:%.*]] = load i24, ptr [[ATOMIC_TEMP76]], align 1, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_SHL78:%.*]] = shl i24 [[BF_LOAD77]], 3, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_ASHR79:%.*]] = ashr i24 [[BF_SHL78]], 10, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_CAST80:%.*]] = sext i24 [[BF_ASHR79]] to i32, !dbg [[DBG50]]
// PPC64-NEXT:    [[CONV81:%.*]] = sitofp i32 [[BF_CAST80]] to ppc_fp128, !dbg [[DBG50]]
// PPC64-NEXT:    store ppc_fp128 [[CONV81]], ptr @ldv, align 16, !dbg [[DBG50]]
// PPC64-NEXT:    [[ATOMIC_LOAD82:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG51:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD82]], ptr [[ATOMIC_TEMP83]], align 8, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_LOAD84:%.*]] = load i64, ptr [[ATOMIC_TEMP83]], align 8, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_SHL85:%.*]] = shl i64 [[BF_LOAD84]], 48, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_ASHR86:%.*]] = ashr i64 [[BF_SHL85]], 63, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_CAST87:%.*]] = trunc i64 [[BF_ASHR86]] to i32, !dbg [[DBG51]]
// PPC64-NEXT:    [[CONV88:%.*]] = sitofp i32 [[BF_CAST87]] to ppc_fp128, !dbg [[DBG51]]
// PPC64-NEXT:    store ppc_fp128 [[CONV88]], ptr @ldv, align 16, !dbg [[DBG51]]
// PPC64-NEXT:    [[ATOMIC_LOAD89:%.*]] = load atomic i8, ptr getelementptr inbounds nuw ([[STRUCT_BITFIELDS4_PACKED:%.*]], ptr @bfx4_packed, i32 0, i32 1) monotonic, align 1, !dbg [[DBG52:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD89]], ptr [[ATOMIC_TEMP90]], align 1, !dbg [[DBG52]]
// PPC64-NEXT:    [[BF_LOAD91:%.*]] = load i8, ptr [[ATOMIC_TEMP90]], align 1, !dbg [[DBG52]]
// PPC64-NEXT:    [[BF_ASHR92:%.*]] = ashr i8 [[BF_LOAD91]], 7, !dbg [[DBG52]]
// PPC64-NEXT:    [[BF_CAST93:%.*]] = sext i8 [[BF_ASHR92]] to i32, !dbg [[DBG52]]
// PPC64-NEXT:    [[CONV94:%.*]] = sitofp i32 [[BF_CAST93]] to ppc_fp128, !dbg [[DBG52]]
// PPC64-NEXT:    store ppc_fp128 [[CONV94]], ptr @ldv, align 16, !dbg [[DBG52]]
// PPC64-NEXT:    [[ATOMIC_LOAD95:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG53:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD95]], ptr [[ATOMIC_TEMP96]], align 8, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_LOAD97:%.*]] = load i64, ptr [[ATOMIC_TEMP96]], align 8, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_SHL98:%.*]] = shl i64 [[BF_LOAD97]], 49, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_ASHR99:%.*]] = ashr i64 [[BF_SHL98]], 57, !dbg [[DBG53]]
// PPC64-NEXT:    [[CONV100:%.*]] = sitofp i64 [[BF_ASHR99]] to ppc_fp128, !dbg [[DBG53]]
// PPC64-NEXT:    store ppc_fp128 [[CONV100]], ptr @ldv, align 16, !dbg [[DBG53]]
// PPC64-NEXT:    [[ATOMIC_LOAD101:%.*]] = load atomic i8, ptr getelementptr inbounds nuw ([[STRUCT_BITFIELDS4_PACKED]], ptr @bfx4_packed, i32 0, i32 1) acquire, align 1, !dbg [[DBG54:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD101]], ptr [[ATOMIC_TEMP102]], align 1, !dbg [[DBG54]]
// PPC64-NEXT:    [[BF_LOAD103:%.*]] = load i8, ptr [[ATOMIC_TEMP102]], align 1, !dbg [[DBG54]]
// PPC64-NEXT:    [[BF_SHL104:%.*]] = shl i8 [[BF_LOAD103]], 1, !dbg [[DBG54]]
// PPC64-NEXT:    [[BF_ASHR105:%.*]] = ashr i8 [[BF_SHL104]], 1, !dbg [[DBG54]]
// PPC64-NEXT:    [[BF_CAST106:%.*]] = sext i8 [[BF_ASHR105]] to i64, !dbg [[DBG54]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    [[CONV107:%.*]] = sitofp i64 [[BF_CAST106]] to ppc_fp128, !dbg [[DBG54]]
// PPC64-NEXT:    store ppc_fp128 [[CONV107]], ptr @ldv, align 16, !dbg [[DBG54]]
// PPC64-NEXT:    [[ATOMIC_LOAD108:%.*]] = load atomic i64, ptr @float2x monotonic, align 8, !dbg [[DBG55:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD108]], ptr [[ATOMIC_TEMP109]], align 8, !dbg [[DBG55]]
// PPC64-NEXT:    [[TMP3:%.*]] = load <2 x float>, ptr [[ATOMIC_TEMP109]], align 8, !dbg [[DBG55]]
// PPC64-NEXT:    [[TMP4:%.*]] = extractelement <2 x float> [[TMP3]], i64 0, !dbg [[DBG55]]
// PPC64-NEXT:    [[CONV110:%.*]] = fptoui float [[TMP4]] to i64, !dbg [[DBG55]]
// PPC64-NEXT:    store i64 [[CONV110]], ptr @ulv, align 8, !dbg [[DBG55]]
// PPC64-NEXT:    ret i32 0, !dbg [[DBG56:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@main
// AARCH64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP27:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP35:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP50:%.*]] = alloca <4 x i32>, align 16
// AARCH64-NEXT:    [[ATOMIC_TEMP54:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP56:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP62:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP67:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP72:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP77:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP84:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP91:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP98:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP104:%.*]] = alloca i64, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP110:%.*]] = alloca <2 x float>, align 8
// AARCH64-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// AARCH64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG9:![0-9]+]]
// AARCH64-NEXT:    [[LOADEDV:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG9]]
// AARCH64-NEXT:    [[STOREDV:%.*]] = zext i1 [[LOADEDV]] to i8, !dbg [[DBG9]]
// AARCH64-NEXT:    store i8 [[STOREDV]], ptr @bv, align 1, !dbg [[DBG9]]
// AARCH64-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, ptr @cx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD1]], ptr @cv, align 1, !dbg [[DBG10]]
// AARCH64-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, ptr @ucx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD2]], ptr @ucv, align 1, !dbg [[DBG11]]
// AARCH64-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG12:![0-9]+]]
// AARCH64-NEXT:    store i16 [[ATOMIC_LOAD3]], ptr @sv, align 2, !dbg [[DBG12]]
// AARCH64-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    store i16 [[ATOMIC_LOAD4]], ptr @usv, align 2, !dbg [[DBG13]]
// AARCH64-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD5]], ptr @iv, align 4, !dbg [[DBG14]]
// AARCH64-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, ptr @uix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD6]], ptr @uiv, align 4, !dbg [[DBG15]]
// AARCH64-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG16:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD7]], ptr @lv, align 8, !dbg [[DBG16]]
// AARCH64-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD8]], ptr @ulv, align 8, !dbg [[DBG17]]
// AARCH64-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD9]], ptr @llv, align 8, !dbg [[DBG18]]
// AARCH64-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, ptr @ullx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD10]], ptr @ullv, align 8, !dbg [[DBG19]]
// AARCH64-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG20:![0-9]+]]
// AARCH64-NEXT:    store float [[ATOMIC_LOAD11]], ptr @fv, align 4, !dbg [[DBG20]]
// AARCH64-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG21:![0-9]+]]
// AARCH64-NEXT:    store double [[ATOMIC_LOAD12]], ptr @dv, align 8, !dbg [[DBG21]]
// AARCH64-NEXT:    [[ATOMIC_LOAD13:%.*]] = load atomic fp128, ptr @ldx monotonic, align 16, !dbg [[DBG22:![0-9]+]]
// AARCH64-NEXT:    store fp128 [[ATOMIC_LOAD13]], ptr @ldv, align 16, !dbg [[DBG22]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP]], i32 noundef 0), !dbg [[DBG23:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_REALP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP]], i32 0, i32 0, !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP_REALP]], align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_IMAGP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP]], i32 0, i32 1, !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP_IMAGP]], align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    store i32 [[ATOMIC_TEMP_REAL]], ptr @civ, align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    store i32 [[ATOMIC_TEMP_IMAG]], ptr getelementptr inbounds nuw ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cfx, ptr noundef [[ATOMIC_TEMP14]], i32 noundef 0), !dbg [[DBG24:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds nuw { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, ptr [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds nuw { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, ptr [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    store float [[ATOMIC_TEMP14_REAL]], ptr @cfv, align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], ptr getelementptr inbounds nuw ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @cdx, ptr noundef [[ATOMIC_TEMP15]], i32 noundef 5), !dbg [[DBG25:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, ptr [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, ptr [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    store double [[ATOMIC_TEMP15_REAL]], ptr @cdv, align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], ptr getelementptr inbounds nuw ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG26:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG26]]
// AARCH64-NEXT:    [[STOREDV17:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG26]]
// AARCH64-NEXT:    store i8 [[STOREDV17]], ptr @bv, align 1, !dbg [[DBG26]]
// AARCH64-NEXT:    [[ATOMIC_LOAD18:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG27:![0-9]+]]
// AARCH64-NEXT:    [[LOADEDV19:%.*]] = trunc i8 [[ATOMIC_LOAD18]] to i1, !dbg [[DBG27]]
// AARCH64-NEXT:    [[CONV:%.*]] = zext i1 [[LOADEDV19]] to i8, !dbg [[DBG27]]
// AARCH64-NEXT:    store i8 [[CONV]], ptr @cv, align 1, !dbg [[DBG27]]
// AARCH64-NEXT:    [[ATOMIC_LOAD20:%.*]] = load atomic i8, ptr @cx seq_cst, align 1, !dbg [[DBG28:![0-9]+]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD20]], ptr @ucv, align 1, !dbg [[DBG28]]
// AARCH64-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG29:![0-9]+]]
// AARCH64-NEXT:    [[CONV22:%.*]] = trunc i64 [[ATOMIC_LOAD21]] to i16, !dbg [[DBG29]]
// AARCH64-NEXT:    store i16 [[CONV22]], ptr @sv, align 2, !dbg [[DBG29]]
// AARCH64-NEXT:    [[ATOMIC_LOAD23:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// AARCH64-NEXT:    [[CONV24:%.*]] = trunc i64 [[ATOMIC_LOAD23]] to i16, !dbg [[DBG30]]
// AARCH64-NEXT:    store i16 [[CONV24]], ptr @usv, align 2, !dbg [[DBG30]]
// AARCH64-NEXT:    [[ATOMIC_LOAD25:%.*]] = load atomic i32, ptr @uix seq_cst, align 4, !dbg [[DBG31:![0-9]+]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD25]], ptr @iv, align 4, !dbg [[DBG31]]
// AARCH64-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG32:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD26]], ptr @uiv, align 4, !dbg [[DBG32]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP27]], i32 noundef 0), !dbg [[DBG33:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP27_REALP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP27]], i32 0, i32 0, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP27_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP27_REALP]], align 4, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP27_IMAGP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP27]], i32 0, i32 1, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP27_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP27_IMAGP]], align 4, !dbg [[DBG33]]
// AARCH64-NEXT:    [[CONV28:%.*]] = sext i32 [[ATOMIC_TEMP27_REAL]] to i64, !dbg [[DBG33]]
// AARCH64-NEXT:    store i64 [[CONV28]], ptr @lv, align 8, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_LOAD29:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG34:![0-9]+]]
// AARCH64-NEXT:    [[CONV30:%.*]] = fptoui float [[ATOMIC_LOAD29]] to i64, !dbg [[DBG34]]
// AARCH64-NEXT:    store i64 [[CONV30]], ptr @ulv, align 8, !dbg [[DBG34]]
// AARCH64-NEXT:    [[ATOMIC_LOAD31:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG35:![0-9]+]]
// AARCH64-NEXT:    [[CONV32:%.*]] = fptosi double [[ATOMIC_LOAD31]] to i64, !dbg [[DBG35]]
// AARCH64-NEXT:    store i64 [[CONV32]], ptr @llv, align 8, !dbg [[DBG35]]
// AARCH64-NEXT:    [[ATOMIC_LOAD33:%.*]] = load atomic fp128, ptr @ldx monotonic, align 16, !dbg [[DBG36:![0-9]+]]
// AARCH64-NEXT:    [[CONV34:%.*]] = fptoui fp128 [[ATOMIC_LOAD33]] to i64, !dbg [[DBG36]]
// AARCH64-NEXT:    store i64 [[CONV34]], ptr @ullv, align 8, !dbg [[DBG36]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP35]], i32 noundef 0), !dbg [[DBG37:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP35_REALP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP35]], i32 0, i32 0, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP35_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP35_REALP]], align 4, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP35_IMAGP:%.*]] = getelementptr inbounds nuw { i32, i32 }, ptr [[ATOMIC_TEMP35]], i32 0, i32 1, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP35_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP35_IMAGP]], align 4, !dbg [[DBG37]]
// AARCH64-NEXT:    [[CONV36:%.*]] = sitofp i32 [[ATOMIC_TEMP35_REAL]] to float, !dbg [[DBG37]]
// AARCH64-NEXT:    store float [[CONV36]], ptr @fv, align 4, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_LOAD37:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG38:![0-9]+]]
// AARCH64-NEXT:    [[CONV38:%.*]] = sitofp i16 [[ATOMIC_LOAD37]] to double, !dbg [[DBG38]]
// AARCH64-NEXT:    store double [[CONV38]], ptr @dv, align 8, !dbg [[DBG38]]
// AARCH64-NEXT:    [[ATOMIC_LOAD39:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG39:![0-9]+]]
// AARCH64-NEXT:    [[LOADEDV40:%.*]] = trunc i8 [[ATOMIC_LOAD39]] to i1, !dbg [[DBG39]]
// AARCH64-NEXT:    [[CONV41:%.*]] = uitofp i1 [[LOADEDV40]] to fp128, !dbg [[DBG39]]
// AARCH64-NEXT:    store fp128 [[CONV41]], ptr @ldv, align 16, !dbg [[DBG39]]
// AARCH64-NEXT:    [[ATOMIC_LOAD42:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// AARCH64-NEXT:    [[LOADEDV43:%.*]] = trunc i8 [[ATOMIC_LOAD42]] to i1, !dbg [[DBG40]]
// AARCH64-NEXT:    [[CONV44:%.*]] = zext i1 [[LOADEDV43]] to i32, !dbg [[DBG40]]
// AARCH64-NEXT:    store i32 [[CONV44]], ptr @civ, align 4, !dbg [[DBG40]]
// AARCH64-NEXT:    store i32 0, ptr getelementptr inbounds nuw ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG40]]
// AARCH64-NEXT:    [[ATOMIC_LOAD45:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG41:![0-9]+]]
// AARCH64-NEXT:    [[CONV46:%.*]] = uitofp i16 [[ATOMIC_LOAD45]] to float, !dbg [[DBG41]]
// AARCH64-NEXT:    store float [[CONV46]], ptr @cfv, align 4, !dbg [[DBG41]]
// AARCH64-NEXT:    store float 0.000000e+00, ptr getelementptr inbounds nuw ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG41]]
// AARCH64-NEXT:    [[ATOMIC_LOAD47:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG42:![0-9]+]]
// AARCH64-NEXT:    [[CONV48:%.*]] = sitofp i64 [[ATOMIC_LOAD47]] to double, !dbg [[DBG42]]
// AARCH64-NEXT:    store double [[CONV48]], ptr @cdv, align 8, !dbg [[DBG42]]
// AARCH64-NEXT:    store double 0.000000e+00, ptr getelementptr inbounds nuw ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG42]]
// AARCH64-NEXT:    [[ATOMIC_LOAD49:%.*]] = load atomic i128, ptr @int4x monotonic, align 16, !dbg [[DBG43:![0-9]+]]
// AARCH64-NEXT:    store i128 [[ATOMIC_LOAD49]], ptr [[ATOMIC_TEMP50]], align 16, !dbg [[DBG43]]
// AARCH64-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[ATOMIC_TEMP50]], align 16, !dbg [[DBG43]]
// AARCH64-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP0]], i32 0, !dbg [[DBG43]]
// AARCH64-NEXT:    [[TOBOOL51:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG43]]
// AARCH64-NEXT:    [[STOREDV52:%.*]] = zext i1 [[TOBOOL51]] to i8, !dbg [[DBG43]]
// AARCH64-NEXT:    store i8 [[STOREDV52]], ptr @bv, align 1, !dbg [[DBG43]]
// AARCH64-NEXT:    [[ATOMIC_LOAD53:%.*]] = load atomic i32, ptr getelementptr (i8, ptr @bfx, i64 4) monotonic, align 4, !dbg [[DBG44:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD53]], ptr [[ATOMIC_TEMP54]], align 4, !dbg [[DBG44]]
// AARCH64-NEXT:    [[BF_LOAD:%.*]] = load i32, ptr [[ATOMIC_TEMP54]], align 4, !dbg [[DBG44]]
// AARCH64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD]], 1, !dbg [[DBG44]]
// AARCH64-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_SHL]], 1, !dbg [[DBG44]]
// AARCH64-NEXT:    [[CONV55:%.*]] = sitofp i32 [[BF_ASHR]] to fp128, !dbg [[DBG44]]
// AARCH64-NEXT:    store fp128 [[CONV55]], ptr @ldv, align 16, !dbg [[DBG44]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 4, ptr noundef getelementptr (i8, ptr @bfx_packed, i64 4), ptr noundef [[ATOMIC_TEMP56]], i32 noundef 0), !dbg [[DBG45:![0-9]+]]
// AARCH64-NEXT:    [[BF_LOAD57:%.*]] = load i32, ptr [[ATOMIC_TEMP56]], align 1, !dbg [[DBG45]]
// AARCH64-NEXT:    [[BF_SHL58:%.*]] = shl i32 [[BF_LOAD57]], 1, !dbg [[DBG45]]
// AARCH64-NEXT:    [[BF_ASHR59:%.*]] = ashr i32 [[BF_SHL58]], 1, !dbg [[DBG45]]
// AARCH64-NEXT:    [[CONV60:%.*]] = sitofp i32 [[BF_ASHR59]] to fp128, !dbg [[DBG45]]
// AARCH64-NEXT:    store fp128 [[CONV60]], ptr @ldv, align 16, !dbg [[DBG45]]
// AARCH64-NEXT:    [[ATOMIC_LOAD61:%.*]] = load atomic i32, ptr @bfx2 monotonic, align 4, !dbg [[DBG46:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD61]], ptr [[ATOMIC_TEMP62]], align 4, !dbg [[DBG46]]
// AARCH64-NEXT:    [[BF_LOAD63:%.*]] = load i32, ptr [[ATOMIC_TEMP62]], align 4, !dbg [[DBG46]]
// AARCH64-NEXT:    [[BF_ASHR64:%.*]] = ashr i32 [[BF_LOAD63]], 31, !dbg [[DBG46]]
// AARCH64-NEXT:    [[CONV65:%.*]] = sitofp i32 [[BF_ASHR64]] to fp128, !dbg [[DBG46]]
// AARCH64-NEXT:    store fp128 [[CONV65]], ptr @ldv, align 16, !dbg [[DBG46]]
// AARCH64-NEXT:    [[ATOMIC_LOAD66:%.*]] = load atomic i8, ptr getelementptr (i8, ptr @bfx2_packed, i64 3) monotonic, align 1, !dbg [[DBG47:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD66]], ptr [[ATOMIC_TEMP67]], align 1, !dbg [[DBG47]]
// AARCH64-NEXT:    [[BF_LOAD68:%.*]] = load i8, ptr [[ATOMIC_TEMP67]], align 1, !dbg [[DBG47]]
// AARCH64-NEXT:    [[BF_ASHR69:%.*]] = ashr i8 [[BF_LOAD68]], 7, !dbg [[DBG47]]
// AARCH64-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR69]] to i32, !dbg [[DBG47]]
// AARCH64-NEXT:    [[CONV70:%.*]] = sitofp i32 [[BF_CAST]] to fp128, !dbg [[DBG47]]
// AARCH64-NEXT:    store fp128 [[CONV70]], ptr @ldv, align 16, !dbg [[DBG47]]
// AARCH64-NEXT:    [[ATOMIC_LOAD71:%.*]] = load atomic i32, ptr @bfx3 monotonic, align 4, !dbg [[DBG48:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD71]], ptr [[ATOMIC_TEMP72]], align 4, !dbg [[DBG48]]
// AARCH64-NEXT:    [[BF_LOAD73:%.*]] = load i32, ptr [[ATOMIC_TEMP72]], align 4, !dbg [[DBG48]]
// AARCH64-NEXT:    [[BF_SHL74:%.*]] = shl i32 [[BF_LOAD73]], 7, !dbg [[DBG48]]
// AARCH64-NEXT:    [[BF_ASHR75:%.*]] = ashr i32 [[BF_SHL74]], 18, !dbg [[DBG48]]
// AARCH64-NEXT:    [[CONV76:%.*]] = sitofp i32 [[BF_ASHR75]] to fp128, !dbg [[DBG48]]
// AARCH64-NEXT:    store fp128 [[CONV76]], ptr @ldv, align 16, !dbg [[DBG48]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 3, ptr noundef getelementptr (i8, ptr @bfx3_packed, i64 1), ptr noundef [[ATOMIC_TEMP77]], i32 noundef 0), !dbg [[DBG49:![0-9]+]]
// AARCH64-NEXT:    [[BF_LOAD78:%.*]] = load i24, ptr [[ATOMIC_TEMP77]], align 1, !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_SHL79:%.*]] = shl i24 [[BF_LOAD78]], 7, !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_ASHR80:%.*]] = ashr i24 [[BF_SHL79]], 10, !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_CAST81:%.*]] = sext i24 [[BF_ASHR80]] to i32, !dbg [[DBG49]]
// AARCH64-NEXT:    [[CONV82:%.*]] = sitofp i32 [[BF_CAST81]] to fp128, !dbg [[DBG49]]
// AARCH64-NEXT:    store fp128 [[CONV82]], ptr @ldv, align 16, !dbg [[DBG49]]
// AARCH64-NEXT:    [[ATOMIC_LOAD83:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG50:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD83]], ptr [[ATOMIC_TEMP84]], align 8, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_LOAD85:%.*]] = load i64, ptr [[ATOMIC_TEMP84]], align 8, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_SHL86:%.*]] = shl i64 [[BF_LOAD85]], 47, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_ASHR87:%.*]] = ashr i64 [[BF_SHL86]], 63, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_CAST88:%.*]] = trunc i64 [[BF_ASHR87]] to i32, !dbg [[DBG50]]
// AARCH64-NEXT:    [[CONV89:%.*]] = sitofp i32 [[BF_CAST88]] to fp128, !dbg [[DBG50]]
// AARCH64-NEXT:    store fp128 [[CONV89]], ptr @ldv, align 16, !dbg [[DBG50]]
// AARCH64-NEXT:    [[ATOMIC_LOAD90:%.*]] = load atomic i8, ptr getelementptr inbounds nuw ([[STRUCT_BITFIELDS4_PACKED:%.*]], ptr @bfx4_packed, i32 0, i32 1) monotonic, align 1, !dbg [[DBG51:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD90]], ptr [[ATOMIC_TEMP91]], align 1, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_LOAD92:%.*]] = load i8, ptr [[ATOMIC_TEMP91]], align 1, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_SHL93:%.*]] = shl i8 [[BF_LOAD92]], 7, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_ASHR94:%.*]] = ashr i8 [[BF_SHL93]], 7, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_CAST95:%.*]] = sext i8 [[BF_ASHR94]] to i32, !dbg [[DBG51]]
// AARCH64-NEXT:    [[CONV96:%.*]] = sitofp i32 [[BF_CAST95]] to fp128, !dbg [[DBG51]]
// AARCH64-NEXT:    store fp128 [[CONV96]], ptr @ldv, align 16, !dbg [[DBG51]]
// AARCH64-NEXT:    [[ATOMIC_LOAD97:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG52:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD97]], ptr [[ATOMIC_TEMP98]], align 8, !dbg [[DBG52]]
// AARCH64-NEXT:    [[BF_LOAD99:%.*]] = load i64, ptr [[ATOMIC_TEMP98]], align 8, !dbg [[DBG52]]
// AARCH64-NEXT:    [[BF_SHL100:%.*]] = shl i64 [[BF_LOAD99]], 40, !dbg [[DBG52]]
// AARCH64-NEXT:    [[BF_ASHR101:%.*]] = ashr i64 [[BF_SHL100]], 57, !dbg [[DBG52]]
// AARCH64-NEXT:    [[CONV102:%.*]] = sitofp i64 [[BF_ASHR101]] to fp128, !dbg [[DBG52]]
// AARCH64-NEXT:    store fp128 [[CONV102]], ptr @ldv, align 16, !dbg [[DBG52]]
// AARCH64-NEXT:    [[ATOMIC_LOAD103:%.*]] = load atomic i8, ptr getelementptr inbounds nuw ([[STRUCT_BITFIELDS4_PACKED]], ptr @bfx4_packed, i32 0, i32 1) acquire, align 1, !dbg [[DBG53:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD103]], ptr [[ATOMIC_TEMP104]], align 1, !dbg [[DBG53]]
// AARCH64-NEXT:    [[BF_LOAD105:%.*]] = load i8, ptr [[ATOMIC_TEMP104]], align 1, !dbg [[DBG53]]
// AARCH64-NEXT:    [[BF_ASHR106:%.*]] = ashr i8 [[BF_LOAD105]], 1, !dbg [[DBG53]]
// AARCH64-NEXT:    [[BF_CAST107:%.*]] = sext i8 [[BF_ASHR106]] to i64, !dbg [[DBG53]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    [[CONV108:%.*]] = sitofp i64 [[BF_CAST107]] to fp128, !dbg [[DBG53]]
// AARCH64-NEXT:    store fp128 [[CONV108]], ptr @ldv, align 16, !dbg [[DBG53]]
// AARCH64-NEXT:    [[ATOMIC_LOAD109:%.*]] = load atomic i64, ptr @float2x monotonic, align 8, !dbg [[DBG54:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD109]], ptr [[ATOMIC_TEMP110]], align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load <2 x float>, ptr [[ATOMIC_TEMP110]], align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    [[TMP2:%.*]] = extractelement <2 x float> [[TMP1]], i64 0, !dbg [[DBG54]]
// AARCH64-NEXT:    [[CONV111:%.*]] = fptoui float [[TMP2]] to i64, !dbg [[DBG54]]
// AARCH64-NEXT:    store i64 [[CONV111]], ptr @ulv, align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    ret i32 0, !dbg [[DBG55:![0-9]+]]
//
