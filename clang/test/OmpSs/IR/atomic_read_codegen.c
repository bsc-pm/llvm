// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs
// RUN: %clang_cc1 -x c -no-opaque-pointers -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -x c -no-opaque-pointers -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -x c -no-opaque-pointers -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics
#ifndef HEADER
#define HEADER

_Bool bv, bx;
char cv, cx;
unsigned char ucv, ucx;
short sv, sx;
unsigned short usv, usx;
int iv, ix;
unsigned int uiv, uix;
long lv, lx;
unsigned long ulv, ulx;
long long llv, llx;
unsigned long long ullv, ullx;
float fv, fx;
double dv, dx;
long double ldv, ldx;
_Complex int civ, cix;
_Complex float cfv, cfx;
_Complex double cdv, cdx;

typedef int int4 __attribute__((__vector_size__(16)));
int4 int4x;

struct BitFields {
  int : 32;
  int a : 31;
} bfx;

struct BitFields_packed {
  int : 32;
  int a : 31;
} __attribute__ ((__packed__)) bfx_packed;

struct BitFields2 {
  int : 31;
  int a : 1;
} bfx2;

struct BitFields2_packed {
  int : 31;
  int a : 1;
} __attribute__ ((__packed__)) bfx2_packed;

struct BitFields3 {
  int : 11;
  int a : 14;
} bfx3;

struct BitFields3_packed {
  int : 11;
  int a : 14;
} __attribute__ ((__packed__)) bfx3_packed;

struct BitFields4 {
  short : 16;
  int a: 1;
  long b : 7;
} bfx4;

struct BitFields4_packed {
  short : 16;
  int a: 1;
  long b : 7;
} __attribute__ ((__packed__)) bfx4_packed;

typedef float float2 __attribute__((ext_vector_type(2)));
float2 float2x;

#if defined(__x86_64__)
// Register "0" is currently an invalid register for global register variables.
// Use "esp" instead of "0".
// register int rix __asm__("0");
register int rix __asm__("esp");
#endif

int main(void) {
#pragma oss atomic read
  bv = bx;
#pragma oss atomic read
  cv = cx;
#pragma oss atomic read
  ucv = ucx;
#pragma oss atomic read
  sv = sx;
#pragma oss atomic read
  usv = usx;
#pragma oss atomic read
  iv = ix;
#pragma oss atomic read
  uiv = uix;
#pragma oss atomic read
  lv = lx;
#pragma oss atomic read
  ulv = ulx;
#pragma oss atomic read
  llv = llx;
#pragma oss atomic read
  ullv = ullx;
#pragma oss atomic read
  fv = fx;
#pragma oss atomic read
  dv = dx;
#pragma oss atomic read
  ldv = ldx;
#pragma oss atomic read
  civ = cix;
#pragma oss atomic read
  cfv = cfx;
#pragma oss atomic seq_cst read
  cdv = cdx;
#pragma oss atomic read
  bv = ulx;
#pragma oss atomic read
  cv = bx;
#pragma oss atomic read seq_cst
  ucv = cx;
#pragma oss atomic read
  sv = ulx;
#pragma oss atomic read
  usv = lx;
#pragma oss atomic seq_cst, read
  iv = uix;
#pragma oss atomic read
  uiv = ix;
#pragma oss atomic read
  lv = cix;
#pragma oss atomic read
  ulv = fx;
#pragma oss atomic read
  llv = dx;
#pragma oss atomic read
  ullv = ldx;
#pragma oss atomic read
  fv = cix;
#pragma oss atomic read
  dv = sx;
#pragma oss atomic read
  ldv = bx;
#pragma oss atomic read
  civ = bx;
#pragma oss atomic read
  cfv = usx;
#pragma oss atomic read
  cdv = llx;
#pragma oss atomic read
  bv = int4x[0];
#pragma oss atomic read
  ldv = bfx.a;
#pragma oss atomic read
  ldv = bfx_packed.a;
#pragma oss atomic read
  ldv = bfx2.a;
#pragma oss atomic read
  ldv = bfx2_packed.a;
#pragma oss atomic read
  ldv = bfx3.a;
#pragma oss atomic read
  ldv = bfx3_packed.a;
#pragma oss atomic read
  ldv = bfx4.a;
#pragma oss atomic relaxed read
  ldv = bfx4_packed.a;
#pragma oss atomic read relaxed
  ldv = bfx4.b;
#pragma oss atomic read acquire
  ldv = bfx4_packed.b;
#pragma oss atomic read
  ulv = float2x.x;
#if defined(__x86_64__)
#pragma oss atomic read seq_cst
  dv = rix;
#endif
  return 0;
}

#endif
// LIN64-LABEL: define {{[^@]+}}@main
// LIN64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG6:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca x86_fp80, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP13:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP28:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP34:%.*]] = alloca x86_fp80, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP36:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP50:%.*]] = alloca <4 x i32>, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP54:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP56:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP62:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP67:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP72:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP77:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP84:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP91:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP98:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP104:%.*]] = alloca i64, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP110:%.*]] = alloca <2 x float>, align 8
// LIN64-NEXT:    store i32 0, i32* [[RETVAL]], align 4
// LIN64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG10]]
// LIN64-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG10]]
// LIN64-NEXT:    store i8 [[FROMBOOL]], i8* @bv, align 1, !dbg [[DBG10]]
// LIN64-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, i8* @cx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD1]], i8* @cv, align 1, !dbg [[DBG11]]
// LIN64-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, i8* @ucx monotonic, align 1, !dbg [[DBG12:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD2]], i8* @ucv, align 1, !dbg [[DBG12]]
// LIN64-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, i16* @sx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    store i16 [[ATOMIC_LOAD3]], i16* @sv, align 2, !dbg [[DBG13]]
// LIN64-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, i16* @usx monotonic, align 2, !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    store i16 [[ATOMIC_LOAD4]], i16* @usv, align 2, !dbg [[DBG14]]
// LIN64-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, i32* @ix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD5]], i32* @iv, align 4, !dbg [[DBG15]]
// LIN64-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, i32* @uix monotonic, align 4, !dbg [[DBG16:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD6]], i32* @uiv, align 4, !dbg [[DBG16]]
// LIN64-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, i64* @lx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD7]], i64* @lv, align 8, !dbg [[DBG17]]
// LIN64-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD8]], i64* @ulv, align 8, !dbg [[DBG18]]
// LIN64-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, i64* @llx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD9]], i64* @llv, align 8, !dbg [[DBG19]]
// LIN64-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, i64* @ullx monotonic, align 8, !dbg [[DBG20:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD10]], i64* @ullv, align 8, !dbg [[DBG20]]
// LIN64-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic i32, i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG21:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = bitcast i32 [[ATOMIC_LOAD11]] to float, !dbg [[DBG21]]
// LIN64-NEXT:    store float [[TMP0]], float* @fv, align 4, !dbg [[DBG21]]
// LIN64-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic i64, i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG22:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = bitcast i64 [[ATOMIC_LOAD12]] to double, !dbg [[DBG22]]
// LIN64-NEXT:    store double [[TMP1]], double* @dv, align 8, !dbg [[DBG22]]
// LIN64-NEXT:    [[TMP2:%.*]] = bitcast x86_fp80* [[ATOMIC_TEMP]] to i8*, !dbg [[DBG23:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (x86_fp80* @ldx to i8*), i8* noundef [[TMP2]], i32 noundef 0), !dbg [[DBG23]]
// LIN64-NEXT:    [[TMP3:%.*]] = load x86_fp80, x86_fp80* [[ATOMIC_TEMP]], align 16, !dbg [[DBG23]]
// LIN64-NEXT:    store x86_fp80 [[TMP3]], x86_fp80* @ldv, align 16, !dbg [[DBG23]]
// LIN64-NEXT:    [[TMP4:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP13]] to i8*, !dbg [[DBG24:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP4]], i32 noundef 0), !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP13]], i32 0, i32 0, !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP13_REALP]], align 4, !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP13]], i32 0, i32 1, !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP13_IMAGP]], align 4, !dbg [[DBG24]]
// LIN64-NEXT:    store i32 [[ATOMIC_TEMP13_REAL]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG24]]
// LIN64-NEXT:    store i32 [[ATOMIC_TEMP13_IMAG]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG24]]
// LIN64-NEXT:    [[TMP5:%.*]] = bitcast { float, float }* [[ATOMIC_TEMP14]] to i8*, !dbg [[DBG25:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ float, float }* @cfx to i8*), i8* noundef [[TMP5]], i32 noundef 0), !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, float* [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, float* [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG25]]
// LIN64-NEXT:    store float [[ATOMIC_TEMP14_REAL]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG25]]
// LIN64-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG25]]
// LIN64-NEXT:    [[TMP6:%.*]] = bitcast { double, double }* [[ATOMIC_TEMP15]] to i8*, !dbg [[DBG26:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast ({ double, double }* @cdx to i8*), i8* noundef [[TMP6]], i32 noundef 5), !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, double* [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, double* [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG26]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    store double [[ATOMIC_TEMP15_REAL]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG26]]
// LIN64-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG27:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL17:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG27]]
// LIN64-NEXT:    [[FROMBOOL18:%.*]] = zext i1 [[TOBOOL17]] to i8, !dbg [[DBG27]]
// LIN64-NEXT:    store i8 [[FROMBOOL18]], i8* @bv, align 1, !dbg [[DBG27]]
// LIN64-NEXT:    [[ATOMIC_LOAD19:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG28:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL20:%.*]] = trunc i8 [[ATOMIC_LOAD19]] to i1, !dbg [[DBG28]]
// LIN64-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL20]] to i8, !dbg [[DBG28]]
// LIN64-NEXT:    store i8 [[CONV]], i8* @cv, align 1, !dbg [[DBG28]]
// LIN64-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i8, i8* @cx seq_cst, align 1, !dbg [[DBG29:![0-9]+]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD21]], i8* @ucv, align 1, !dbg [[DBG29]]
// LIN64-NEXT:    [[ATOMIC_LOAD22:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// LIN64-NEXT:    [[CONV23:%.*]] = trunc i64 [[ATOMIC_LOAD22]] to i16, !dbg [[DBG30]]
// LIN64-NEXT:    store i16 [[CONV23]], i16* @sv, align 2, !dbg [[DBG30]]
// LIN64-NEXT:    [[ATOMIC_LOAD24:%.*]] = load atomic i64, i64* @lx monotonic, align 8, !dbg [[DBG31:![0-9]+]]
// LIN64-NEXT:    [[CONV25:%.*]] = trunc i64 [[ATOMIC_LOAD24]] to i16, !dbg [[DBG31]]
// LIN64-NEXT:    store i16 [[CONV25]], i16* @usv, align 2, !dbg [[DBG31]]
// LIN64-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, i32* @uix seq_cst, align 4, !dbg [[DBG32:![0-9]+]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD26]], i32* @iv, align 4, !dbg [[DBG32]]
// LIN64-NEXT:    [[ATOMIC_LOAD27:%.*]] = load atomic i32, i32* @ix monotonic, align 4, !dbg [[DBG33:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD27]], i32* @uiv, align 4, !dbg [[DBG33]]
// LIN64-NEXT:    [[TMP7:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP28]] to i8*, !dbg [[DBG34:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP7]], i32 noundef 0), !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP28_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP28]], i32 0, i32 0, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP28_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP28_REALP]], align 4, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP28_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP28]], i32 0, i32 1, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP28_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP28_IMAGP]], align 4, !dbg [[DBG34]]
// LIN64-NEXT:    [[CONV29:%.*]] = sext i32 [[ATOMIC_TEMP28_REAL]] to i64, !dbg [[DBG34]]
// LIN64-NEXT:    store i64 [[CONV29]], i64* @lv, align 8, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_LOAD30:%.*]] = load atomic i32, i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG35:![0-9]+]]
// LIN64-NEXT:    [[TMP8:%.*]] = bitcast i32 [[ATOMIC_LOAD30]] to float, !dbg [[DBG35]]
// LIN64-NEXT:    [[CONV31:%.*]] = fptoui float [[TMP8]] to i64, !dbg [[DBG35]]
// LIN64-NEXT:    store i64 [[CONV31]], i64* @ulv, align 8, !dbg [[DBG35]]
// LIN64-NEXT:    [[ATOMIC_LOAD32:%.*]] = load atomic i64, i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG36:![0-9]+]]
// LIN64-NEXT:    [[TMP9:%.*]] = bitcast i64 [[ATOMIC_LOAD32]] to double, !dbg [[DBG36]]
// LIN64-NEXT:    [[CONV33:%.*]] = fptosi double [[TMP9]] to i64, !dbg [[DBG36]]
// LIN64-NEXT:    store i64 [[CONV33]], i64* @llv, align 8, !dbg [[DBG36]]
// LIN64-NEXT:    [[TMP10:%.*]] = bitcast x86_fp80* [[ATOMIC_TEMP34]] to i8*, !dbg [[DBG37:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (x86_fp80* @ldx to i8*), i8* noundef [[TMP10]], i32 noundef 0), !dbg [[DBG37]]
// LIN64-NEXT:    [[TMP11:%.*]] = load x86_fp80, x86_fp80* [[ATOMIC_TEMP34]], align 16, !dbg [[DBG37]]
// LIN64-NEXT:    [[CONV35:%.*]] = fptoui x86_fp80 [[TMP11]] to i64, !dbg [[DBG37]]
// LIN64-NEXT:    store i64 [[CONV35]], i64* @ullv, align 8, !dbg [[DBG37]]
// LIN64-NEXT:    [[TMP12:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP36]] to i8*, !dbg [[DBG38:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP12]], i32 noundef 0), !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP36_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP36]], i32 0, i32 0, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP36_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP36_REALP]], align 4, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP36_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP36]], i32 0, i32 1, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP36_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP36_IMAGP]], align 4, !dbg [[DBG38]]
// LIN64-NEXT:    [[CONV37:%.*]] = sitofp i32 [[ATOMIC_TEMP36_REAL]] to float, !dbg [[DBG38]]
// LIN64-NEXT:    store float [[CONV37]], float* @fv, align 4, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_LOAD38:%.*]] = load atomic i16, i16* @sx monotonic, align 2, !dbg [[DBG39:![0-9]+]]
// LIN64-NEXT:    [[CONV39:%.*]] = sitofp i16 [[ATOMIC_LOAD38]] to double, !dbg [[DBG39]]
// LIN64-NEXT:    store double [[CONV39]], double* @dv, align 8, !dbg [[DBG39]]
// LIN64-NEXT:    [[ATOMIC_LOAD40:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL41:%.*]] = trunc i8 [[ATOMIC_LOAD40]] to i1, !dbg [[DBG40]]
// LIN64-NEXT:    [[CONV42:%.*]] = uitofp i1 [[TOBOOL41]] to x86_fp80, !dbg [[DBG40]]
// LIN64-NEXT:    store x86_fp80 [[CONV42]], x86_fp80* @ldv, align 16, !dbg [[DBG40]]
// LIN64-NEXT:    [[ATOMIC_LOAD43:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG41:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL44:%.*]] = trunc i8 [[ATOMIC_LOAD43]] to i1, !dbg [[DBG41]]
// LIN64-NEXT:    [[CONV45:%.*]] = zext i1 [[TOBOOL44]] to i32, !dbg [[DBG41]]
// LIN64-NEXT:    store i32 [[CONV45]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG41]]
// LIN64-NEXT:    store i32 0, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG41]]
// LIN64-NEXT:    [[ATOMIC_LOAD46:%.*]] = load atomic i16, i16* @usx monotonic, align 2, !dbg [[DBG42:![0-9]+]]
// LIN64-NEXT:    [[CONV47:%.*]] = uitofp i16 [[ATOMIC_LOAD46]] to float, !dbg [[DBG42]]
// LIN64-NEXT:    store float [[CONV47]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG42]]
// LIN64-NEXT:    store float 0.000000e+00, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG42]]
// LIN64-NEXT:    [[ATOMIC_LOAD48:%.*]] = load atomic i64, i64* @llx monotonic, align 8, !dbg [[DBG43:![0-9]+]]
// LIN64-NEXT:    [[CONV49:%.*]] = sitofp i64 [[ATOMIC_LOAD48]] to double, !dbg [[DBG43]]
// LIN64-NEXT:    store double [[CONV49]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG43]]
// LIN64-NEXT:    store double 0.000000e+00, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG43]]
// LIN64-NEXT:    [[TMP13:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP50]] to i8*, !dbg [[DBG44:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (<4 x i32>* @int4x to i8*), i8* noundef [[TMP13]], i32 noundef 0), !dbg [[DBG44]]
// LIN64-NEXT:    [[TMP14:%.*]] = load <4 x i32>, <4 x i32>* [[ATOMIC_TEMP50]], align 16, !dbg [[DBG44]]
// LIN64-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP14]], i32 0, !dbg [[DBG44]]
// LIN64-NEXT:    [[TOBOOL51:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG44]]
// LIN64-NEXT:    [[FROMBOOL52:%.*]] = zext i1 [[TOBOOL51]] to i8, !dbg [[DBG44]]
// LIN64-NEXT:    store i8 [[FROMBOOL52]], i8* @bv, align 1, !dbg [[DBG44]]
// LIN64-NEXT:    [[ATOMIC_LOAD53:%.*]] = load atomic i32, i32* bitcast (i8* getelementptr (i8, i8* bitcast (%struct.BitFields* @bfx to i8*), i64 4) to i32*) monotonic, align 4, !dbg [[DBG45:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD53]], i32* [[ATOMIC_TEMP54]], align 4, !dbg [[DBG45]]
// LIN64-NEXT:    [[BF_LOAD:%.*]] = load i32, i32* [[ATOMIC_TEMP54]], align 4, !dbg [[DBG45]]
// LIN64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD]], 1, !dbg [[DBG45]]
// LIN64-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_SHL]], 1, !dbg [[DBG45]]
// LIN64-NEXT:    [[CONV55:%.*]] = sitofp i32 [[BF_ASHR]] to x86_fp80, !dbg [[DBG45]]
// LIN64-NEXT:    store x86_fp80 [[CONV55]], x86_fp80* @ldv, align 16, !dbg [[DBG45]]
// LIN64-NEXT:    [[TMP15:%.*]] = bitcast i32* [[ATOMIC_TEMP56]] to i8*, !dbg [[DBG46:![0-9]+]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 4, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i64 4), i8* noundef [[TMP15]], i32 noundef 0), !dbg [[DBG46]]
// LIN64-NEXT:    [[BF_LOAD57:%.*]] = load i32, i32* [[ATOMIC_TEMP56]], align 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[BF_SHL58:%.*]] = shl i32 [[BF_LOAD57]], 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[BF_ASHR59:%.*]] = ashr i32 [[BF_SHL58]], 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[CONV60:%.*]] = sitofp i32 [[BF_ASHR59]] to x86_fp80, !dbg [[DBG46]]
// LIN64-NEXT:    store x86_fp80 [[CONV60]], x86_fp80* @ldv, align 16, !dbg [[DBG46]]
// LIN64-NEXT:    [[ATOMIC_LOAD61:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS2:%.*]], %struct.BitFields2* @bfx2, i32 0, i32 0) monotonic, align 4, !dbg [[DBG47:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD61]], i32* [[ATOMIC_TEMP62]], align 4, !dbg [[DBG47]]
// LIN64-NEXT:    [[BF_LOAD63:%.*]] = load i32, i32* [[ATOMIC_TEMP62]], align 4, !dbg [[DBG47]]
// LIN64-NEXT:    [[BF_ASHR64:%.*]] = ashr i32 [[BF_LOAD63]], 31, !dbg [[DBG47]]
// LIN64-NEXT:    [[CONV65:%.*]] = sitofp i32 [[BF_ASHR64]] to x86_fp80, !dbg [[DBG47]]
// LIN64-NEXT:    store x86_fp80 [[CONV65]], x86_fp80* @ldv, align 16, !dbg [[DBG47]]
// LIN64-NEXT:    [[ATOMIC_LOAD66:%.*]] = load atomic i8, i8* getelementptr (i8, i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*), i64 3) monotonic, align 1, !dbg [[DBG48:![0-9]+]]
// LIN64-NEXT:    [[TMP16:%.*]] = bitcast i32* [[ATOMIC_TEMP67]] to i8*, !dbg [[DBG48]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD66]], i8* [[TMP16]], align 1, !dbg [[DBG48]]
// LIN64-NEXT:    [[BF_LOAD68:%.*]] = load i8, i8* [[TMP16]], align 1, !dbg [[DBG48]]
// LIN64-NEXT:    [[BF_ASHR69:%.*]] = ashr i8 [[BF_LOAD68]], 7, !dbg [[DBG48]]
// LIN64-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR69]] to i32, !dbg [[DBG48]]
// LIN64-NEXT:    [[CONV70:%.*]] = sitofp i32 [[BF_CAST]] to x86_fp80, !dbg [[DBG48]]
// LIN64-NEXT:    store x86_fp80 [[CONV70]], x86_fp80* @ldv, align 16, !dbg [[DBG48]]
// LIN64-NEXT:    [[ATOMIC_LOAD71:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS3:%.*]], %struct.BitFields3* @bfx3, i32 0, i32 0) monotonic, align 4, !dbg [[DBG49:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD71]], i32* [[ATOMIC_TEMP72]], align 4, !dbg [[DBG49]]
// LIN64-NEXT:    [[BF_LOAD73:%.*]] = load i32, i32* [[ATOMIC_TEMP72]], align 4, !dbg [[DBG49]]
// LIN64-NEXT:    [[BF_SHL74:%.*]] = shl i32 [[BF_LOAD73]], 7, !dbg [[DBG49]]
// LIN64-NEXT:    [[BF_ASHR75:%.*]] = ashr i32 [[BF_SHL74]], 18, !dbg [[DBG49]]
// LIN64-NEXT:    [[CONV76:%.*]] = sitofp i32 [[BF_ASHR75]] to x86_fp80, !dbg [[DBG49]]
// LIN64-NEXT:    store x86_fp80 [[CONV76]], x86_fp80* @ldv, align 16, !dbg [[DBG49]]
// LIN64-NEXT:    [[TMP17:%.*]] = bitcast i32* [[ATOMIC_TEMP77]] to i24*, !dbg [[DBG50:![0-9]+]]
// LIN64-NEXT:    [[TMP18:%.*]] = bitcast i24* [[TMP17]] to i8*, !dbg [[DBG50]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 3, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i64 1), i8* noundef [[TMP18]], i32 noundef 0), !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_LOAD78:%.*]] = load i24, i24* [[TMP17]], align 1, !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_SHL79:%.*]] = shl i24 [[BF_LOAD78]], 7, !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_ASHR80:%.*]] = ashr i24 [[BF_SHL79]], 10, !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_CAST81:%.*]] = sext i24 [[BF_ASHR80]] to i32, !dbg [[DBG50]]
// LIN64-NEXT:    [[CONV82:%.*]] = sitofp i32 [[BF_CAST81]] to x86_fp80, !dbg [[DBG50]]
// LIN64-NEXT:    store x86_fp80 [[CONV82]], x86_fp80* @ldv, align 16, !dbg [[DBG50]]
// LIN64-NEXT:    [[ATOMIC_LOAD83:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG51:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD83]], i64* [[ATOMIC_TEMP84]], align 8, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_LOAD85:%.*]] = load i64, i64* [[ATOMIC_TEMP84]], align 8, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_SHL86:%.*]] = shl i64 [[BF_LOAD85]], 47, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_ASHR87:%.*]] = ashr i64 [[BF_SHL86]], 63, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_CAST88:%.*]] = trunc i64 [[BF_ASHR87]] to i32, !dbg [[DBG51]]
// LIN64-NEXT:    [[CONV89:%.*]] = sitofp i32 [[BF_CAST88]] to x86_fp80, !dbg [[DBG51]]
// LIN64-NEXT:    store x86_fp80 [[CONV89]], x86_fp80* @ldv, align 16, !dbg [[DBG51]]
// LIN64-NEXT:    [[ATOMIC_LOAD90:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) monotonic, align 1, !dbg [[DBG52:![0-9]+]]
// LIN64-NEXT:    [[TMP19:%.*]] = bitcast i32* [[ATOMIC_TEMP91]] to i8*, !dbg [[DBG52]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD90]], i8* [[TMP19]], align 1, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_LOAD92:%.*]] = load i8, i8* [[TMP19]], align 1, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_SHL93:%.*]] = shl i8 [[BF_LOAD92]], 7, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_ASHR94:%.*]] = ashr i8 [[BF_SHL93]], 7, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_CAST95:%.*]] = sext i8 [[BF_ASHR94]] to i32, !dbg [[DBG52]]
// LIN64-NEXT:    [[CONV96:%.*]] = sitofp i32 [[BF_CAST95]] to x86_fp80, !dbg [[DBG52]]
// LIN64-NEXT:    store x86_fp80 [[CONV96]], x86_fp80* @ldv, align 16, !dbg [[DBG52]]
// LIN64-NEXT:    [[ATOMIC_LOAD97:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG53:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD97]], i64* [[ATOMIC_TEMP98]], align 8, !dbg [[DBG53]]
// LIN64-NEXT:    [[BF_LOAD99:%.*]] = load i64, i64* [[ATOMIC_TEMP98]], align 8, !dbg [[DBG53]]
// LIN64-NEXT:    [[BF_SHL100:%.*]] = shl i64 [[BF_LOAD99]], 40, !dbg [[DBG53]]
// LIN64-NEXT:    [[BF_ASHR101:%.*]] = ashr i64 [[BF_SHL100]], 57, !dbg [[DBG53]]
// LIN64-NEXT:    [[CONV102:%.*]] = sitofp i64 [[BF_ASHR101]] to x86_fp80, !dbg [[DBG53]]
// LIN64-NEXT:    store x86_fp80 [[CONV102]], x86_fp80* @ldv, align 16, !dbg [[DBG53]]
// LIN64-NEXT:    [[ATOMIC_LOAD103:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) acquire, align 1, !dbg [[DBG54:![0-9]+]]
// LIN64-NEXT:    [[TMP20:%.*]] = bitcast i64* [[ATOMIC_TEMP104]] to i8*, !dbg [[DBG54]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD103]], i8* [[TMP20]], align 1, !dbg [[DBG54]]
// LIN64-NEXT:    [[BF_LOAD105:%.*]] = load i8, i8* [[TMP20]], align 1, !dbg [[DBG54]]
// LIN64-NEXT:    [[BF_ASHR106:%.*]] = ashr i8 [[BF_LOAD105]], 1, !dbg [[DBG54]]
// LIN64-NEXT:    [[BF_CAST107:%.*]] = sext i8 [[BF_ASHR106]] to i64, !dbg [[DBG54]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    [[CONV108:%.*]] = sitofp i64 [[BF_CAST107]] to x86_fp80, !dbg [[DBG54]]
// LIN64-NEXT:    store x86_fp80 [[CONV108]], x86_fp80* @ldv, align 16, !dbg [[DBG54]]
// LIN64-NEXT:    [[ATOMIC_LOAD109:%.*]] = load atomic i64, i64* bitcast (<2 x float>* @float2x to i64*) monotonic, align 8, !dbg [[DBG55:![0-9]+]]
// LIN64-NEXT:    [[TMP21:%.*]] = bitcast <2 x float>* [[ATOMIC_TEMP110]] to i64*, !dbg [[DBG55]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD109]], i64* [[TMP21]], align 8, !dbg [[DBG55]]
// LIN64-NEXT:    [[TMP22:%.*]] = load <2 x float>, <2 x float>* [[ATOMIC_TEMP110]], align 8, !dbg [[DBG55]]
// LIN64-NEXT:    [[TMP23:%.*]] = extractelement <2 x float> [[TMP22]], i64 0, !dbg [[DBG55]]
// LIN64-NEXT:    [[CONV111:%.*]] = fptoui float [[TMP23]] to i64, !dbg [[DBG55]]
// LIN64-NEXT:    store i64 [[CONV111]], i64* @ulv, align 8, !dbg [[DBG55]]
// LIN64-NEXT:    [[TMP24:%.*]] = call i32 @llvm.read_register.i32(metadata [[META2:![0-9]+]]), !dbg [[DBG56:![0-9]+]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    [[CONV112:%.*]] = sitofp i32 [[TMP24]] to double, !dbg [[DBG56]]
// LIN64-NEXT:    store double [[CONV112]], double* @dv, align 8, !dbg [[DBG56]]
// LIN64-NEXT:    ret i32 0, !dbg [[DBG57:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@main
// PPC64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca ppc_fp128, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP13:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP28:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP34:%.*]] = alloca ppc_fp128, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP36:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP50:%.*]] = alloca <4 x i32>, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP54:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP56:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP61:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP66:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP72:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP77:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP84:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP91:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP97:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP103:%.*]] = alloca i64, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP110:%.*]] = alloca <2 x float>, align 8
// PPC64-NEXT:    store i32 0, i32* [[RETVAL]], align 4
// PPC64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG9:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG9]]
// PPC64-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG9]]
// PPC64-NEXT:    store i8 [[FROMBOOL]], i8* @bv, align 1, !dbg [[DBG9]]
// PPC64-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, i8* @cx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD1]], i8* @cv, align 1, !dbg [[DBG10]]
// PPC64-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, i8* @ucx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD2]], i8* @ucv, align 1, !dbg [[DBG11]]
// PPC64-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, i16* @sx monotonic, align 2, !dbg [[DBG12:![0-9]+]]
// PPC64-NEXT:    store i16 [[ATOMIC_LOAD3]], i16* @sv, align 2, !dbg [[DBG12]]
// PPC64-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, i16* @usx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    store i16 [[ATOMIC_LOAD4]], i16* @usv, align 2, !dbg [[DBG13]]
// PPC64-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, i32* @ix monotonic, align 4, !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD5]], i32* @iv, align 4, !dbg [[DBG14]]
// PPC64-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, i32* @uix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD6]], i32* @uiv, align 4, !dbg [[DBG15]]
// PPC64-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, i64* @lx monotonic, align 8, !dbg [[DBG16:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD7]], i64* @lv, align 8, !dbg [[DBG16]]
// PPC64-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD8]], i64* @ulv, align 8, !dbg [[DBG17]]
// PPC64-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, i64* @llx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD9]], i64* @llv, align 8, !dbg [[DBG18]]
// PPC64-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, i64* @ullx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD10]], i64* @ullv, align 8, !dbg [[DBG19]]
// PPC64-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic i32, i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG20:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = bitcast i32 [[ATOMIC_LOAD11]] to float, !dbg [[DBG20]]
// PPC64-NEXT:    store float [[TMP0]], float* @fv, align 4, !dbg [[DBG20]]
// PPC64-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic i64, i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG21:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = bitcast i64 [[ATOMIC_LOAD12]] to double, !dbg [[DBG21]]
// PPC64-NEXT:    store double [[TMP1]], double* @dv, align 8, !dbg [[DBG21]]
// PPC64-NEXT:    [[TMP2:%.*]] = bitcast ppc_fp128* [[ATOMIC_TEMP]] to i8*, !dbg [[DBG22:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (ppc_fp128* @ldx to i8*), i8* noundef [[TMP2]], i32 noundef signext 0), !dbg [[DBG22]]
// PPC64-NEXT:    [[TMP3:%.*]] = load ppc_fp128, ppc_fp128* [[ATOMIC_TEMP]], align 16, !dbg [[DBG22]]
// PPC64-NEXT:    store ppc_fp128 [[TMP3]], ppc_fp128* @ldv, align 16, !dbg [[DBG22]]
// PPC64-NEXT:    [[TMP4:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP13]] to i8*, !dbg [[DBG23:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP4]], i32 noundef signext 0), !dbg [[DBG23]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP13]], i32 0, i32 0, !dbg [[DBG23]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP13_REALP]], align 4, !dbg [[DBG23]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP13]], i32 0, i32 1, !dbg [[DBG23]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP13_IMAGP]], align 4, !dbg [[DBG23]]
// PPC64-NEXT:    store i32 [[ATOMIC_TEMP13_REAL]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG23]]
// PPC64-NEXT:    store i32 [[ATOMIC_TEMP13_IMAG]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG23]]
// PPC64-NEXT:    [[TMP5:%.*]] = bitcast { float, float }* [[ATOMIC_TEMP14]] to i8*, !dbg [[DBG24:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ float, float }* @cfx to i8*), i8* noundef [[TMP5]], i32 noundef signext 0), !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, float* [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, float* [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG24]]
// PPC64-NEXT:    store float [[ATOMIC_TEMP14_REAL]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG24]]
// PPC64-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG24]]
// PPC64-NEXT:    [[TMP6:%.*]] = bitcast { double, double }* [[ATOMIC_TEMP15]] to i8*, !dbg [[DBG25:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast ({ double, double }* @cdx to i8*), i8* noundef [[TMP6]], i32 noundef signext 5), !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, double* [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, double* [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG25]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    store double [[ATOMIC_TEMP15_REAL]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG25]]
// PPC64-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG26:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL17:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG26]]
// PPC64-NEXT:    [[FROMBOOL18:%.*]] = zext i1 [[TOBOOL17]] to i8, !dbg [[DBG26]]
// PPC64-NEXT:    store i8 [[FROMBOOL18]], i8* @bv, align 1, !dbg [[DBG26]]
// PPC64-NEXT:    [[ATOMIC_LOAD19:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG27:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL20:%.*]] = trunc i8 [[ATOMIC_LOAD19]] to i1, !dbg [[DBG27]]
// PPC64-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL20]] to i8, !dbg [[DBG27]]
// PPC64-NEXT:    store i8 [[CONV]], i8* @cv, align 1, !dbg [[DBG27]]
// PPC64-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i8, i8* @cx seq_cst, align 1, !dbg [[DBG28:![0-9]+]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD21]], i8* @ucv, align 1, !dbg [[DBG28]]
// PPC64-NEXT:    [[ATOMIC_LOAD22:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG29:![0-9]+]]
// PPC64-NEXT:    [[CONV23:%.*]] = trunc i64 [[ATOMIC_LOAD22]] to i16, !dbg [[DBG29]]
// PPC64-NEXT:    store i16 [[CONV23]], i16* @sv, align 2, !dbg [[DBG29]]
// PPC64-NEXT:    [[ATOMIC_LOAD24:%.*]] = load atomic i64, i64* @lx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// PPC64-NEXT:    [[CONV25:%.*]] = trunc i64 [[ATOMIC_LOAD24]] to i16, !dbg [[DBG30]]
// PPC64-NEXT:    store i16 [[CONV25]], i16* @usv, align 2, !dbg [[DBG30]]
// PPC64-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, i32* @uix seq_cst, align 4, !dbg [[DBG31:![0-9]+]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD26]], i32* @iv, align 4, !dbg [[DBG31]]
// PPC64-NEXT:    [[ATOMIC_LOAD27:%.*]] = load atomic i32, i32* @ix monotonic, align 4, !dbg [[DBG32:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD27]], i32* @uiv, align 4, !dbg [[DBG32]]
// PPC64-NEXT:    [[TMP7:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP28]] to i8*, !dbg [[DBG33:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP7]], i32 noundef signext 0), !dbg [[DBG33]]
// PPC64-NEXT:    [[ATOMIC_TEMP28_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP28]], i32 0, i32 0, !dbg [[DBG33]]
// PPC64-NEXT:    [[ATOMIC_TEMP28_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP28_REALP]], align 4, !dbg [[DBG33]]
// PPC64-NEXT:    [[ATOMIC_TEMP28_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP28]], i32 0, i32 1, !dbg [[DBG33]]
// PPC64-NEXT:    [[ATOMIC_TEMP28_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP28_IMAGP]], align 4, !dbg [[DBG33]]
// PPC64-NEXT:    [[CONV29:%.*]] = sext i32 [[ATOMIC_TEMP28_REAL]] to i64, !dbg [[DBG33]]
// PPC64-NEXT:    store i64 [[CONV29]], i64* @lv, align 8, !dbg [[DBG33]]
// PPC64-NEXT:    [[ATOMIC_LOAD30:%.*]] = load atomic i32, i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG34:![0-9]+]]
// PPC64-NEXT:    [[TMP8:%.*]] = bitcast i32 [[ATOMIC_LOAD30]] to float, !dbg [[DBG34]]
// PPC64-NEXT:    [[CONV31:%.*]] = fptoui float [[TMP8]] to i64, !dbg [[DBG34]]
// PPC64-NEXT:    store i64 [[CONV31]], i64* @ulv, align 8, !dbg [[DBG34]]
// PPC64-NEXT:    [[ATOMIC_LOAD32:%.*]] = load atomic i64, i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG35:![0-9]+]]
// PPC64-NEXT:    [[TMP9:%.*]] = bitcast i64 [[ATOMIC_LOAD32]] to double, !dbg [[DBG35]]
// PPC64-NEXT:    [[CONV33:%.*]] = fptosi double [[TMP9]] to i64, !dbg [[DBG35]]
// PPC64-NEXT:    store i64 [[CONV33]], i64* @llv, align 8, !dbg [[DBG35]]
// PPC64-NEXT:    [[TMP10:%.*]] = bitcast ppc_fp128* [[ATOMIC_TEMP34]] to i8*, !dbg [[DBG36:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (ppc_fp128* @ldx to i8*), i8* noundef [[TMP10]], i32 noundef signext 0), !dbg [[DBG36]]
// PPC64-NEXT:    [[TMP11:%.*]] = load ppc_fp128, ppc_fp128* [[ATOMIC_TEMP34]], align 16, !dbg [[DBG36]]
// PPC64-NEXT:    [[CONV35:%.*]] = fptoui ppc_fp128 [[TMP11]] to i64, !dbg [[DBG36]]
// PPC64-NEXT:    store i64 [[CONV35]], i64* @ullv, align 8, !dbg [[DBG36]]
// PPC64-NEXT:    [[TMP12:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP36]] to i8*, !dbg [[DBG37:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP12]], i32 noundef signext 0), !dbg [[DBG37]]
// PPC64-NEXT:    [[ATOMIC_TEMP36_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP36]], i32 0, i32 0, !dbg [[DBG37]]
// PPC64-NEXT:    [[ATOMIC_TEMP36_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP36_REALP]], align 4, !dbg [[DBG37]]
// PPC64-NEXT:    [[ATOMIC_TEMP36_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP36]], i32 0, i32 1, !dbg [[DBG37]]
// PPC64-NEXT:    [[ATOMIC_TEMP36_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP36_IMAGP]], align 4, !dbg [[DBG37]]
// PPC64-NEXT:    [[CONV37:%.*]] = sitofp i32 [[ATOMIC_TEMP36_REAL]] to float, !dbg [[DBG37]]
// PPC64-NEXT:    store float [[CONV37]], float* @fv, align 4, !dbg [[DBG37]]
// PPC64-NEXT:    [[ATOMIC_LOAD38:%.*]] = load atomic i16, i16* @sx monotonic, align 2, !dbg [[DBG38:![0-9]+]]
// PPC64-NEXT:    [[CONV39:%.*]] = sitofp i16 [[ATOMIC_LOAD38]] to double, !dbg [[DBG38]]
// PPC64-NEXT:    store double [[CONV39]], double* @dv, align 8, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_LOAD40:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG39:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL41:%.*]] = trunc i8 [[ATOMIC_LOAD40]] to i1, !dbg [[DBG39]]
// PPC64-NEXT:    [[CONV42:%.*]] = uitofp i1 [[TOBOOL41]] to ppc_fp128, !dbg [[DBG39]]
// PPC64-NEXT:    store ppc_fp128 [[CONV42]], ppc_fp128* @ldv, align 16, !dbg [[DBG39]]
// PPC64-NEXT:    [[ATOMIC_LOAD43:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL44:%.*]] = trunc i8 [[ATOMIC_LOAD43]] to i1, !dbg [[DBG40]]
// PPC64-NEXT:    [[CONV45:%.*]] = zext i1 [[TOBOOL44]] to i32, !dbg [[DBG40]]
// PPC64-NEXT:    store i32 [[CONV45]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG40]]
// PPC64-NEXT:    store i32 0, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG40]]
// PPC64-NEXT:    [[ATOMIC_LOAD46:%.*]] = load atomic i16, i16* @usx monotonic, align 2, !dbg [[DBG41:![0-9]+]]
// PPC64-NEXT:    [[CONV47:%.*]] = uitofp i16 [[ATOMIC_LOAD46]] to float, !dbg [[DBG41]]
// PPC64-NEXT:    store float [[CONV47]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG41]]
// PPC64-NEXT:    store float 0.000000e+00, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG41]]
// PPC64-NEXT:    [[ATOMIC_LOAD48:%.*]] = load atomic i64, i64* @llx monotonic, align 8, !dbg [[DBG42:![0-9]+]]
// PPC64-NEXT:    [[CONV49:%.*]] = sitofp i64 [[ATOMIC_LOAD48]] to double, !dbg [[DBG42]]
// PPC64-NEXT:    store double [[CONV49]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG42]]
// PPC64-NEXT:    store double 0.000000e+00, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG42]]
// PPC64-NEXT:    [[TMP13:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP50]] to i8*, !dbg [[DBG43:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast (<4 x i32>* @int4x to i8*), i8* noundef [[TMP13]], i32 noundef signext 0), !dbg [[DBG43]]
// PPC64-NEXT:    [[TMP14:%.*]] = load <4 x i32>, <4 x i32>* [[ATOMIC_TEMP50]], align 16, !dbg [[DBG43]]
// PPC64-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP14]], i32 0, !dbg [[DBG43]]
// PPC64-NEXT:    [[TOBOOL51:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG43]]
// PPC64-NEXT:    [[FROMBOOL52:%.*]] = zext i1 [[TOBOOL51]] to i8, !dbg [[DBG43]]
// PPC64-NEXT:    store i8 [[FROMBOOL52]], i8* @bv, align 1, !dbg [[DBG43]]
// PPC64-NEXT:    [[ATOMIC_LOAD53:%.*]] = load atomic i32, i32* bitcast (%struct.BitFields* @bfx to i32*) monotonic, align 4, !dbg [[DBG44:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD53]], i32* [[ATOMIC_TEMP54]], align 4, !dbg [[DBG44]]
// PPC64-NEXT:    [[BF_LOAD:%.*]] = load i32, i32* [[ATOMIC_TEMP54]], align 4, !dbg [[DBG44]]
// PPC64-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_LOAD]], 1, !dbg [[DBG44]]
// PPC64-NEXT:    [[CONV55:%.*]] = sitofp i32 [[BF_ASHR]] to ppc_fp128, !dbg [[DBG44]]
// PPC64-NEXT:    store ppc_fp128 [[CONV55]], ppc_fp128* @ldv, align 16, !dbg [[DBG44]]
// PPC64-NEXT:    [[TMP15:%.*]] = bitcast i32* [[ATOMIC_TEMP56]] to i8*, !dbg [[DBG45:![0-9]+]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 4, i8* noundef bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i8* noundef [[TMP15]], i32 noundef signext 0), !dbg [[DBG45]]
// PPC64-NEXT:    [[BF_LOAD57:%.*]] = load i32, i32* [[ATOMIC_TEMP56]], align 1, !dbg [[DBG45]]
// PPC64-NEXT:    [[BF_ASHR58:%.*]] = ashr i32 [[BF_LOAD57]], 1, !dbg [[DBG45]]
// PPC64-NEXT:    [[CONV59:%.*]] = sitofp i32 [[BF_ASHR58]] to ppc_fp128, !dbg [[DBG45]]
// PPC64-NEXT:    store ppc_fp128 [[CONV59]], ppc_fp128* @ldv, align 16, !dbg [[DBG45]]
// PPC64-NEXT:    [[ATOMIC_LOAD60:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS2:%.*]], %struct.BitFields2* @bfx2, i32 0, i32 0) monotonic, align 4, !dbg [[DBG46:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD60]], i32* [[ATOMIC_TEMP61]], align 4, !dbg [[DBG46]]
// PPC64-NEXT:    [[BF_LOAD62:%.*]] = load i32, i32* [[ATOMIC_TEMP61]], align 4, !dbg [[DBG46]]
// PPC64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD62]], 31, !dbg [[DBG46]]
// PPC64-NEXT:    [[BF_ASHR63:%.*]] = ashr i32 [[BF_SHL]], 31, !dbg [[DBG46]]
// PPC64-NEXT:    [[CONV64:%.*]] = sitofp i32 [[BF_ASHR63]] to ppc_fp128, !dbg [[DBG46]]
// PPC64-NEXT:    store ppc_fp128 [[CONV64]], ppc_fp128* @ldv, align 16, !dbg [[DBG46]]
// PPC64-NEXT:    [[ATOMIC_LOAD65:%.*]] = load atomic i8, i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*) monotonic, align 1, !dbg [[DBG47:![0-9]+]]
// PPC64-NEXT:    [[TMP16:%.*]] = bitcast i32* [[ATOMIC_TEMP66]] to i8*, !dbg [[DBG47]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD65]], i8* [[TMP16]], align 1, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_LOAD67:%.*]] = load i8, i8* [[TMP16]], align 1, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_SHL68:%.*]] = shl i8 [[BF_LOAD67]], 7, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_ASHR69:%.*]] = ashr i8 [[BF_SHL68]], 7, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR69]] to i32, !dbg [[DBG47]]
// PPC64-NEXT:    [[CONV70:%.*]] = sitofp i32 [[BF_CAST]] to ppc_fp128, !dbg [[DBG47]]
// PPC64-NEXT:    store ppc_fp128 [[CONV70]], ppc_fp128* @ldv, align 16, !dbg [[DBG47]]
// PPC64-NEXT:    [[ATOMIC_LOAD71:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS3:%.*]], %struct.BitFields3* @bfx3, i32 0, i32 0) monotonic, align 4, !dbg [[DBG48:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD71]], i32* [[ATOMIC_TEMP72]], align 4, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_LOAD73:%.*]] = load i32, i32* [[ATOMIC_TEMP72]], align 4, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_SHL74:%.*]] = shl i32 [[BF_LOAD73]], 11, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_ASHR75:%.*]] = ashr i32 [[BF_SHL74]], 18, !dbg [[DBG48]]
// PPC64-NEXT:    [[CONV76:%.*]] = sitofp i32 [[BF_ASHR75]] to ppc_fp128, !dbg [[DBG48]]
// PPC64-NEXT:    store ppc_fp128 [[CONV76]], ppc_fp128* @ldv, align 16, !dbg [[DBG48]]
// PPC64-NEXT:    [[TMP17:%.*]] = bitcast i32* [[ATOMIC_TEMP77]] to i24*, !dbg [[DBG49:![0-9]+]]
// PPC64-NEXT:    [[TMP18:%.*]] = bitcast i24* [[TMP17]] to i8*, !dbg [[DBG49]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 3, i8* noundef bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i8* noundef [[TMP18]], i32 noundef signext 0), !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_LOAD78:%.*]] = load i24, i24* [[TMP17]], align 1, !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_SHL79:%.*]] = shl i24 [[BF_LOAD78]], 3, !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_ASHR80:%.*]] = ashr i24 [[BF_SHL79]], 10, !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_CAST81:%.*]] = sext i24 [[BF_ASHR80]] to i32, !dbg [[DBG49]]
// PPC64-NEXT:    [[CONV82:%.*]] = sitofp i32 [[BF_CAST81]] to ppc_fp128, !dbg [[DBG49]]
// PPC64-NEXT:    store ppc_fp128 [[CONV82]], ppc_fp128* @ldv, align 16, !dbg [[DBG49]]
// PPC64-NEXT:    [[ATOMIC_LOAD83:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG50:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD83]], i64* [[ATOMIC_TEMP84]], align 8, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_LOAD85:%.*]] = load i64, i64* [[ATOMIC_TEMP84]], align 8, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_SHL86:%.*]] = shl i64 [[BF_LOAD85]], 48, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_ASHR87:%.*]] = ashr i64 [[BF_SHL86]], 63, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_CAST88:%.*]] = trunc i64 [[BF_ASHR87]] to i32, !dbg [[DBG50]]
// PPC64-NEXT:    [[CONV89:%.*]] = sitofp i32 [[BF_CAST88]] to ppc_fp128, !dbg [[DBG50]]
// PPC64-NEXT:    store ppc_fp128 [[CONV89]], ppc_fp128* @ldv, align 16, !dbg [[DBG50]]
// PPC64-NEXT:    [[ATOMIC_LOAD90:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i32 0) monotonic, align 1, !dbg [[DBG51:![0-9]+]]
// PPC64-NEXT:    [[TMP19:%.*]] = bitcast i32* [[ATOMIC_TEMP91]] to i8*, !dbg [[DBG51]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD90]], i8* [[TMP19]], align 1, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_LOAD92:%.*]] = load i8, i8* [[TMP19]], align 1, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_ASHR93:%.*]] = ashr i8 [[BF_LOAD92]], 7, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_CAST94:%.*]] = sext i8 [[BF_ASHR93]] to i32, !dbg [[DBG51]]
// PPC64-NEXT:    [[CONV95:%.*]] = sitofp i32 [[BF_CAST94]] to ppc_fp128, !dbg [[DBG51]]
// PPC64-NEXT:    store ppc_fp128 [[CONV95]], ppc_fp128* @ldv, align 16, !dbg [[DBG51]]
// PPC64-NEXT:    [[ATOMIC_LOAD96:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG52:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD96]], i64* [[ATOMIC_TEMP97]], align 8, !dbg [[DBG52]]
// PPC64-NEXT:    [[BF_LOAD98:%.*]] = load i64, i64* [[ATOMIC_TEMP97]], align 8, !dbg [[DBG52]]
// PPC64-NEXT:    [[BF_SHL99:%.*]] = shl i64 [[BF_LOAD98]], 49, !dbg [[DBG52]]
// PPC64-NEXT:    [[BF_ASHR100:%.*]] = ashr i64 [[BF_SHL99]], 57, !dbg [[DBG52]]
// PPC64-NEXT:    [[CONV101:%.*]] = sitofp i64 [[BF_ASHR100]] to ppc_fp128, !dbg [[DBG52]]
// PPC64-NEXT:    store ppc_fp128 [[CONV101]], ppc_fp128* @ldv, align 16, !dbg [[DBG52]]
// PPC64-NEXT:    [[ATOMIC_LOAD102:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i32 0) acquire, align 1, !dbg [[DBG53:![0-9]+]]
// PPC64-NEXT:    [[TMP20:%.*]] = bitcast i64* [[ATOMIC_TEMP103]] to i8*, !dbg [[DBG53]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD102]], i8* [[TMP20]], align 1, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_LOAD104:%.*]] = load i8, i8* [[TMP20]], align 1, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_SHL105:%.*]] = shl i8 [[BF_LOAD104]], 1, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_ASHR106:%.*]] = ashr i8 [[BF_SHL105]], 1, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_CAST107:%.*]] = sext i8 [[BF_ASHR106]] to i64, !dbg [[DBG53]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    [[CONV108:%.*]] = sitofp i64 [[BF_CAST107]] to ppc_fp128, !dbg [[DBG53]]
// PPC64-NEXT:    store ppc_fp128 [[CONV108]], ppc_fp128* @ldv, align 16, !dbg [[DBG53]]
// PPC64-NEXT:    [[ATOMIC_LOAD109:%.*]] = load atomic i64, i64* bitcast (<2 x float>* @float2x to i64*) monotonic, align 8, !dbg [[DBG54:![0-9]+]]
// PPC64-NEXT:    [[TMP21:%.*]] = bitcast <2 x float>* [[ATOMIC_TEMP110]] to i64*, !dbg [[DBG54]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD109]], i64* [[TMP21]], align 8, !dbg [[DBG54]]
// PPC64-NEXT:    [[TMP22:%.*]] = load <2 x float>, <2 x float>* [[ATOMIC_TEMP110]], align 8, !dbg [[DBG54]]
// PPC64-NEXT:    [[TMP23:%.*]] = extractelement <2 x float> [[TMP22]], i64 0, !dbg [[DBG54]]
// PPC64-NEXT:    [[CONV111:%.*]] = fptoui float [[TMP23]] to i64, !dbg [[DBG54]]
// PPC64-NEXT:    store i64 [[CONV111]], i64* @ulv, align 8, !dbg [[DBG54]]
// PPC64-NEXT:    ret i32 0, !dbg [[DBG55:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@main
// AARCH64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP28:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP36:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP51:%.*]] = alloca <4 x i32>, align 16
// AARCH64-NEXT:    [[ATOMIC_TEMP55:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP57:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP63:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP68:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP73:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP78:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP85:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP92:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP99:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP105:%.*]] = alloca i64, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP111:%.*]] = alloca <2 x float>, align 8
// AARCH64-NEXT:    store i32 0, i32* [[RETVAL]], align 4
// AARCH64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG9:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG9]]
// AARCH64-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG9]]
// AARCH64-NEXT:    store i8 [[FROMBOOL]], i8* @bv, align 1, !dbg [[DBG9]]
// AARCH64-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, i8* @cx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD1]], i8* @cv, align 1, !dbg [[DBG10]]
// AARCH64-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, i8* @ucx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD2]], i8* @ucv, align 1, !dbg [[DBG11]]
// AARCH64-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, i16* @sx monotonic, align 2, !dbg [[DBG12:![0-9]+]]
// AARCH64-NEXT:    store i16 [[ATOMIC_LOAD3]], i16* @sv, align 2, !dbg [[DBG12]]
// AARCH64-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, i16* @usx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    store i16 [[ATOMIC_LOAD4]], i16* @usv, align 2, !dbg [[DBG13]]
// AARCH64-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, i32* @ix monotonic, align 4, !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD5]], i32* @iv, align 4, !dbg [[DBG14]]
// AARCH64-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, i32* @uix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD6]], i32* @uiv, align 4, !dbg [[DBG15]]
// AARCH64-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, i64* @lx monotonic, align 8, !dbg [[DBG16:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD7]], i64* @lv, align 8, !dbg [[DBG16]]
// AARCH64-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD8]], i64* @ulv, align 8, !dbg [[DBG17]]
// AARCH64-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, i64* @llx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD9]], i64* @llv, align 8, !dbg [[DBG18]]
// AARCH64-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, i64* @ullx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD10]], i64* @ullv, align 8, !dbg [[DBG19]]
// AARCH64-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic i32, i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG20:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = bitcast i32 [[ATOMIC_LOAD11]] to float, !dbg [[DBG20]]
// AARCH64-NEXT:    store float [[TMP0]], float* @fv, align 4, !dbg [[DBG20]]
// AARCH64-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic i64, i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG21:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = bitcast i64 [[ATOMIC_LOAD12]] to double, !dbg [[DBG21]]
// AARCH64-NEXT:    store double [[TMP1]], double* @dv, align 8, !dbg [[DBG21]]
// AARCH64-NEXT:    [[ATOMIC_LOAD13:%.*]] = load atomic i128, i128* bitcast (fp128* @ldx to i128*) monotonic, align 16, !dbg [[DBG22:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = bitcast i128 [[ATOMIC_LOAD13]] to fp128, !dbg [[DBG22]]
// AARCH64-NEXT:    store fp128 [[TMP2]], fp128* @ldv, align 16, !dbg [[DBG22]]
// AARCH64-NEXT:    [[TMP3:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP]] to i8*, !dbg [[DBG23:![0-9]+]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP3]], i32 noundef 0), !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP]], i32 0, i32 0, !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP_REALP]], align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP]], i32 0, i32 1, !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP_IMAGP]], align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    store i32 [[ATOMIC_TEMP_REAL]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    store i32 [[ATOMIC_TEMP_IMAG]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    [[TMP4:%.*]] = bitcast { float, float }* [[ATOMIC_TEMP14]] to i8*, !dbg [[DBG24:![0-9]+]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ float, float }* @cfx to i8*), i8* noundef [[TMP4]], i32 noundef 0), !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, float* [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, float* [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    store float [[ATOMIC_TEMP14_REAL]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    [[TMP5:%.*]] = bitcast { double, double }* [[ATOMIC_TEMP15]] to i8*, !dbg [[DBG25:![0-9]+]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 16, i8* noundef bitcast ({ double, double }* @cdx to i8*), i8* noundef [[TMP5]], i32 noundef 5), !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, double* [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, double* [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    store double [[ATOMIC_TEMP15_REAL]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG26:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL17:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG26]]
// AARCH64-NEXT:    [[FROMBOOL18:%.*]] = zext i1 [[TOBOOL17]] to i8, !dbg [[DBG26]]
// AARCH64-NEXT:    store i8 [[FROMBOOL18]], i8* @bv, align 1, !dbg [[DBG26]]
// AARCH64-NEXT:    [[ATOMIC_LOAD19:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG27:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL20:%.*]] = trunc i8 [[ATOMIC_LOAD19]] to i1, !dbg [[DBG27]]
// AARCH64-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL20]] to i8, !dbg [[DBG27]]
// AARCH64-NEXT:    store i8 [[CONV]], i8* @cv, align 1, !dbg [[DBG27]]
// AARCH64-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i8, i8* @cx seq_cst, align 1, !dbg [[DBG28:![0-9]+]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD21]], i8* @ucv, align 1, !dbg [[DBG28]]
// AARCH64-NEXT:    [[ATOMIC_LOAD22:%.*]] = load atomic i64, i64* @ulx monotonic, align 8, !dbg [[DBG29:![0-9]+]]
// AARCH64-NEXT:    [[CONV23:%.*]] = trunc i64 [[ATOMIC_LOAD22]] to i16, !dbg [[DBG29]]
// AARCH64-NEXT:    store i16 [[CONV23]], i16* @sv, align 2, !dbg [[DBG29]]
// AARCH64-NEXT:    [[ATOMIC_LOAD24:%.*]] = load atomic i64, i64* @lx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// AARCH64-NEXT:    [[CONV25:%.*]] = trunc i64 [[ATOMIC_LOAD24]] to i16, !dbg [[DBG30]]
// AARCH64-NEXT:    store i16 [[CONV25]], i16* @usv, align 2, !dbg [[DBG30]]
// AARCH64-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, i32* @uix seq_cst, align 4, !dbg [[DBG31:![0-9]+]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD26]], i32* @iv, align 4, !dbg [[DBG31]]
// AARCH64-NEXT:    [[ATOMIC_LOAD27:%.*]] = load atomic i32, i32* @ix monotonic, align 4, !dbg [[DBG32:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD27]], i32* @uiv, align 4, !dbg [[DBG32]]
// AARCH64-NEXT:    [[TMP6:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP28]] to i8*, !dbg [[DBG33:![0-9]+]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP6]], i32 noundef 0), !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP28_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP28]], i32 0, i32 0, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP28_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP28_REALP]], align 4, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP28_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP28]], i32 0, i32 1, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP28_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP28_IMAGP]], align 4, !dbg [[DBG33]]
// AARCH64-NEXT:    [[CONV29:%.*]] = sext i32 [[ATOMIC_TEMP28_REAL]] to i64, !dbg [[DBG33]]
// AARCH64-NEXT:    store i64 [[CONV29]], i64* @lv, align 8, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_LOAD30:%.*]] = load atomic i32, i32* bitcast (float* @fx to i32*) monotonic, align 4, !dbg [[DBG34:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = bitcast i32 [[ATOMIC_LOAD30]] to float, !dbg [[DBG34]]
// AARCH64-NEXT:    [[CONV31:%.*]] = fptoui float [[TMP7]] to i64, !dbg [[DBG34]]
// AARCH64-NEXT:    store i64 [[CONV31]], i64* @ulv, align 8, !dbg [[DBG34]]
// AARCH64-NEXT:    [[ATOMIC_LOAD32:%.*]] = load atomic i64, i64* bitcast (double* @dx to i64*) monotonic, align 8, !dbg [[DBG35:![0-9]+]]
// AARCH64-NEXT:    [[TMP8:%.*]] = bitcast i64 [[ATOMIC_LOAD32]] to double, !dbg [[DBG35]]
// AARCH64-NEXT:    [[CONV33:%.*]] = fptosi double [[TMP8]] to i64, !dbg [[DBG35]]
// AARCH64-NEXT:    store i64 [[CONV33]], i64* @llv, align 8, !dbg [[DBG35]]
// AARCH64-NEXT:    [[ATOMIC_LOAD34:%.*]] = load atomic i128, i128* bitcast (fp128* @ldx to i128*) monotonic, align 16, !dbg [[DBG36:![0-9]+]]
// AARCH64-NEXT:    [[TMP9:%.*]] = bitcast i128 [[ATOMIC_LOAD34]] to fp128, !dbg [[DBG36]]
// AARCH64-NEXT:    [[CONV35:%.*]] = fptoui fp128 [[TMP9]] to i64, !dbg [[DBG36]]
// AARCH64-NEXT:    store i64 [[CONV35]], i64* @ullv, align 8, !dbg [[DBG36]]
// AARCH64-NEXT:    [[TMP10:%.*]] = bitcast { i32, i32 }* [[ATOMIC_TEMP36]] to i8*, !dbg [[DBG37:![0-9]+]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, i8* noundef bitcast ({ i32, i32 }* @cix to i8*), i8* noundef [[TMP10]], i32 noundef 0), !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP36_REALP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP36]], i32 0, i32 0, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP36_REAL:%.*]] = load i32, i32* [[ATOMIC_TEMP36_REALP]], align 4, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP36_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, { i32, i32 }* [[ATOMIC_TEMP36]], i32 0, i32 1, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP36_IMAG:%.*]] = load i32, i32* [[ATOMIC_TEMP36_IMAGP]], align 4, !dbg [[DBG37]]
// AARCH64-NEXT:    [[CONV37:%.*]] = sitofp i32 [[ATOMIC_TEMP36_REAL]] to float, !dbg [[DBG37]]
// AARCH64-NEXT:    store float [[CONV37]], float* @fv, align 4, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_LOAD38:%.*]] = load atomic i16, i16* @sx monotonic, align 2, !dbg [[DBG38:![0-9]+]]
// AARCH64-NEXT:    [[CONV39:%.*]] = sitofp i16 [[ATOMIC_LOAD38]] to double, !dbg [[DBG38]]
// AARCH64-NEXT:    store double [[CONV39]], double* @dv, align 8, !dbg [[DBG38]]
// AARCH64-NEXT:    [[ATOMIC_LOAD40:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG39:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL41:%.*]] = trunc i8 [[ATOMIC_LOAD40]] to i1, !dbg [[DBG39]]
// AARCH64-NEXT:    [[CONV42:%.*]] = uitofp i1 [[TOBOOL41]] to fp128, !dbg [[DBG39]]
// AARCH64-NEXT:    store fp128 [[CONV42]], fp128* @ldv, align 16, !dbg [[DBG39]]
// AARCH64-NEXT:    [[ATOMIC_LOAD43:%.*]] = load atomic i8, i8* @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL44:%.*]] = trunc i8 [[ATOMIC_LOAD43]] to i1, !dbg [[DBG40]]
// AARCH64-NEXT:    [[CONV45:%.*]] = zext i1 [[TOBOOL44]] to i32, !dbg [[DBG40]]
// AARCH64-NEXT:    store i32 [[CONV45]], i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 0), align 4, !dbg [[DBG40]]
// AARCH64-NEXT:    store i32 0, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1), align 4, !dbg [[DBG40]]
// AARCH64-NEXT:    [[ATOMIC_LOAD46:%.*]] = load atomic i16, i16* @usx monotonic, align 2, !dbg [[DBG41:![0-9]+]]
// AARCH64-NEXT:    [[CONV47:%.*]] = uitofp i16 [[ATOMIC_LOAD46]] to float, !dbg [[DBG41]]
// AARCH64-NEXT:    store float [[CONV47]], float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 0), align 4, !dbg [[DBG41]]
// AARCH64-NEXT:    store float 0.000000e+00, float* getelementptr inbounds ({ float, float }, { float, float }* @cfv, i32 0, i32 1), align 4, !dbg [[DBG41]]
// AARCH64-NEXT:    [[ATOMIC_LOAD48:%.*]] = load atomic i64, i64* @llx monotonic, align 8, !dbg [[DBG42:![0-9]+]]
// AARCH64-NEXT:    [[CONV49:%.*]] = sitofp i64 [[ATOMIC_LOAD48]] to double, !dbg [[DBG42]]
// AARCH64-NEXT:    store double [[CONV49]], double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 0), align 8, !dbg [[DBG42]]
// AARCH64-NEXT:    store double 0.000000e+00, double* getelementptr inbounds ({ double, double }, { double, double }* @cdv, i32 0, i32 1), align 8, !dbg [[DBG42]]
// AARCH64-NEXT:    [[ATOMIC_LOAD50:%.*]] = load atomic i128, i128* bitcast (<4 x i32>* @int4x to i128*) monotonic, align 16, !dbg [[DBG43:![0-9]+]]
// AARCH64-NEXT:    [[TMP11:%.*]] = bitcast <4 x i32>* [[ATOMIC_TEMP51]] to i128*, !dbg [[DBG43]]
// AARCH64-NEXT:    store i128 [[ATOMIC_LOAD50]], i128* [[TMP11]], align 16, !dbg [[DBG43]]
// AARCH64-NEXT:    [[TMP12:%.*]] = load <4 x i32>, <4 x i32>* [[ATOMIC_TEMP51]], align 16, !dbg [[DBG43]]
// AARCH64-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP12]], i32 0, !dbg [[DBG43]]
// AARCH64-NEXT:    [[TOBOOL52:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG43]]
// AARCH64-NEXT:    [[FROMBOOL53:%.*]] = zext i1 [[TOBOOL52]] to i8, !dbg [[DBG43]]
// AARCH64-NEXT:    store i8 [[FROMBOOL53]], i8* @bv, align 1, !dbg [[DBG43]]
// AARCH64-NEXT:    [[ATOMIC_LOAD54:%.*]] = load atomic i32, i32* bitcast (i8* getelementptr (i8, i8* bitcast (%struct.BitFields* @bfx to i8*), i64 4) to i32*) monotonic, align 4, !dbg [[DBG44:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD54]], i32* [[ATOMIC_TEMP55]], align 4, !dbg [[DBG44]]
// AARCH64-NEXT:    [[BF_LOAD:%.*]] = load i32, i32* [[ATOMIC_TEMP55]], align 4, !dbg [[DBG44]]
// AARCH64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD]], 1, !dbg [[DBG44]]
// AARCH64-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_SHL]], 1, !dbg [[DBG44]]
// AARCH64-NEXT:    [[CONV56:%.*]] = sitofp i32 [[BF_ASHR]] to fp128, !dbg [[DBG44]]
// AARCH64-NEXT:    store fp128 [[CONV56]], fp128* @ldv, align 16, !dbg [[DBG44]]
// AARCH64-NEXT:    [[TMP13:%.*]] = bitcast i32* [[ATOMIC_TEMP57]] to i8*, !dbg [[DBG45:![0-9]+]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 4, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields_packed* @bfx_packed to i8*), i64 4), i8* noundef [[TMP13]], i32 noundef 0), !dbg [[DBG45]]
// AARCH64-NEXT:    [[BF_LOAD58:%.*]] = load i32, i32* [[ATOMIC_TEMP57]], align 1, !dbg [[DBG45]]
// AARCH64-NEXT:    [[BF_SHL59:%.*]] = shl i32 [[BF_LOAD58]], 1, !dbg [[DBG45]]
// AARCH64-NEXT:    [[BF_ASHR60:%.*]] = ashr i32 [[BF_SHL59]], 1, !dbg [[DBG45]]
// AARCH64-NEXT:    [[CONV61:%.*]] = sitofp i32 [[BF_ASHR60]] to fp128, !dbg [[DBG45]]
// AARCH64-NEXT:    store fp128 [[CONV61]], fp128* @ldv, align 16, !dbg [[DBG45]]
// AARCH64-NEXT:    [[ATOMIC_LOAD62:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS2:%.*]], %struct.BitFields2* @bfx2, i32 0, i32 0) monotonic, align 4, !dbg [[DBG46:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD62]], i32* [[ATOMIC_TEMP63]], align 4, !dbg [[DBG46]]
// AARCH64-NEXT:    [[BF_LOAD64:%.*]] = load i32, i32* [[ATOMIC_TEMP63]], align 4, !dbg [[DBG46]]
// AARCH64-NEXT:    [[BF_ASHR65:%.*]] = ashr i32 [[BF_LOAD64]], 31, !dbg [[DBG46]]
// AARCH64-NEXT:    [[CONV66:%.*]] = sitofp i32 [[BF_ASHR65]] to fp128, !dbg [[DBG46]]
// AARCH64-NEXT:    store fp128 [[CONV66]], fp128* @ldv, align 16, !dbg [[DBG46]]
// AARCH64-NEXT:    [[ATOMIC_LOAD67:%.*]] = load atomic i8, i8* getelementptr (i8, i8* bitcast (%struct.BitFields2_packed* @bfx2_packed to i8*), i64 3) monotonic, align 1, !dbg [[DBG47:![0-9]+]]
// AARCH64-NEXT:    [[TMP14:%.*]] = bitcast i32* [[ATOMIC_TEMP68]] to i8*, !dbg [[DBG47]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD67]], i8* [[TMP14]], align 1, !dbg [[DBG47]]
// AARCH64-NEXT:    [[BF_LOAD69:%.*]] = load i8, i8* [[TMP14]], align 1, !dbg [[DBG47]]
// AARCH64-NEXT:    [[BF_ASHR70:%.*]] = ashr i8 [[BF_LOAD69]], 7, !dbg [[DBG47]]
// AARCH64-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR70]] to i32, !dbg [[DBG47]]
// AARCH64-NEXT:    [[CONV71:%.*]] = sitofp i32 [[BF_CAST]] to fp128, !dbg [[DBG47]]
// AARCH64-NEXT:    store fp128 [[CONV71]], fp128* @ldv, align 16, !dbg [[DBG47]]
// AARCH64-NEXT:    [[ATOMIC_LOAD72:%.*]] = load atomic i32, i32* getelementptr inbounds ([[STRUCT_BITFIELDS3:%.*]], %struct.BitFields3* @bfx3, i32 0, i32 0) monotonic, align 4, !dbg [[DBG48:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD72]], i32* [[ATOMIC_TEMP73]], align 4, !dbg [[DBG48]]
// AARCH64-NEXT:    [[BF_LOAD74:%.*]] = load i32, i32* [[ATOMIC_TEMP73]], align 4, !dbg [[DBG48]]
// AARCH64-NEXT:    [[BF_SHL75:%.*]] = shl i32 [[BF_LOAD74]], 7, !dbg [[DBG48]]
// AARCH64-NEXT:    [[BF_ASHR76:%.*]] = ashr i32 [[BF_SHL75]], 18, !dbg [[DBG48]]
// AARCH64-NEXT:    [[CONV77:%.*]] = sitofp i32 [[BF_ASHR76]] to fp128, !dbg [[DBG48]]
// AARCH64-NEXT:    store fp128 [[CONV77]], fp128* @ldv, align 16, !dbg [[DBG48]]
// AARCH64-NEXT:    [[TMP15:%.*]] = bitcast i32* [[ATOMIC_TEMP78]] to i24*, !dbg [[DBG49:![0-9]+]]
// AARCH64-NEXT:    [[TMP16:%.*]] = bitcast i24* [[TMP15]] to i8*, !dbg [[DBG49]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 3, i8* noundef getelementptr (i8, i8* bitcast (%struct.BitFields3_packed* @bfx3_packed to i8*), i64 1), i8* noundef [[TMP16]], i32 noundef 0), !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_LOAD79:%.*]] = load i24, i24* [[TMP15]], align 1, !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_SHL80:%.*]] = shl i24 [[BF_LOAD79]], 7, !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_ASHR81:%.*]] = ashr i24 [[BF_SHL80]], 10, !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_CAST82:%.*]] = sext i24 [[BF_ASHR81]] to i32, !dbg [[DBG49]]
// AARCH64-NEXT:    [[CONV83:%.*]] = sitofp i32 [[BF_CAST82]] to fp128, !dbg [[DBG49]]
// AARCH64-NEXT:    store fp128 [[CONV83]], fp128* @ldv, align 16, !dbg [[DBG49]]
// AARCH64-NEXT:    [[ATOMIC_LOAD84:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG50:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD84]], i64* [[ATOMIC_TEMP85]], align 8, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_LOAD86:%.*]] = load i64, i64* [[ATOMIC_TEMP85]], align 8, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_SHL87:%.*]] = shl i64 [[BF_LOAD86]], 47, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_ASHR88:%.*]] = ashr i64 [[BF_SHL87]], 63, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_CAST89:%.*]] = trunc i64 [[BF_ASHR88]] to i32, !dbg [[DBG50]]
// AARCH64-NEXT:    [[CONV90:%.*]] = sitofp i32 [[BF_CAST89]] to fp128, !dbg [[DBG50]]
// AARCH64-NEXT:    store fp128 [[CONV90]], fp128* @ldv, align 16, !dbg [[DBG50]]
// AARCH64-NEXT:    [[ATOMIC_LOAD91:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) monotonic, align 1, !dbg [[DBG51:![0-9]+]]
// AARCH64-NEXT:    [[TMP17:%.*]] = bitcast i32* [[ATOMIC_TEMP92]] to i8*, !dbg [[DBG51]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD91]], i8* [[TMP17]], align 1, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_LOAD93:%.*]] = load i8, i8* [[TMP17]], align 1, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_SHL94:%.*]] = shl i8 [[BF_LOAD93]], 7, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_ASHR95:%.*]] = ashr i8 [[BF_SHL94]], 7, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_CAST96:%.*]] = sext i8 [[BF_ASHR95]] to i32, !dbg [[DBG51]]
// AARCH64-NEXT:    [[CONV97:%.*]] = sitofp i32 [[BF_CAST96]] to fp128, !dbg [[DBG51]]
// AARCH64-NEXT:    store fp128 [[CONV97]], fp128* @ldv, align 16, !dbg [[DBG51]]
// AARCH64-NEXT:    [[ATOMIC_LOAD98:%.*]] = load atomic i64, i64* bitcast (%struct.BitFields4* @bfx4 to i64*) monotonic, align 8, !dbg [[DBG52:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD98]], i64* [[ATOMIC_TEMP99]], align 8, !dbg [[DBG52]]
// AARCH64-NEXT:    [[BF_LOAD100:%.*]] = load i64, i64* [[ATOMIC_TEMP99]], align 8, !dbg [[DBG52]]
// AARCH64-NEXT:    [[BF_SHL101:%.*]] = shl i64 [[BF_LOAD100]], 40, !dbg [[DBG52]]
// AARCH64-NEXT:    [[BF_ASHR102:%.*]] = ashr i64 [[BF_SHL101]], 57, !dbg [[DBG52]]
// AARCH64-NEXT:    [[CONV103:%.*]] = sitofp i64 [[BF_ASHR102]] to fp128, !dbg [[DBG52]]
// AARCH64-NEXT:    store fp128 [[CONV103]], fp128* @ldv, align 16, !dbg [[DBG52]]
// AARCH64-NEXT:    [[ATOMIC_LOAD104:%.*]] = load atomic i8, i8* getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], %struct.BitFields4_packed* @bfx4_packed, i32 0, i32 0, i64 2) acquire, align 1, !dbg [[DBG53:![0-9]+]]
// AARCH64-NEXT:    [[TMP18:%.*]] = bitcast i64* [[ATOMIC_TEMP105]] to i8*, !dbg [[DBG53]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD104]], i8* [[TMP18]], align 1, !dbg [[DBG53]]
// AARCH64-NEXT:    [[BF_LOAD106:%.*]] = load i8, i8* [[TMP18]], align 1, !dbg [[DBG53]]
// AARCH64-NEXT:    [[BF_ASHR107:%.*]] = ashr i8 [[BF_LOAD106]], 1, !dbg [[DBG53]]
// AARCH64-NEXT:    [[BF_CAST108:%.*]] = sext i8 [[BF_ASHR107]] to i64, !dbg [[DBG53]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    [[CONV109:%.*]] = sitofp i64 [[BF_CAST108]] to fp128, !dbg [[DBG53]]
// AARCH64-NEXT:    store fp128 [[CONV109]], fp128* @ldv, align 16, !dbg [[DBG53]]
// AARCH64-NEXT:    [[ATOMIC_LOAD110:%.*]] = load atomic i64, i64* bitcast (<2 x float>* @float2x to i64*) monotonic, align 8, !dbg [[DBG54:![0-9]+]]
// AARCH64-NEXT:    [[TMP19:%.*]] = bitcast <2 x float>* [[ATOMIC_TEMP111]] to i64*, !dbg [[DBG54]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD110]], i64* [[TMP19]], align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    [[TMP20:%.*]] = load <2 x float>, <2 x float>* [[ATOMIC_TEMP111]], align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    [[TMP21:%.*]] = extractelement <2 x float> [[TMP20]], i64 0, !dbg [[DBG54]]
// AARCH64-NEXT:    [[CONV112:%.*]] = fptoui float [[TMP21]] to i64, !dbg [[DBG54]]
// AARCH64-NEXT:    store i64 [[CONV112]], i64* @ulv, align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    ret i32 0, !dbg [[DBG55:![0-9]+]]
//
