// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs
// RUN: %clang_cc1 -x c -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -x c -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -x c -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics
#ifndef HEADER
#define HEADER

_Bool bv, bx;
char cv, cx;
unsigned char ucv, ucx;
short sv, sx;
unsigned short usv, usx;
int iv, ix;
unsigned int uiv, uix;
long lv, lx;
unsigned long ulv, ulx;
long long llv, llx;
unsigned long long ullv, ullx;
float fv, fx;
double dv, dx;
long double ldv, ldx;
_Complex int civ, cix;
_Complex float cfv, cfx;
_Complex double cdv, cdx;

typedef int int4 __attribute__((__vector_size__(16)));
int4 int4x;

struct BitFields {
  int : 32;
  int a : 31;
} bfx;

struct BitFields_packed {
  int : 32;
  int a : 31;
} __attribute__ ((__packed__)) bfx_packed;

struct BitFields2 {
  int : 31;
  int a : 1;
} bfx2;

struct BitFields2_packed {
  int : 31;
  int a : 1;
} __attribute__ ((__packed__)) bfx2_packed;

struct BitFields3 {
  int : 11;
  int a : 14;
} bfx3;

struct BitFields3_packed {
  int : 11;
  int a : 14;
} __attribute__ ((__packed__)) bfx3_packed;

struct BitFields4 {
  short : 16;
  int a: 1;
  long b : 7;
} bfx4;

struct BitFields4_packed {
  short : 16;
  int a: 1;
  long b : 7;
} __attribute__ ((__packed__)) bfx4_packed;

typedef float float2 __attribute__((ext_vector_type(2)));
float2 float2x;

#if defined(__x86_64__)
// Register "0" is currently an invalid register for global register variables.
// Use "esp" instead of "0".
// register int rix __asm__("0");
register int rix __asm__("esp");
#endif

int main(void) {
#pragma oss atomic read
  bv = bx;
#pragma oss atomic read
  cv = cx;
#pragma oss atomic read
  ucv = ucx;
#pragma oss atomic read
  sv = sx;
#pragma oss atomic read
  usv = usx;
#pragma oss atomic read
  iv = ix;
#pragma oss atomic read
  uiv = uix;
#pragma oss atomic read
  lv = lx;
#pragma oss atomic read
  ulv = ulx;
#pragma oss atomic read
  llv = llx;
#pragma oss atomic read
  ullv = ullx;
#pragma oss atomic read
  fv = fx;
#pragma oss atomic read
  dv = dx;
#pragma oss atomic read
  ldv = ldx;
#pragma oss atomic read
  civ = cix;
#pragma oss atomic read
  cfv = cfx;
#pragma oss atomic seq_cst read
  cdv = cdx;
#pragma oss atomic read
  bv = ulx;
#pragma oss atomic read
  cv = bx;
#pragma oss atomic read seq_cst
  ucv = cx;
#pragma oss atomic read
  sv = ulx;
#pragma oss atomic read
  usv = lx;
#pragma oss atomic seq_cst, read
  iv = uix;
#pragma oss atomic read
  uiv = ix;
#pragma oss atomic read
  lv = cix;
#pragma oss atomic read
  ulv = fx;
#pragma oss atomic read
  llv = dx;
#pragma oss atomic read
  ullv = ldx;
#pragma oss atomic read
  fv = cix;
#pragma oss atomic read
  dv = sx;
#pragma oss atomic read
  ldv = bx;
#pragma oss atomic read
  civ = bx;
#pragma oss atomic read
  cfv = usx;
#pragma oss atomic read
  cdv = llx;
#pragma oss atomic read
  bv = int4x[0];
#pragma oss atomic read
  ldv = bfx.a;
#pragma oss atomic read
  ldv = bfx_packed.a;
#pragma oss atomic read
  ldv = bfx2.a;
#pragma oss atomic read
  ldv = bfx2_packed.a;
#pragma oss atomic read
  ldv = bfx3.a;
#pragma oss atomic read
  ldv = bfx3_packed.a;
#pragma oss atomic read
  ldv = bfx4.a;
#pragma oss atomic relaxed read
  ldv = bfx4_packed.a;
#pragma oss atomic read relaxed
  ldv = bfx4.b;
#pragma oss atomic read acquire
  ldv = bfx4_packed.b;
#pragma oss atomic read
  ulv = float2x.x;
#if defined(__x86_64__)
#pragma oss atomic read seq_cst
  dv = rix;
#endif
  return 0;
}

#endif
// LIN64-LABEL: define {{[^@]+}}@main
// LIN64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG6:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca x86_fp80, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP13:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP28:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP34:%.*]] = alloca x86_fp80, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP36:%.*]] = alloca { i32, i32 }, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP50:%.*]] = alloca <4 x i32>, align 16
// LIN64-NEXT:    [[ATOMIC_TEMP54:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP56:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP62:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP67:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP72:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[ATOMIC_TEMP77:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP84:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP91:%.*]] = alloca i32, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP98:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[ATOMIC_TEMP104:%.*]] = alloca i64, align 1
// LIN64-NEXT:    [[ATOMIC_TEMP110:%.*]] = alloca <2 x float>, align 8
// LIN64-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// LIN64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG10]]
// LIN64-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG10]]
// LIN64-NEXT:    store i8 [[FROMBOOL]], ptr @bv, align 1, !dbg [[DBG10]]
// LIN64-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, ptr @cx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD1]], ptr @cv, align 1, !dbg [[DBG11]]
// LIN64-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, ptr @ucx monotonic, align 1, !dbg [[DBG12:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD2]], ptr @ucv, align 1, !dbg [[DBG12]]
// LIN64-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    store i16 [[ATOMIC_LOAD3]], ptr @sv, align 2, !dbg [[DBG13]]
// LIN64-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    store i16 [[ATOMIC_LOAD4]], ptr @usv, align 2, !dbg [[DBG14]]
// LIN64-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD5]], ptr @iv, align 4, !dbg [[DBG15]]
// LIN64-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, ptr @uix monotonic, align 4, !dbg [[DBG16:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD6]], ptr @uiv, align 4, !dbg [[DBG16]]
// LIN64-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD7]], ptr @lv, align 8, !dbg [[DBG17]]
// LIN64-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD8]], ptr @ulv, align 8, !dbg [[DBG18]]
// LIN64-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD9]], ptr @llv, align 8, !dbg [[DBG19]]
// LIN64-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, ptr @ullx monotonic, align 8, !dbg [[DBG20:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD10]], ptr @ullv, align 8, !dbg [[DBG20]]
// LIN64-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG21:![0-9]+]]
// LIN64-NEXT:    store float [[ATOMIC_LOAD11]], ptr @fv, align 4, !dbg [[DBG21]]
// LIN64-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG22:![0-9]+]]
// LIN64-NEXT:    store double [[ATOMIC_LOAD12]], ptr @dv, align 8, !dbg [[DBG22]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @ldx, ptr noundef [[ATOMIC_TEMP]], i32 noundef 0), !dbg [[DBG23:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = load x86_fp80, ptr [[ATOMIC_TEMP]], align 16, !dbg [[DBG23]]
// LIN64-NEXT:    store x86_fp80 [[TMP0]], ptr @ldv, align 16, !dbg [[DBG23]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP13]], i32 noundef 0), !dbg [[DBG24:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_REALP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP13]], i32 0, i32 0, !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP13_REALP]], align 4, !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP13]], i32 0, i32 1, !dbg [[DBG24]]
// LIN64-NEXT:    [[ATOMIC_TEMP13_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP13_IMAGP]], align 4, !dbg [[DBG24]]
// LIN64-NEXT:    store i32 [[ATOMIC_TEMP13_REAL]], ptr @civ, align 4, !dbg [[DBG24]]
// LIN64-NEXT:    store i32 [[ATOMIC_TEMP13_IMAG]], ptr getelementptr inbounds ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG24]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cfx, ptr noundef [[ATOMIC_TEMP14]], i32 noundef 0), !dbg [[DBG25:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, ptr [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG25]]
// LIN64-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, ptr [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG25]]
// LIN64-NEXT:    store float [[ATOMIC_TEMP14_REAL]], ptr @cfv, align 4, !dbg [[DBG25]]
// LIN64-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], ptr getelementptr inbounds ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG25]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @cdx, ptr noundef [[ATOMIC_TEMP15]], i32 noundef 5), !dbg [[DBG26:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, ptr [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, ptr [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG26]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    store double [[ATOMIC_TEMP15_REAL]], ptr @cdv, align 8, !dbg [[DBG26]]
// LIN64-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], ptr getelementptr inbounds ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG26]]
// LIN64-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG27:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL17:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG27]]
// LIN64-NEXT:    [[FROMBOOL18:%.*]] = zext i1 [[TOBOOL17]] to i8, !dbg [[DBG27]]
// LIN64-NEXT:    store i8 [[FROMBOOL18]], ptr @bv, align 1, !dbg [[DBG27]]
// LIN64-NEXT:    [[ATOMIC_LOAD19:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG28:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL20:%.*]] = trunc i8 [[ATOMIC_LOAD19]] to i1, !dbg [[DBG28]]
// LIN64-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL20]] to i8, !dbg [[DBG28]]
// LIN64-NEXT:    store i8 [[CONV]], ptr @cv, align 1, !dbg [[DBG28]]
// LIN64-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i8, ptr @cx seq_cst, align 1, !dbg [[DBG29:![0-9]+]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD21]], ptr @ucv, align 1, !dbg [[DBG29]]
// LIN64-NEXT:    [[ATOMIC_LOAD22:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// LIN64-NEXT:    [[CONV23:%.*]] = trunc i64 [[ATOMIC_LOAD22]] to i16, !dbg [[DBG30]]
// LIN64-NEXT:    store i16 [[CONV23]], ptr @sv, align 2, !dbg [[DBG30]]
// LIN64-NEXT:    [[ATOMIC_LOAD24:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG31:![0-9]+]]
// LIN64-NEXT:    [[CONV25:%.*]] = trunc i64 [[ATOMIC_LOAD24]] to i16, !dbg [[DBG31]]
// LIN64-NEXT:    store i16 [[CONV25]], ptr @usv, align 2, !dbg [[DBG31]]
// LIN64-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, ptr @uix seq_cst, align 4, !dbg [[DBG32:![0-9]+]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD26]], ptr @iv, align 4, !dbg [[DBG32]]
// LIN64-NEXT:    [[ATOMIC_LOAD27:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG33:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD27]], ptr @uiv, align 4, !dbg [[DBG33]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP28]], i32 noundef 0), !dbg [[DBG34:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP28_REALP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP28]], i32 0, i32 0, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP28_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP28_REALP]], align 4, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP28_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP28]], i32 0, i32 1, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_TEMP28_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP28_IMAGP]], align 4, !dbg [[DBG34]]
// LIN64-NEXT:    [[CONV29:%.*]] = sext i32 [[ATOMIC_TEMP28_REAL]] to i64, !dbg [[DBG34]]
// LIN64-NEXT:    store i64 [[CONV29]], ptr @lv, align 8, !dbg [[DBG34]]
// LIN64-NEXT:    [[ATOMIC_LOAD30:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG35:![0-9]+]]
// LIN64-NEXT:    [[CONV31:%.*]] = fptoui float [[ATOMIC_LOAD30]] to i64, !dbg [[DBG35]]
// LIN64-NEXT:    store i64 [[CONV31]], ptr @ulv, align 8, !dbg [[DBG35]]
// LIN64-NEXT:    [[ATOMIC_LOAD32:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG36:![0-9]+]]
// LIN64-NEXT:    [[CONV33:%.*]] = fptosi double [[ATOMIC_LOAD32]] to i64, !dbg [[DBG36]]
// LIN64-NEXT:    store i64 [[CONV33]], ptr @llv, align 8, !dbg [[DBG36]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @ldx, ptr noundef [[ATOMIC_TEMP34]], i32 noundef 0), !dbg [[DBG37:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load x86_fp80, ptr [[ATOMIC_TEMP34]], align 16, !dbg [[DBG37]]
// LIN64-NEXT:    [[CONV35:%.*]] = fptoui x86_fp80 [[TMP1]] to i64, !dbg [[DBG37]]
// LIN64-NEXT:    store i64 [[CONV35]], ptr @ullv, align 8, !dbg [[DBG37]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP36]], i32 noundef 0), !dbg [[DBG38:![0-9]+]]
// LIN64-NEXT:    [[ATOMIC_TEMP36_REALP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP36]], i32 0, i32 0, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP36_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP36_REALP]], align 4, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP36_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP36]], i32 0, i32 1, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_TEMP36_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP36_IMAGP]], align 4, !dbg [[DBG38]]
// LIN64-NEXT:    [[CONV37:%.*]] = sitofp i32 [[ATOMIC_TEMP36_REAL]] to float, !dbg [[DBG38]]
// LIN64-NEXT:    store float [[CONV37]], ptr @fv, align 4, !dbg [[DBG38]]
// LIN64-NEXT:    [[ATOMIC_LOAD38:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG39:![0-9]+]]
// LIN64-NEXT:    [[CONV39:%.*]] = sitofp i16 [[ATOMIC_LOAD38]] to double, !dbg [[DBG39]]
// LIN64-NEXT:    store double [[CONV39]], ptr @dv, align 8, !dbg [[DBG39]]
// LIN64-NEXT:    [[ATOMIC_LOAD40:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL41:%.*]] = trunc i8 [[ATOMIC_LOAD40]] to i1, !dbg [[DBG40]]
// LIN64-NEXT:    [[CONV42:%.*]] = uitofp i1 [[TOBOOL41]] to x86_fp80, !dbg [[DBG40]]
// LIN64-NEXT:    store x86_fp80 [[CONV42]], ptr @ldv, align 16, !dbg [[DBG40]]
// LIN64-NEXT:    [[ATOMIC_LOAD43:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG41:![0-9]+]]
// LIN64-NEXT:    [[TOBOOL44:%.*]] = trunc i8 [[ATOMIC_LOAD43]] to i1, !dbg [[DBG41]]
// LIN64-NEXT:    [[CONV45:%.*]] = zext i1 [[TOBOOL44]] to i32, !dbg [[DBG41]]
// LIN64-NEXT:    store i32 [[CONV45]], ptr @civ, align 4, !dbg [[DBG41]]
// LIN64-NEXT:    store i32 0, ptr getelementptr inbounds ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG41]]
// LIN64-NEXT:    [[ATOMIC_LOAD46:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG42:![0-9]+]]
// LIN64-NEXT:    [[CONV47:%.*]] = uitofp i16 [[ATOMIC_LOAD46]] to float, !dbg [[DBG42]]
// LIN64-NEXT:    store float [[CONV47]], ptr @cfv, align 4, !dbg [[DBG42]]
// LIN64-NEXT:    store float 0.000000e+00, ptr getelementptr inbounds ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG42]]
// LIN64-NEXT:    [[ATOMIC_LOAD48:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG43:![0-9]+]]
// LIN64-NEXT:    [[CONV49:%.*]] = sitofp i64 [[ATOMIC_LOAD48]] to double, !dbg [[DBG43]]
// LIN64-NEXT:    store double [[CONV49]], ptr @cdv, align 8, !dbg [[DBG43]]
// LIN64-NEXT:    store double 0.000000e+00, ptr getelementptr inbounds ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG43]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @int4x, ptr noundef [[ATOMIC_TEMP50]], i32 noundef 0), !dbg [[DBG44:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ATOMIC_TEMP50]], align 16, !dbg [[DBG44]]
// LIN64-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP2]], i32 0, !dbg [[DBG44]]
// LIN64-NEXT:    [[TOBOOL51:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG44]]
// LIN64-NEXT:    [[FROMBOOL52:%.*]] = zext i1 [[TOBOOL51]] to i8, !dbg [[DBG44]]
// LIN64-NEXT:    store i8 [[FROMBOOL52]], ptr @bv, align 1, !dbg [[DBG44]]
// LIN64-NEXT:    [[ATOMIC_LOAD53:%.*]] = load atomic i32, ptr getelementptr (i8, ptr @bfx, i64 4) monotonic, align 4, !dbg [[DBG45:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD53]], ptr [[ATOMIC_TEMP54]], align 4, !dbg [[DBG45]]
// LIN64-NEXT:    [[BF_LOAD:%.*]] = load i32, ptr [[ATOMIC_TEMP54]], align 4, !dbg [[DBG45]]
// LIN64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD]], 1, !dbg [[DBG45]]
// LIN64-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_SHL]], 1, !dbg [[DBG45]]
// LIN64-NEXT:    [[CONV55:%.*]] = sitofp i32 [[BF_ASHR]] to x86_fp80, !dbg [[DBG45]]
// LIN64-NEXT:    store x86_fp80 [[CONV55]], ptr @ldv, align 16, !dbg [[DBG45]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 4, ptr noundef getelementptr (i8, ptr @bfx_packed, i64 4), ptr noundef [[ATOMIC_TEMP56]], i32 noundef 0), !dbg [[DBG46:![0-9]+]]
// LIN64-NEXT:    [[BF_LOAD57:%.*]] = load i32, ptr [[ATOMIC_TEMP56]], align 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[BF_SHL58:%.*]] = shl i32 [[BF_LOAD57]], 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[BF_ASHR59:%.*]] = ashr i32 [[BF_SHL58]], 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[CONV60:%.*]] = sitofp i32 [[BF_ASHR59]] to x86_fp80, !dbg [[DBG46]]
// LIN64-NEXT:    store x86_fp80 [[CONV60]], ptr @ldv, align 16, !dbg [[DBG46]]
// LIN64-NEXT:    [[ATOMIC_LOAD61:%.*]] = load atomic i32, ptr @bfx2 monotonic, align 4, !dbg [[DBG47:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD61]], ptr [[ATOMIC_TEMP62]], align 4, !dbg [[DBG47]]
// LIN64-NEXT:    [[BF_LOAD63:%.*]] = load i32, ptr [[ATOMIC_TEMP62]], align 4, !dbg [[DBG47]]
// LIN64-NEXT:    [[BF_ASHR64:%.*]] = ashr i32 [[BF_LOAD63]], 31, !dbg [[DBG47]]
// LIN64-NEXT:    [[CONV65:%.*]] = sitofp i32 [[BF_ASHR64]] to x86_fp80, !dbg [[DBG47]]
// LIN64-NEXT:    store x86_fp80 [[CONV65]], ptr @ldv, align 16, !dbg [[DBG47]]
// LIN64-NEXT:    [[ATOMIC_LOAD66:%.*]] = load atomic i8, ptr getelementptr (i8, ptr @bfx2_packed, i64 3) monotonic, align 1, !dbg [[DBG48:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD66]], ptr [[ATOMIC_TEMP67]], align 1, !dbg [[DBG48]]
// LIN64-NEXT:    [[BF_LOAD68:%.*]] = load i8, ptr [[ATOMIC_TEMP67]], align 1, !dbg [[DBG48]]
// LIN64-NEXT:    [[BF_ASHR69:%.*]] = ashr i8 [[BF_LOAD68]], 7, !dbg [[DBG48]]
// LIN64-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR69]] to i32, !dbg [[DBG48]]
// LIN64-NEXT:    [[CONV70:%.*]] = sitofp i32 [[BF_CAST]] to x86_fp80, !dbg [[DBG48]]
// LIN64-NEXT:    store x86_fp80 [[CONV70]], ptr @ldv, align 16, !dbg [[DBG48]]
// LIN64-NEXT:    [[ATOMIC_LOAD71:%.*]] = load atomic i32, ptr @bfx3 monotonic, align 4, !dbg [[DBG49:![0-9]+]]
// LIN64-NEXT:    store i32 [[ATOMIC_LOAD71]], ptr [[ATOMIC_TEMP72]], align 4, !dbg [[DBG49]]
// LIN64-NEXT:    [[BF_LOAD73:%.*]] = load i32, ptr [[ATOMIC_TEMP72]], align 4, !dbg [[DBG49]]
// LIN64-NEXT:    [[BF_SHL74:%.*]] = shl i32 [[BF_LOAD73]], 7, !dbg [[DBG49]]
// LIN64-NEXT:    [[BF_ASHR75:%.*]] = ashr i32 [[BF_SHL74]], 18, !dbg [[DBG49]]
// LIN64-NEXT:    [[CONV76:%.*]] = sitofp i32 [[BF_ASHR75]] to x86_fp80, !dbg [[DBG49]]
// LIN64-NEXT:    store x86_fp80 [[CONV76]], ptr @ldv, align 16, !dbg [[DBG49]]
// LIN64-NEXT:    call void @__atomic_load(i64 noundef 3, ptr noundef getelementptr (i8, ptr @bfx3_packed, i64 1), ptr noundef [[ATOMIC_TEMP77]], i32 noundef 0), !dbg [[DBG50:![0-9]+]]
// LIN64-NEXT:    [[BF_LOAD78:%.*]] = load i24, ptr [[ATOMIC_TEMP77]], align 1, !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_SHL79:%.*]] = shl i24 [[BF_LOAD78]], 7, !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_ASHR80:%.*]] = ashr i24 [[BF_SHL79]], 10, !dbg [[DBG50]]
// LIN64-NEXT:    [[BF_CAST81:%.*]] = sext i24 [[BF_ASHR80]] to i32, !dbg [[DBG50]]
// LIN64-NEXT:    [[CONV82:%.*]] = sitofp i32 [[BF_CAST81]] to x86_fp80, !dbg [[DBG50]]
// LIN64-NEXT:    store x86_fp80 [[CONV82]], ptr @ldv, align 16, !dbg [[DBG50]]
// LIN64-NEXT:    [[ATOMIC_LOAD83:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG51:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD83]], ptr [[ATOMIC_TEMP84]], align 8, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_LOAD85:%.*]] = load i64, ptr [[ATOMIC_TEMP84]], align 8, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_SHL86:%.*]] = shl i64 [[BF_LOAD85]], 47, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_ASHR87:%.*]] = ashr i64 [[BF_SHL86]], 63, !dbg [[DBG51]]
// LIN64-NEXT:    [[BF_CAST88:%.*]] = trunc i64 [[BF_ASHR87]] to i32, !dbg [[DBG51]]
// LIN64-NEXT:    [[CONV89:%.*]] = sitofp i32 [[BF_CAST88]] to x86_fp80, !dbg [[DBG51]]
// LIN64-NEXT:    store x86_fp80 [[CONV89]], ptr @ldv, align 16, !dbg [[DBG51]]
// LIN64-NEXT:    [[ATOMIC_LOAD90:%.*]] = load atomic i8, ptr getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], ptr @bfx4_packed, i32 0, i32 1) monotonic, align 1, !dbg [[DBG52:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD90]], ptr [[ATOMIC_TEMP91]], align 1, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_LOAD92:%.*]] = load i8, ptr [[ATOMIC_TEMP91]], align 1, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_SHL93:%.*]] = shl i8 [[BF_LOAD92]], 7, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_ASHR94:%.*]] = ashr i8 [[BF_SHL93]], 7, !dbg [[DBG52]]
// LIN64-NEXT:    [[BF_CAST95:%.*]] = sext i8 [[BF_ASHR94]] to i32, !dbg [[DBG52]]
// LIN64-NEXT:    [[CONV96:%.*]] = sitofp i32 [[BF_CAST95]] to x86_fp80, !dbg [[DBG52]]
// LIN64-NEXT:    store x86_fp80 [[CONV96]], ptr @ldv, align 16, !dbg [[DBG52]]
// LIN64-NEXT:    [[ATOMIC_LOAD97:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG53:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD97]], ptr [[ATOMIC_TEMP98]], align 8, !dbg [[DBG53]]
// LIN64-NEXT:    [[BF_LOAD99:%.*]] = load i64, ptr [[ATOMIC_TEMP98]], align 8, !dbg [[DBG53]]
// LIN64-NEXT:    [[BF_SHL100:%.*]] = shl i64 [[BF_LOAD99]], 40, !dbg [[DBG53]]
// LIN64-NEXT:    [[BF_ASHR101:%.*]] = ashr i64 [[BF_SHL100]], 57, !dbg [[DBG53]]
// LIN64-NEXT:    [[CONV102:%.*]] = sitofp i64 [[BF_ASHR101]] to x86_fp80, !dbg [[DBG53]]
// LIN64-NEXT:    store x86_fp80 [[CONV102]], ptr @ldv, align 16, !dbg [[DBG53]]
// LIN64-NEXT:    [[ATOMIC_LOAD103:%.*]] = load atomic i8, ptr getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], ptr @bfx4_packed, i32 0, i32 1) acquire, align 1, !dbg [[DBG54:![0-9]+]]
// LIN64-NEXT:    store i8 [[ATOMIC_LOAD103]], ptr [[ATOMIC_TEMP104]], align 1, !dbg [[DBG54]]
// LIN64-NEXT:    [[BF_LOAD105:%.*]] = load i8, ptr [[ATOMIC_TEMP104]], align 1, !dbg [[DBG54]]
// LIN64-NEXT:    [[BF_ASHR106:%.*]] = ashr i8 [[BF_LOAD105]], 1, !dbg [[DBG54]]
// LIN64-NEXT:    [[BF_CAST107:%.*]] = sext i8 [[BF_ASHR106]] to i64, !dbg [[DBG54]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    [[CONV108:%.*]] = sitofp i64 [[BF_CAST107]] to x86_fp80, !dbg [[DBG54]]
// LIN64-NEXT:    store x86_fp80 [[CONV108]], ptr @ldv, align 16, !dbg [[DBG54]]
// LIN64-NEXT:    [[ATOMIC_LOAD109:%.*]] = load atomic i64, ptr @float2x monotonic, align 8, !dbg [[DBG55:![0-9]+]]
// LIN64-NEXT:    store i64 [[ATOMIC_LOAD109]], ptr [[ATOMIC_TEMP110]], align 8, !dbg [[DBG55]]
// LIN64-NEXT:    [[TMP3:%.*]] = load <2 x float>, ptr [[ATOMIC_TEMP110]], align 8, !dbg [[DBG55]]
// LIN64-NEXT:    [[TMP4:%.*]] = extractelement <2 x float> [[TMP3]], i64 0, !dbg [[DBG55]]
// LIN64-NEXT:    [[CONV111:%.*]] = fptoui float [[TMP4]] to i64, !dbg [[DBG55]]
// LIN64-NEXT:    store i64 [[CONV111]], ptr @ulv, align 8, !dbg [[DBG55]]
// LIN64-NEXT:    [[TMP5:%.*]] = call i32 @llvm.read_register.i32(metadata [[META2:![0-9]+]]), !dbg [[DBG56:![0-9]+]]
// LIN64-NEXT:    fence acquire
// LIN64-NEXT:    [[CONV112:%.*]] = sitofp i32 [[TMP5]] to double, !dbg [[DBG56]]
// LIN64-NEXT:    store double [[CONV112]], ptr @dv, align 8, !dbg [[DBG56]]
// LIN64-NEXT:    ret i32 0, !dbg [[DBG57:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@main
// PPC64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG6:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca ppc_fp128, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP13:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP28:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP34:%.*]] = alloca ppc_fp128, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP36:%.*]] = alloca { i32, i32 }, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP50:%.*]] = alloca <4 x i32>, align 16
// PPC64-NEXT:    [[ATOMIC_TEMP54:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP56:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP61:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP66:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP72:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[ATOMIC_TEMP77:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP84:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP91:%.*]] = alloca i32, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP97:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[ATOMIC_TEMP103:%.*]] = alloca i64, align 1
// PPC64-NEXT:    [[ATOMIC_TEMP110:%.*]] = alloca <2 x float>, align 8
// PPC64-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// PPC64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG10]]
// PPC64-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG10]]
// PPC64-NEXT:    store i8 [[FROMBOOL]], ptr @bv, align 1, !dbg [[DBG10]]
// PPC64-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, ptr @cx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD1]], ptr @cv, align 1, !dbg [[DBG11]]
// PPC64-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, ptr @ucx monotonic, align 1, !dbg [[DBG12:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD2]], ptr @ucv, align 1, !dbg [[DBG12]]
// PPC64-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    store i16 [[ATOMIC_LOAD3]], ptr @sv, align 2, !dbg [[DBG13]]
// PPC64-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    store i16 [[ATOMIC_LOAD4]], ptr @usv, align 2, !dbg [[DBG14]]
// PPC64-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD5]], ptr @iv, align 4, !dbg [[DBG15]]
// PPC64-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, ptr @uix monotonic, align 4, !dbg [[DBG16:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD6]], ptr @uiv, align 4, !dbg [[DBG16]]
// PPC64-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD7]], ptr @lv, align 8, !dbg [[DBG17]]
// PPC64-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD8]], ptr @ulv, align 8, !dbg [[DBG18]]
// PPC64-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD9]], ptr @llv, align 8, !dbg [[DBG19]]
// PPC64-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, ptr @ullx monotonic, align 8, !dbg [[DBG20:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD10]], ptr @ullv, align 8, !dbg [[DBG20]]
// PPC64-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG21:![0-9]+]]
// PPC64-NEXT:    store float [[ATOMIC_LOAD11]], ptr @fv, align 4, !dbg [[DBG21]]
// PPC64-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG22:![0-9]+]]
// PPC64-NEXT:    store double [[ATOMIC_LOAD12]], ptr @dv, align 8, !dbg [[DBG22]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @ldx, ptr noundef [[ATOMIC_TEMP]], i32 noundef signext 0), !dbg [[DBG23:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = load ppc_fp128, ptr [[ATOMIC_TEMP]], align 16, !dbg [[DBG23]]
// PPC64-NEXT:    store ppc_fp128 [[TMP0]], ptr @ldv, align 16, !dbg [[DBG23]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP13]], i32 noundef signext 0), !dbg [[DBG24:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_REALP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP13]], i32 0, i32 0, !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP13_REALP]], align 4, !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP13]], i32 0, i32 1, !dbg [[DBG24]]
// PPC64-NEXT:    [[ATOMIC_TEMP13_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP13_IMAGP]], align 4, !dbg [[DBG24]]
// PPC64-NEXT:    store i32 [[ATOMIC_TEMP13_REAL]], ptr @civ, align 4, !dbg [[DBG24]]
// PPC64-NEXT:    store i32 [[ATOMIC_TEMP13_IMAG]], ptr getelementptr inbounds ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG24]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cfx, ptr noundef [[ATOMIC_TEMP14]], i32 noundef signext 0), !dbg [[DBG25:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, ptr [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG25]]
// PPC64-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, ptr [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG25]]
// PPC64-NEXT:    store float [[ATOMIC_TEMP14_REAL]], ptr @cfv, align 4, !dbg [[DBG25]]
// PPC64-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], ptr getelementptr inbounds ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG25]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @cdx, ptr noundef [[ATOMIC_TEMP15]], i32 noundef signext 5), !dbg [[DBG26:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG26]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, ptr [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG26]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG26]]
// PPC64-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, ptr [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG26]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    store double [[ATOMIC_TEMP15_REAL]], ptr @cdv, align 8, !dbg [[DBG26]]
// PPC64-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], ptr getelementptr inbounds ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG26]]
// PPC64-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG27:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL17:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG27]]
// PPC64-NEXT:    [[FROMBOOL18:%.*]] = zext i1 [[TOBOOL17]] to i8, !dbg [[DBG27]]
// PPC64-NEXT:    store i8 [[FROMBOOL18]], ptr @bv, align 1, !dbg [[DBG27]]
// PPC64-NEXT:    [[ATOMIC_LOAD19:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG28:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL20:%.*]] = trunc i8 [[ATOMIC_LOAD19]] to i1, !dbg [[DBG28]]
// PPC64-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL20]] to i8, !dbg [[DBG28]]
// PPC64-NEXT:    store i8 [[CONV]], ptr @cv, align 1, !dbg [[DBG28]]
// PPC64-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i8, ptr @cx seq_cst, align 1, !dbg [[DBG29:![0-9]+]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD21]], ptr @ucv, align 1, !dbg [[DBG29]]
// PPC64-NEXT:    [[ATOMIC_LOAD22:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// PPC64-NEXT:    [[CONV23:%.*]] = trunc i64 [[ATOMIC_LOAD22]] to i16, !dbg [[DBG30]]
// PPC64-NEXT:    store i16 [[CONV23]], ptr @sv, align 2, !dbg [[DBG30]]
// PPC64-NEXT:    [[ATOMIC_LOAD24:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG31:![0-9]+]]
// PPC64-NEXT:    [[CONV25:%.*]] = trunc i64 [[ATOMIC_LOAD24]] to i16, !dbg [[DBG31]]
// PPC64-NEXT:    store i16 [[CONV25]], ptr @usv, align 2, !dbg [[DBG31]]
// PPC64-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, ptr @uix seq_cst, align 4, !dbg [[DBG32:![0-9]+]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD26]], ptr @iv, align 4, !dbg [[DBG32]]
// PPC64-NEXT:    [[ATOMIC_LOAD27:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG33:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD27]], ptr @uiv, align 4, !dbg [[DBG33]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP28]], i32 noundef signext 0), !dbg [[DBG34:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP28_REALP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP28]], i32 0, i32 0, !dbg [[DBG34]]
// PPC64-NEXT:    [[ATOMIC_TEMP28_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP28_REALP]], align 4, !dbg [[DBG34]]
// PPC64-NEXT:    [[ATOMIC_TEMP28_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP28]], i32 0, i32 1, !dbg [[DBG34]]
// PPC64-NEXT:    [[ATOMIC_TEMP28_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP28_IMAGP]], align 4, !dbg [[DBG34]]
// PPC64-NEXT:    [[CONV29:%.*]] = sext i32 [[ATOMIC_TEMP28_REAL]] to i64, !dbg [[DBG34]]
// PPC64-NEXT:    store i64 [[CONV29]], ptr @lv, align 8, !dbg [[DBG34]]
// PPC64-NEXT:    [[ATOMIC_LOAD30:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG35:![0-9]+]]
// PPC64-NEXT:    [[CONV31:%.*]] = fptoui float [[ATOMIC_LOAD30]] to i64, !dbg [[DBG35]]
// PPC64-NEXT:    store i64 [[CONV31]], ptr @ulv, align 8, !dbg [[DBG35]]
// PPC64-NEXT:    [[ATOMIC_LOAD32:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG36:![0-9]+]]
// PPC64-NEXT:    [[CONV33:%.*]] = fptosi double [[ATOMIC_LOAD32]] to i64, !dbg [[DBG36]]
// PPC64-NEXT:    store i64 [[CONV33]], ptr @llv, align 8, !dbg [[DBG36]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @ldx, ptr noundef [[ATOMIC_TEMP34]], i32 noundef signext 0), !dbg [[DBG37:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load ppc_fp128, ptr [[ATOMIC_TEMP34]], align 16, !dbg [[DBG37]]
// PPC64-NEXT:    [[CONV35:%.*]] = fptoui ppc_fp128 [[TMP1]] to i64, !dbg [[DBG37]]
// PPC64-NEXT:    store i64 [[CONV35]], ptr @ullv, align 8, !dbg [[DBG37]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP36]], i32 noundef signext 0), !dbg [[DBG38:![0-9]+]]
// PPC64-NEXT:    [[ATOMIC_TEMP36_REALP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP36]], i32 0, i32 0, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_TEMP36_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP36_REALP]], align 4, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_TEMP36_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP36]], i32 0, i32 1, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_TEMP36_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP36_IMAGP]], align 4, !dbg [[DBG38]]
// PPC64-NEXT:    [[CONV37:%.*]] = sitofp i32 [[ATOMIC_TEMP36_REAL]] to float, !dbg [[DBG38]]
// PPC64-NEXT:    store float [[CONV37]], ptr @fv, align 4, !dbg [[DBG38]]
// PPC64-NEXT:    [[ATOMIC_LOAD38:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG39:![0-9]+]]
// PPC64-NEXT:    [[CONV39:%.*]] = sitofp i16 [[ATOMIC_LOAD38]] to double, !dbg [[DBG39]]
// PPC64-NEXT:    store double [[CONV39]], ptr @dv, align 8, !dbg [[DBG39]]
// PPC64-NEXT:    [[ATOMIC_LOAD40:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL41:%.*]] = trunc i8 [[ATOMIC_LOAD40]] to i1, !dbg [[DBG40]]
// PPC64-NEXT:    [[CONV42:%.*]] = uitofp i1 [[TOBOOL41]] to ppc_fp128, !dbg [[DBG40]]
// PPC64-NEXT:    store ppc_fp128 [[CONV42]], ptr @ldv, align 16, !dbg [[DBG40]]
// PPC64-NEXT:    [[ATOMIC_LOAD43:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG41:![0-9]+]]
// PPC64-NEXT:    [[TOBOOL44:%.*]] = trunc i8 [[ATOMIC_LOAD43]] to i1, !dbg [[DBG41]]
// PPC64-NEXT:    [[CONV45:%.*]] = zext i1 [[TOBOOL44]] to i32, !dbg [[DBG41]]
// PPC64-NEXT:    store i32 [[CONV45]], ptr @civ, align 4, !dbg [[DBG41]]
// PPC64-NEXT:    store i32 0, ptr getelementptr inbounds ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG41]]
// PPC64-NEXT:    [[ATOMIC_LOAD46:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG42:![0-9]+]]
// PPC64-NEXT:    [[CONV47:%.*]] = uitofp i16 [[ATOMIC_LOAD46]] to float, !dbg [[DBG42]]
// PPC64-NEXT:    store float [[CONV47]], ptr @cfv, align 4, !dbg [[DBG42]]
// PPC64-NEXT:    store float 0.000000e+00, ptr getelementptr inbounds ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG42]]
// PPC64-NEXT:    [[ATOMIC_LOAD48:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG43:![0-9]+]]
// PPC64-NEXT:    [[CONV49:%.*]] = sitofp i64 [[ATOMIC_LOAD48]] to double, !dbg [[DBG43]]
// PPC64-NEXT:    store double [[CONV49]], ptr @cdv, align 8, !dbg [[DBG43]]
// PPC64-NEXT:    store double 0.000000e+00, ptr getelementptr inbounds ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG43]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @int4x, ptr noundef [[ATOMIC_TEMP50]], i32 noundef signext 0), !dbg [[DBG44:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ATOMIC_TEMP50]], align 16, !dbg [[DBG44]]
// PPC64-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP2]], i32 0, !dbg [[DBG44]]
// PPC64-NEXT:    [[TOBOOL51:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG44]]
// PPC64-NEXT:    [[FROMBOOL52:%.*]] = zext i1 [[TOBOOL51]] to i8, !dbg [[DBG44]]
// PPC64-NEXT:    store i8 [[FROMBOOL52]], ptr @bv, align 1, !dbg [[DBG44]]
// PPC64-NEXT:    [[ATOMIC_LOAD53:%.*]] = load atomic i32, ptr @bfx monotonic, align 4, !dbg [[DBG45:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD53]], ptr [[ATOMIC_TEMP54]], align 4, !dbg [[DBG45]]
// PPC64-NEXT:    [[BF_LOAD:%.*]] = load i32, ptr [[ATOMIC_TEMP54]], align 4, !dbg [[DBG45]]
// PPC64-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_LOAD]], 1, !dbg [[DBG45]]
// PPC64-NEXT:    [[CONV55:%.*]] = sitofp i32 [[BF_ASHR]] to ppc_fp128, !dbg [[DBG45]]
// PPC64-NEXT:    store ppc_fp128 [[CONV55]], ptr @ldv, align 16, !dbg [[DBG45]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 4, ptr noundef @bfx_packed, ptr noundef [[ATOMIC_TEMP56]], i32 noundef signext 0), !dbg [[DBG46:![0-9]+]]
// PPC64-NEXT:    [[BF_LOAD57:%.*]] = load i32, ptr [[ATOMIC_TEMP56]], align 1, !dbg [[DBG46]]
// PPC64-NEXT:    [[BF_ASHR58:%.*]] = ashr i32 [[BF_LOAD57]], 1, !dbg [[DBG46]]
// PPC64-NEXT:    [[CONV59:%.*]] = sitofp i32 [[BF_ASHR58]] to ppc_fp128, !dbg [[DBG46]]
// PPC64-NEXT:    store ppc_fp128 [[CONV59]], ptr @ldv, align 16, !dbg [[DBG46]]
// PPC64-NEXT:    [[ATOMIC_LOAD60:%.*]] = load atomic i32, ptr @bfx2 monotonic, align 4, !dbg [[DBG47:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD60]], ptr [[ATOMIC_TEMP61]], align 4, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_LOAD62:%.*]] = load i32, ptr [[ATOMIC_TEMP61]], align 4, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD62]], 31, !dbg [[DBG47]]
// PPC64-NEXT:    [[BF_ASHR63:%.*]] = ashr i32 [[BF_SHL]], 31, !dbg [[DBG47]]
// PPC64-NEXT:    [[CONV64:%.*]] = sitofp i32 [[BF_ASHR63]] to ppc_fp128, !dbg [[DBG47]]
// PPC64-NEXT:    store ppc_fp128 [[CONV64]], ptr @ldv, align 16, !dbg [[DBG47]]
// PPC64-NEXT:    [[ATOMIC_LOAD65:%.*]] = load atomic i8, ptr @bfx2_packed monotonic, align 1, !dbg [[DBG48:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD65]], ptr [[ATOMIC_TEMP66]], align 1, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_LOAD67:%.*]] = load i8, ptr [[ATOMIC_TEMP66]], align 1, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_SHL68:%.*]] = shl i8 [[BF_LOAD67]], 7, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_ASHR69:%.*]] = ashr i8 [[BF_SHL68]], 7, !dbg [[DBG48]]
// PPC64-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR69]] to i32, !dbg [[DBG48]]
// PPC64-NEXT:    [[CONV70:%.*]] = sitofp i32 [[BF_CAST]] to ppc_fp128, !dbg [[DBG48]]
// PPC64-NEXT:    store ppc_fp128 [[CONV70]], ptr @ldv, align 16, !dbg [[DBG48]]
// PPC64-NEXT:    [[ATOMIC_LOAD71:%.*]] = load atomic i32, ptr @bfx3 monotonic, align 4, !dbg [[DBG49:![0-9]+]]
// PPC64-NEXT:    store i32 [[ATOMIC_LOAD71]], ptr [[ATOMIC_TEMP72]], align 4, !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_LOAD73:%.*]] = load i32, ptr [[ATOMIC_TEMP72]], align 4, !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_SHL74:%.*]] = shl i32 [[BF_LOAD73]], 11, !dbg [[DBG49]]
// PPC64-NEXT:    [[BF_ASHR75:%.*]] = ashr i32 [[BF_SHL74]], 18, !dbg [[DBG49]]
// PPC64-NEXT:    [[CONV76:%.*]] = sitofp i32 [[BF_ASHR75]] to ppc_fp128, !dbg [[DBG49]]
// PPC64-NEXT:    store ppc_fp128 [[CONV76]], ptr @ldv, align 16, !dbg [[DBG49]]
// PPC64-NEXT:    call void @__atomic_load(i64 noundef 3, ptr noundef @bfx3_packed, ptr noundef [[ATOMIC_TEMP77]], i32 noundef signext 0), !dbg [[DBG50:![0-9]+]]
// PPC64-NEXT:    [[BF_LOAD78:%.*]] = load i24, ptr [[ATOMIC_TEMP77]], align 1, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_SHL79:%.*]] = shl i24 [[BF_LOAD78]], 3, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_ASHR80:%.*]] = ashr i24 [[BF_SHL79]], 10, !dbg [[DBG50]]
// PPC64-NEXT:    [[BF_CAST81:%.*]] = sext i24 [[BF_ASHR80]] to i32, !dbg [[DBG50]]
// PPC64-NEXT:    [[CONV82:%.*]] = sitofp i32 [[BF_CAST81]] to ppc_fp128, !dbg [[DBG50]]
// PPC64-NEXT:    store ppc_fp128 [[CONV82]], ptr @ldv, align 16, !dbg [[DBG50]]
// PPC64-NEXT:    [[ATOMIC_LOAD83:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG51:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD83]], ptr [[ATOMIC_TEMP84]], align 8, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_LOAD85:%.*]] = load i64, ptr [[ATOMIC_TEMP84]], align 8, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_SHL86:%.*]] = shl i64 [[BF_LOAD85]], 48, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_ASHR87:%.*]] = ashr i64 [[BF_SHL86]], 63, !dbg [[DBG51]]
// PPC64-NEXT:    [[BF_CAST88:%.*]] = trunc i64 [[BF_ASHR87]] to i32, !dbg [[DBG51]]
// PPC64-NEXT:    [[CONV89:%.*]] = sitofp i32 [[BF_CAST88]] to ppc_fp128, !dbg [[DBG51]]
// PPC64-NEXT:    store ppc_fp128 [[CONV89]], ptr @ldv, align 16, !dbg [[DBG51]]
// PPC64-NEXT:    [[ATOMIC_LOAD90:%.*]] = load atomic i8, ptr getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], ptr @bfx4_packed, i32 0, i32 1) monotonic, align 1, !dbg [[DBG52:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD90]], ptr [[ATOMIC_TEMP91]], align 1, !dbg [[DBG52]]
// PPC64-NEXT:    [[BF_LOAD92:%.*]] = load i8, ptr [[ATOMIC_TEMP91]], align 1, !dbg [[DBG52]]
// PPC64-NEXT:    [[BF_ASHR93:%.*]] = ashr i8 [[BF_LOAD92]], 7, !dbg [[DBG52]]
// PPC64-NEXT:    [[BF_CAST94:%.*]] = sext i8 [[BF_ASHR93]] to i32, !dbg [[DBG52]]
// PPC64-NEXT:    [[CONV95:%.*]] = sitofp i32 [[BF_CAST94]] to ppc_fp128, !dbg [[DBG52]]
// PPC64-NEXT:    store ppc_fp128 [[CONV95]], ptr @ldv, align 16, !dbg [[DBG52]]
// PPC64-NEXT:    [[ATOMIC_LOAD96:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG53:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD96]], ptr [[ATOMIC_TEMP97]], align 8, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_LOAD98:%.*]] = load i64, ptr [[ATOMIC_TEMP97]], align 8, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_SHL99:%.*]] = shl i64 [[BF_LOAD98]], 49, !dbg [[DBG53]]
// PPC64-NEXT:    [[BF_ASHR100:%.*]] = ashr i64 [[BF_SHL99]], 57, !dbg [[DBG53]]
// PPC64-NEXT:    [[CONV101:%.*]] = sitofp i64 [[BF_ASHR100]] to ppc_fp128, !dbg [[DBG53]]
// PPC64-NEXT:    store ppc_fp128 [[CONV101]], ptr @ldv, align 16, !dbg [[DBG53]]
// PPC64-NEXT:    [[ATOMIC_LOAD102:%.*]] = load atomic i8, ptr getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], ptr @bfx4_packed, i32 0, i32 1) acquire, align 1, !dbg [[DBG54:![0-9]+]]
// PPC64-NEXT:    store i8 [[ATOMIC_LOAD102]], ptr [[ATOMIC_TEMP103]], align 1, !dbg [[DBG54]]
// PPC64-NEXT:    [[BF_LOAD104:%.*]] = load i8, ptr [[ATOMIC_TEMP103]], align 1, !dbg [[DBG54]]
// PPC64-NEXT:    [[BF_SHL105:%.*]] = shl i8 [[BF_LOAD104]], 1, !dbg [[DBG54]]
// PPC64-NEXT:    [[BF_ASHR106:%.*]] = ashr i8 [[BF_SHL105]], 1, !dbg [[DBG54]]
// PPC64-NEXT:    [[BF_CAST107:%.*]] = sext i8 [[BF_ASHR106]] to i64, !dbg [[DBG54]]
// PPC64-NEXT:    fence acquire
// PPC64-NEXT:    [[CONV108:%.*]] = sitofp i64 [[BF_CAST107]] to ppc_fp128, !dbg [[DBG54]]
// PPC64-NEXT:    store ppc_fp128 [[CONV108]], ptr @ldv, align 16, !dbg [[DBG54]]
// PPC64-NEXT:    [[ATOMIC_LOAD109:%.*]] = load atomic i64, ptr @float2x monotonic, align 8, !dbg [[DBG55:![0-9]+]]
// PPC64-NEXT:    store i64 [[ATOMIC_LOAD109]], ptr [[ATOMIC_TEMP110]], align 8, !dbg [[DBG55]]
// PPC64-NEXT:    [[TMP3:%.*]] = load <2 x float>, ptr [[ATOMIC_TEMP110]], align 8, !dbg [[DBG55]]
// PPC64-NEXT:    [[TMP4:%.*]] = extractelement <2 x float> [[TMP3]], i64 0, !dbg [[DBG55]]
// PPC64-NEXT:    [[CONV111:%.*]] = fptoui float [[TMP4]] to i64, !dbg [[DBG55]]
// PPC64-NEXT:    store i64 [[CONV111]], ptr @ulv, align 8, !dbg [[DBG55]]
// PPC64-NEXT:    ret i32 0, !dbg [[DBG56:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@main
// AARCH64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP14:%.*]] = alloca { float, float }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP15:%.*]] = alloca { double, double }, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP28:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP36:%.*]] = alloca { i32, i32 }, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP51:%.*]] = alloca <4 x i32>, align 16
// AARCH64-NEXT:    [[ATOMIC_TEMP55:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP57:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP63:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP68:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP73:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[ATOMIC_TEMP78:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP85:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP92:%.*]] = alloca i32, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP99:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[ATOMIC_TEMP105:%.*]] = alloca i64, align 1
// AARCH64-NEXT:    [[ATOMIC_TEMP111:%.*]] = alloca <2 x float>, align 8
// AARCH64-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// AARCH64-NEXT:    [[ATOMIC_LOAD:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG9:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[ATOMIC_LOAD]] to i1, !dbg [[DBG9]]
// AARCH64-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TOBOOL]] to i8, !dbg [[DBG9]]
// AARCH64-NEXT:    store i8 [[FROMBOOL]], ptr @bv, align 1, !dbg [[DBG9]]
// AARCH64-NEXT:    [[ATOMIC_LOAD1:%.*]] = load atomic i8, ptr @cx monotonic, align 1, !dbg [[DBG10:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD1]], ptr @cv, align 1, !dbg [[DBG10]]
// AARCH64-NEXT:    [[ATOMIC_LOAD2:%.*]] = load atomic i8, ptr @ucx monotonic, align 1, !dbg [[DBG11:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD2]], ptr @ucv, align 1, !dbg [[DBG11]]
// AARCH64-NEXT:    [[ATOMIC_LOAD3:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG12:![0-9]+]]
// AARCH64-NEXT:    store i16 [[ATOMIC_LOAD3]], ptr @sv, align 2, !dbg [[DBG12]]
// AARCH64-NEXT:    [[ATOMIC_LOAD4:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    store i16 [[ATOMIC_LOAD4]], ptr @usv, align 2, !dbg [[DBG13]]
// AARCH64-NEXT:    [[ATOMIC_LOAD5:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD5]], ptr @iv, align 4, !dbg [[DBG14]]
// AARCH64-NEXT:    [[ATOMIC_LOAD6:%.*]] = load atomic i32, ptr @uix monotonic, align 4, !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD6]], ptr @uiv, align 4, !dbg [[DBG15]]
// AARCH64-NEXT:    [[ATOMIC_LOAD7:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG16:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD7]], ptr @lv, align 8, !dbg [[DBG16]]
// AARCH64-NEXT:    [[ATOMIC_LOAD8:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG17:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD8]], ptr @ulv, align 8, !dbg [[DBG17]]
// AARCH64-NEXT:    [[ATOMIC_LOAD9:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG18:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD9]], ptr @llv, align 8, !dbg [[DBG18]]
// AARCH64-NEXT:    [[ATOMIC_LOAD10:%.*]] = load atomic i64, ptr @ullx monotonic, align 8, !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD10]], ptr @ullv, align 8, !dbg [[DBG19]]
// AARCH64-NEXT:    [[ATOMIC_LOAD11:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG20:![0-9]+]]
// AARCH64-NEXT:    store float [[ATOMIC_LOAD11]], ptr @fv, align 4, !dbg [[DBG20]]
// AARCH64-NEXT:    [[ATOMIC_LOAD12:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG21:![0-9]+]]
// AARCH64-NEXT:    store double [[ATOMIC_LOAD12]], ptr @dv, align 8, !dbg [[DBG21]]
// AARCH64-NEXT:    [[ATOMIC_LOAD13:%.*]] = load atomic fp128, ptr @ldx monotonic, align 16, !dbg [[DBG22:![0-9]+]]
// AARCH64-NEXT:    store fp128 [[ATOMIC_LOAD13]], ptr @ldv, align 16, !dbg [[DBG22]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP]], i32 noundef 0), !dbg [[DBG23:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_REALP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP]], i32 0, i32 0, !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP_REALP]], align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP]], i32 0, i32 1, !dbg [[DBG23]]
// AARCH64-NEXT:    [[ATOMIC_TEMP_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP_IMAGP]], align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    store i32 [[ATOMIC_TEMP_REAL]], ptr @civ, align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    store i32 [[ATOMIC_TEMP_IMAG]], ptr getelementptr inbounds ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG23]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cfx, ptr noundef [[ATOMIC_TEMP14]], i32 noundef 0), !dbg [[DBG24:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_REALP:%.*]] = getelementptr inbounds { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 0, !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_REAL:%.*]] = load float, ptr [[ATOMIC_TEMP14_REALP]], align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_IMAGP:%.*]] = getelementptr inbounds { float, float }, ptr [[ATOMIC_TEMP14]], i32 0, i32 1, !dbg [[DBG24]]
// AARCH64-NEXT:    [[ATOMIC_TEMP14_IMAG:%.*]] = load float, ptr [[ATOMIC_TEMP14_IMAGP]], align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    store float [[ATOMIC_TEMP14_REAL]], ptr @cfv, align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    store float [[ATOMIC_TEMP14_IMAG]], ptr getelementptr inbounds ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG24]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef @cdx, ptr noundef [[ATOMIC_TEMP15]], i32 noundef 5), !dbg [[DBG25:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_REALP:%.*]] = getelementptr inbounds { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 0, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_REAL:%.*]] = load double, ptr [[ATOMIC_TEMP15_REALP]], align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_IMAGP:%.*]] = getelementptr inbounds { double, double }, ptr [[ATOMIC_TEMP15]], i32 0, i32 1, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_TEMP15_IMAG:%.*]] = load double, ptr [[ATOMIC_TEMP15_IMAGP]], align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    store double [[ATOMIC_TEMP15_REAL]], ptr @cdv, align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    store double [[ATOMIC_TEMP15_IMAG]], ptr getelementptr inbounds ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG25]]
// AARCH64-NEXT:    [[ATOMIC_LOAD16:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG26:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL17:%.*]] = icmp ne i64 [[ATOMIC_LOAD16]], 0, !dbg [[DBG26]]
// AARCH64-NEXT:    [[FROMBOOL18:%.*]] = zext i1 [[TOBOOL17]] to i8, !dbg [[DBG26]]
// AARCH64-NEXT:    store i8 [[FROMBOOL18]], ptr @bv, align 1, !dbg [[DBG26]]
// AARCH64-NEXT:    [[ATOMIC_LOAD19:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG27:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL20:%.*]] = trunc i8 [[ATOMIC_LOAD19]] to i1, !dbg [[DBG27]]
// AARCH64-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL20]] to i8, !dbg [[DBG27]]
// AARCH64-NEXT:    store i8 [[CONV]], ptr @cv, align 1, !dbg [[DBG27]]
// AARCH64-NEXT:    [[ATOMIC_LOAD21:%.*]] = load atomic i8, ptr @cx seq_cst, align 1, !dbg [[DBG28:![0-9]+]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD21]], ptr @ucv, align 1, !dbg [[DBG28]]
// AARCH64-NEXT:    [[ATOMIC_LOAD22:%.*]] = load atomic i64, ptr @ulx monotonic, align 8, !dbg [[DBG29:![0-9]+]]
// AARCH64-NEXT:    [[CONV23:%.*]] = trunc i64 [[ATOMIC_LOAD22]] to i16, !dbg [[DBG29]]
// AARCH64-NEXT:    store i16 [[CONV23]], ptr @sv, align 2, !dbg [[DBG29]]
// AARCH64-NEXT:    [[ATOMIC_LOAD24:%.*]] = load atomic i64, ptr @lx monotonic, align 8, !dbg [[DBG30:![0-9]+]]
// AARCH64-NEXT:    [[CONV25:%.*]] = trunc i64 [[ATOMIC_LOAD24]] to i16, !dbg [[DBG30]]
// AARCH64-NEXT:    store i16 [[CONV25]], ptr @usv, align 2, !dbg [[DBG30]]
// AARCH64-NEXT:    [[ATOMIC_LOAD26:%.*]] = load atomic i32, ptr @uix seq_cst, align 4, !dbg [[DBG31:![0-9]+]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD26]], ptr @iv, align 4, !dbg [[DBG31]]
// AARCH64-NEXT:    [[ATOMIC_LOAD27:%.*]] = load atomic i32, ptr @ix monotonic, align 4, !dbg [[DBG32:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD27]], ptr @uiv, align 4, !dbg [[DBG32]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP28]], i32 noundef 0), !dbg [[DBG33:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP28_REALP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP28]], i32 0, i32 0, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP28_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP28_REALP]], align 4, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP28_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP28]], i32 0, i32 1, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_TEMP28_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP28_IMAGP]], align 4, !dbg [[DBG33]]
// AARCH64-NEXT:    [[CONV29:%.*]] = sext i32 [[ATOMIC_TEMP28_REAL]] to i64, !dbg [[DBG33]]
// AARCH64-NEXT:    store i64 [[CONV29]], ptr @lv, align 8, !dbg [[DBG33]]
// AARCH64-NEXT:    [[ATOMIC_LOAD30:%.*]] = load atomic float, ptr @fx monotonic, align 4, !dbg [[DBG34:![0-9]+]]
// AARCH64-NEXT:    [[CONV31:%.*]] = fptoui float [[ATOMIC_LOAD30]] to i64, !dbg [[DBG34]]
// AARCH64-NEXT:    store i64 [[CONV31]], ptr @ulv, align 8, !dbg [[DBG34]]
// AARCH64-NEXT:    [[ATOMIC_LOAD32:%.*]] = load atomic double, ptr @dx monotonic, align 8, !dbg [[DBG35:![0-9]+]]
// AARCH64-NEXT:    [[CONV33:%.*]] = fptosi double [[ATOMIC_LOAD32]] to i64, !dbg [[DBG35]]
// AARCH64-NEXT:    store i64 [[CONV33]], ptr @llv, align 8, !dbg [[DBG35]]
// AARCH64-NEXT:    [[ATOMIC_LOAD34:%.*]] = load atomic fp128, ptr @ldx monotonic, align 16, !dbg [[DBG36:![0-9]+]]
// AARCH64-NEXT:    [[CONV35:%.*]] = fptoui fp128 [[ATOMIC_LOAD34]] to i64, !dbg [[DBG36]]
// AARCH64-NEXT:    store i64 [[CONV35]], ptr @ullv, align 8, !dbg [[DBG36]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 8, ptr noundef @cix, ptr noundef [[ATOMIC_TEMP36]], i32 noundef 0), !dbg [[DBG37:![0-9]+]]
// AARCH64-NEXT:    [[ATOMIC_TEMP36_REALP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP36]], i32 0, i32 0, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP36_REAL:%.*]] = load i32, ptr [[ATOMIC_TEMP36_REALP]], align 4, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP36_IMAGP:%.*]] = getelementptr inbounds { i32, i32 }, ptr [[ATOMIC_TEMP36]], i32 0, i32 1, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_TEMP36_IMAG:%.*]] = load i32, ptr [[ATOMIC_TEMP36_IMAGP]], align 4, !dbg [[DBG37]]
// AARCH64-NEXT:    [[CONV37:%.*]] = sitofp i32 [[ATOMIC_TEMP36_REAL]] to float, !dbg [[DBG37]]
// AARCH64-NEXT:    store float [[CONV37]], ptr @fv, align 4, !dbg [[DBG37]]
// AARCH64-NEXT:    [[ATOMIC_LOAD38:%.*]] = load atomic i16, ptr @sx monotonic, align 2, !dbg [[DBG38:![0-9]+]]
// AARCH64-NEXT:    [[CONV39:%.*]] = sitofp i16 [[ATOMIC_LOAD38]] to double, !dbg [[DBG38]]
// AARCH64-NEXT:    store double [[CONV39]], ptr @dv, align 8, !dbg [[DBG38]]
// AARCH64-NEXT:    [[ATOMIC_LOAD40:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG39:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL41:%.*]] = trunc i8 [[ATOMIC_LOAD40]] to i1, !dbg [[DBG39]]
// AARCH64-NEXT:    [[CONV42:%.*]] = uitofp i1 [[TOBOOL41]] to fp128, !dbg [[DBG39]]
// AARCH64-NEXT:    store fp128 [[CONV42]], ptr @ldv, align 16, !dbg [[DBG39]]
// AARCH64-NEXT:    [[ATOMIC_LOAD43:%.*]] = load atomic i8, ptr @bx monotonic, align 1, !dbg [[DBG40:![0-9]+]]
// AARCH64-NEXT:    [[TOBOOL44:%.*]] = trunc i8 [[ATOMIC_LOAD43]] to i1, !dbg [[DBG40]]
// AARCH64-NEXT:    [[CONV45:%.*]] = zext i1 [[TOBOOL44]] to i32, !dbg [[DBG40]]
// AARCH64-NEXT:    store i32 [[CONV45]], ptr @civ, align 4, !dbg [[DBG40]]
// AARCH64-NEXT:    store i32 0, ptr getelementptr inbounds ({ i32, i32 }, ptr @civ, i32 0, i32 1), align 4, !dbg [[DBG40]]
// AARCH64-NEXT:    [[ATOMIC_LOAD46:%.*]] = load atomic i16, ptr @usx monotonic, align 2, !dbg [[DBG41:![0-9]+]]
// AARCH64-NEXT:    [[CONV47:%.*]] = uitofp i16 [[ATOMIC_LOAD46]] to float, !dbg [[DBG41]]
// AARCH64-NEXT:    store float [[CONV47]], ptr @cfv, align 4, !dbg [[DBG41]]
// AARCH64-NEXT:    store float 0.000000e+00, ptr getelementptr inbounds ({ float, float }, ptr @cfv, i32 0, i32 1), align 4, !dbg [[DBG41]]
// AARCH64-NEXT:    [[ATOMIC_LOAD48:%.*]] = load atomic i64, ptr @llx monotonic, align 8, !dbg [[DBG42:![0-9]+]]
// AARCH64-NEXT:    [[CONV49:%.*]] = sitofp i64 [[ATOMIC_LOAD48]] to double, !dbg [[DBG42]]
// AARCH64-NEXT:    store double [[CONV49]], ptr @cdv, align 8, !dbg [[DBG42]]
// AARCH64-NEXT:    store double 0.000000e+00, ptr getelementptr inbounds ({ double, double }, ptr @cdv, i32 0, i32 1), align 8, !dbg [[DBG42]]
// AARCH64-NEXT:    [[ATOMIC_LOAD50:%.*]] = load atomic i128, ptr @int4x monotonic, align 16, !dbg [[DBG43:![0-9]+]]
// AARCH64-NEXT:    store i128 [[ATOMIC_LOAD50]], ptr [[ATOMIC_TEMP51]], align 16, !dbg [[DBG43]]
// AARCH64-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[ATOMIC_TEMP51]], align 16, !dbg [[DBG43]]
// AARCH64-NEXT:    [[VECEXT:%.*]] = extractelement <4 x i32> [[TMP0]], i32 0, !dbg [[DBG43]]
// AARCH64-NEXT:    [[TOBOOL52:%.*]] = icmp ne i32 [[VECEXT]], 0, !dbg [[DBG43]]
// AARCH64-NEXT:    [[FROMBOOL53:%.*]] = zext i1 [[TOBOOL52]] to i8, !dbg [[DBG43]]
// AARCH64-NEXT:    store i8 [[FROMBOOL53]], ptr @bv, align 1, !dbg [[DBG43]]
// AARCH64-NEXT:    [[ATOMIC_LOAD54:%.*]] = load atomic i32, ptr getelementptr (i8, ptr @bfx, i64 4) monotonic, align 4, !dbg [[DBG44:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD54]], ptr [[ATOMIC_TEMP55]], align 4, !dbg [[DBG44]]
// AARCH64-NEXT:    [[BF_LOAD:%.*]] = load i32, ptr [[ATOMIC_TEMP55]], align 4, !dbg [[DBG44]]
// AARCH64-NEXT:    [[BF_SHL:%.*]] = shl i32 [[BF_LOAD]], 1, !dbg [[DBG44]]
// AARCH64-NEXT:    [[BF_ASHR:%.*]] = ashr i32 [[BF_SHL]], 1, !dbg [[DBG44]]
// AARCH64-NEXT:    [[CONV56:%.*]] = sitofp i32 [[BF_ASHR]] to fp128, !dbg [[DBG44]]
// AARCH64-NEXT:    store fp128 [[CONV56]], ptr @ldv, align 16, !dbg [[DBG44]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 4, ptr noundef getelementptr (i8, ptr @bfx_packed, i64 4), ptr noundef [[ATOMIC_TEMP57]], i32 noundef 0), !dbg [[DBG45:![0-9]+]]
// AARCH64-NEXT:    [[BF_LOAD58:%.*]] = load i32, ptr [[ATOMIC_TEMP57]], align 1, !dbg [[DBG45]]
// AARCH64-NEXT:    [[BF_SHL59:%.*]] = shl i32 [[BF_LOAD58]], 1, !dbg [[DBG45]]
// AARCH64-NEXT:    [[BF_ASHR60:%.*]] = ashr i32 [[BF_SHL59]], 1, !dbg [[DBG45]]
// AARCH64-NEXT:    [[CONV61:%.*]] = sitofp i32 [[BF_ASHR60]] to fp128, !dbg [[DBG45]]
// AARCH64-NEXT:    store fp128 [[CONV61]], ptr @ldv, align 16, !dbg [[DBG45]]
// AARCH64-NEXT:    [[ATOMIC_LOAD62:%.*]] = load atomic i32, ptr @bfx2 monotonic, align 4, !dbg [[DBG46:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD62]], ptr [[ATOMIC_TEMP63]], align 4, !dbg [[DBG46]]
// AARCH64-NEXT:    [[BF_LOAD64:%.*]] = load i32, ptr [[ATOMIC_TEMP63]], align 4, !dbg [[DBG46]]
// AARCH64-NEXT:    [[BF_ASHR65:%.*]] = ashr i32 [[BF_LOAD64]], 31, !dbg [[DBG46]]
// AARCH64-NEXT:    [[CONV66:%.*]] = sitofp i32 [[BF_ASHR65]] to fp128, !dbg [[DBG46]]
// AARCH64-NEXT:    store fp128 [[CONV66]], ptr @ldv, align 16, !dbg [[DBG46]]
// AARCH64-NEXT:    [[ATOMIC_LOAD67:%.*]] = load atomic i8, ptr getelementptr (i8, ptr @bfx2_packed, i64 3) monotonic, align 1, !dbg [[DBG47:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD67]], ptr [[ATOMIC_TEMP68]], align 1, !dbg [[DBG47]]
// AARCH64-NEXT:    [[BF_LOAD69:%.*]] = load i8, ptr [[ATOMIC_TEMP68]], align 1, !dbg [[DBG47]]
// AARCH64-NEXT:    [[BF_ASHR70:%.*]] = ashr i8 [[BF_LOAD69]], 7, !dbg [[DBG47]]
// AARCH64-NEXT:    [[BF_CAST:%.*]] = sext i8 [[BF_ASHR70]] to i32, !dbg [[DBG47]]
// AARCH64-NEXT:    [[CONV71:%.*]] = sitofp i32 [[BF_CAST]] to fp128, !dbg [[DBG47]]
// AARCH64-NEXT:    store fp128 [[CONV71]], ptr @ldv, align 16, !dbg [[DBG47]]
// AARCH64-NEXT:    [[ATOMIC_LOAD72:%.*]] = load atomic i32, ptr @bfx3 monotonic, align 4, !dbg [[DBG48:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ATOMIC_LOAD72]], ptr [[ATOMIC_TEMP73]], align 4, !dbg [[DBG48]]
// AARCH64-NEXT:    [[BF_LOAD74:%.*]] = load i32, ptr [[ATOMIC_TEMP73]], align 4, !dbg [[DBG48]]
// AARCH64-NEXT:    [[BF_SHL75:%.*]] = shl i32 [[BF_LOAD74]], 7, !dbg [[DBG48]]
// AARCH64-NEXT:    [[BF_ASHR76:%.*]] = ashr i32 [[BF_SHL75]], 18, !dbg [[DBG48]]
// AARCH64-NEXT:    [[CONV77:%.*]] = sitofp i32 [[BF_ASHR76]] to fp128, !dbg [[DBG48]]
// AARCH64-NEXT:    store fp128 [[CONV77]], ptr @ldv, align 16, !dbg [[DBG48]]
// AARCH64-NEXT:    call void @__atomic_load(i64 noundef 3, ptr noundef getelementptr (i8, ptr @bfx3_packed, i64 1), ptr noundef [[ATOMIC_TEMP78]], i32 noundef 0), !dbg [[DBG49:![0-9]+]]
// AARCH64-NEXT:    [[BF_LOAD79:%.*]] = load i24, ptr [[ATOMIC_TEMP78]], align 1, !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_SHL80:%.*]] = shl i24 [[BF_LOAD79]], 7, !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_ASHR81:%.*]] = ashr i24 [[BF_SHL80]], 10, !dbg [[DBG49]]
// AARCH64-NEXT:    [[BF_CAST82:%.*]] = sext i24 [[BF_ASHR81]] to i32, !dbg [[DBG49]]
// AARCH64-NEXT:    [[CONV83:%.*]] = sitofp i32 [[BF_CAST82]] to fp128, !dbg [[DBG49]]
// AARCH64-NEXT:    store fp128 [[CONV83]], ptr @ldv, align 16, !dbg [[DBG49]]
// AARCH64-NEXT:    [[ATOMIC_LOAD84:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG50:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD84]], ptr [[ATOMIC_TEMP85]], align 8, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_LOAD86:%.*]] = load i64, ptr [[ATOMIC_TEMP85]], align 8, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_SHL87:%.*]] = shl i64 [[BF_LOAD86]], 47, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_ASHR88:%.*]] = ashr i64 [[BF_SHL87]], 63, !dbg [[DBG50]]
// AARCH64-NEXT:    [[BF_CAST89:%.*]] = trunc i64 [[BF_ASHR88]] to i32, !dbg [[DBG50]]
// AARCH64-NEXT:    [[CONV90:%.*]] = sitofp i32 [[BF_CAST89]] to fp128, !dbg [[DBG50]]
// AARCH64-NEXT:    store fp128 [[CONV90]], ptr @ldv, align 16, !dbg [[DBG50]]
// AARCH64-NEXT:    [[ATOMIC_LOAD91:%.*]] = load atomic i8, ptr getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED:%.*]], ptr @bfx4_packed, i32 0, i32 1) monotonic, align 1, !dbg [[DBG51:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD91]], ptr [[ATOMIC_TEMP92]], align 1, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_LOAD93:%.*]] = load i8, ptr [[ATOMIC_TEMP92]], align 1, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_SHL94:%.*]] = shl i8 [[BF_LOAD93]], 7, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_ASHR95:%.*]] = ashr i8 [[BF_SHL94]], 7, !dbg [[DBG51]]
// AARCH64-NEXT:    [[BF_CAST96:%.*]] = sext i8 [[BF_ASHR95]] to i32, !dbg [[DBG51]]
// AARCH64-NEXT:    [[CONV97:%.*]] = sitofp i32 [[BF_CAST96]] to fp128, !dbg [[DBG51]]
// AARCH64-NEXT:    store fp128 [[CONV97]], ptr @ldv, align 16, !dbg [[DBG51]]
// AARCH64-NEXT:    [[ATOMIC_LOAD98:%.*]] = load atomic i64, ptr @bfx4 monotonic, align 8, !dbg [[DBG52:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD98]], ptr [[ATOMIC_TEMP99]], align 8, !dbg [[DBG52]]
// AARCH64-NEXT:    [[BF_LOAD100:%.*]] = load i64, ptr [[ATOMIC_TEMP99]], align 8, !dbg [[DBG52]]
// AARCH64-NEXT:    [[BF_SHL101:%.*]] = shl i64 [[BF_LOAD100]], 40, !dbg [[DBG52]]
// AARCH64-NEXT:    [[BF_ASHR102:%.*]] = ashr i64 [[BF_SHL101]], 57, !dbg [[DBG52]]
// AARCH64-NEXT:    [[CONV103:%.*]] = sitofp i64 [[BF_ASHR102]] to fp128, !dbg [[DBG52]]
// AARCH64-NEXT:    store fp128 [[CONV103]], ptr @ldv, align 16, !dbg [[DBG52]]
// AARCH64-NEXT:    [[ATOMIC_LOAD104:%.*]] = load atomic i8, ptr getelementptr inbounds ([[STRUCT_BITFIELDS4_PACKED]], ptr @bfx4_packed, i32 0, i32 1) acquire, align 1, !dbg [[DBG53:![0-9]+]]
// AARCH64-NEXT:    store i8 [[ATOMIC_LOAD104]], ptr [[ATOMIC_TEMP105]], align 1, !dbg [[DBG53]]
// AARCH64-NEXT:    [[BF_LOAD106:%.*]] = load i8, ptr [[ATOMIC_TEMP105]], align 1, !dbg [[DBG53]]
// AARCH64-NEXT:    [[BF_ASHR107:%.*]] = ashr i8 [[BF_LOAD106]], 1, !dbg [[DBG53]]
// AARCH64-NEXT:    [[BF_CAST108:%.*]] = sext i8 [[BF_ASHR107]] to i64, !dbg [[DBG53]]
// AARCH64-NEXT:    fence acquire
// AARCH64-NEXT:    [[CONV109:%.*]] = sitofp i64 [[BF_CAST108]] to fp128, !dbg [[DBG53]]
// AARCH64-NEXT:    store fp128 [[CONV109]], ptr @ldv, align 16, !dbg [[DBG53]]
// AARCH64-NEXT:    [[ATOMIC_LOAD110:%.*]] = load atomic i64, ptr @float2x monotonic, align 8, !dbg [[DBG54:![0-9]+]]
// AARCH64-NEXT:    store i64 [[ATOMIC_LOAD110]], ptr [[ATOMIC_TEMP111]], align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load <2 x float>, ptr [[ATOMIC_TEMP111]], align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    [[TMP2:%.*]] = extractelement <2 x float> [[TMP1]], i64 0, !dbg [[DBG54]]
// AARCH64-NEXT:    [[CONV112:%.*]] = fptoui float [[TMP2]] to i64, !dbg [[DBG54]]
// AARCH64-NEXT:    store i64 [[CONV112]], ptr @ulv, align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    ret i32 0, !dbg [[DBG55:![0-9]+]]
//
