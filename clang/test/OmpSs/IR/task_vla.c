// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics

// LIN64-LABEL: @foo(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[I:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[P_ARRAY:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store i32 [[N:%.*]], ptr [[N_ADDR]], align 4
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG9:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG10:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG11:![0-9]+]]
// LIN64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 1, !dbg [[DBG12:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = zext i32 [[ADD]] to i64, !dbg [[DBG10]]
// LIN64-NEXT:    [[TMP4:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG10]]
// LIN64-NEXT:    store ptr [[TMP4]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG10]]
// LIN64-NEXT:    [[TMP5:%.*]] = mul nuw i64 [[TMP1]], 7, !dbg [[DBG10]]
// LIN64-NEXT:    [[TMP6:%.*]] = mul nuw i64 [[TMP5]], [[TMP3]], !dbg [[DBG10]]
// LIN64-NEXT:    [[VLA:%.*]] = alloca [7 x i32], i64 [[TMP6]], align 16, !dbg [[DBG10]]
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG10]]
// LIN64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG10]]
// LIN64-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 7, i64 [[TMP3]], i64 7), "QUAL.OSS.FIRSTPRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 7, i64 [[TMP3]], i64 7), "QUAL.OSS.DEP.IN"(ptr [[VLA]], [14 x i8] c"array[i][i+1]\00", ptr @compute_dep, ptr [[VLA]], ptr [[I]], i64 [[TMP1]], i64 7, i64 [[TMP3]]), "QUAL.OSS.DEP.IN"(ptr [[VLA]], [7 x i8] c"*array\00", ptr @compute_dep.1, ptr [[VLA]], i64 [[TMP1]], i64 7, i64 [[TMP3]]) ], !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]), !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP8]], 1, !dbg [[DBG16:![0-9]+]]
// LIN64-NEXT:    [[TMP9:%.*]] = zext i32 [[ADD1]] to i64, !dbg [[DBG17:![0-9]+]]
// LIN64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG18:![0-9]+]]
// LIN64-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP10]], 1, !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    [[TMP11:%.*]] = zext i32 [[ADD2]] to i64, !dbg [[DBG17]]
// LIN64-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ARRAY]], ptr undef), "QUAL.OSS.CAPTURED"(i64 [[TMP9]], i64 7, i64 [[TMP11]], i64 7), "QUAL.OSS.DEP.IN"(ptr [[P_ARRAY]], [11 x i8] c"p_array[0]\00", ptr @compute_dep.2, ptr [[P_ARRAY]], i64 [[TMP9]], i64 7, i64 [[TMP11]]) ], !dbg [[DBG20:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]), !dbg [[DBG21:![0-9]+]]
// LIN64-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG22:![0-9]+]]
// LIN64-NEXT:    call void @llvm.stackrestore(ptr [[TMP13]]), !dbg [[DBG22]]
// LIN64-NEXT:    ret void, !dbg [[DBG22]]
//
// PPC64-LABEL: @foo(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[I:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[P_ARRAY:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store i32 [[N:%.*]], ptr [[N_ADDR]], align 4
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG9:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG10:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG11:![0-9]+]]
// PPC64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 1, !dbg [[DBG12:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = zext i32 [[ADD]] to i64, !dbg [[DBG10]]
// PPC64-NEXT:    [[TMP4:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG10]]
// PPC64-NEXT:    store ptr [[TMP4]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG10]]
// PPC64-NEXT:    [[TMP5:%.*]] = mul nuw i64 [[TMP1]], 7, !dbg [[DBG10]]
// PPC64-NEXT:    [[TMP6:%.*]] = mul nuw i64 [[TMP5]], [[TMP3]], !dbg [[DBG10]]
// PPC64-NEXT:    [[VLA:%.*]] = alloca [7 x i32], i64 [[TMP6]], align 4, !dbg [[DBG10]]
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG10]]
// PPC64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG10]]
// PPC64-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 7, i64 [[TMP3]], i64 7), "QUAL.OSS.FIRSTPRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 7, i64 [[TMP3]], i64 7), "QUAL.OSS.DEP.IN"(ptr [[VLA]], [14 x i8] c"array[i][i+1]\00", ptr @compute_dep, ptr [[VLA]], ptr [[I]], i64 [[TMP1]], i64 7, i64 [[TMP3]]), "QUAL.OSS.DEP.IN"(ptr [[VLA]], [7 x i8] c"*array\00", ptr @compute_dep.1, ptr [[VLA]], i64 [[TMP1]], i64 7, i64 [[TMP3]]) ], !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]), !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP8]], 1, !dbg [[DBG16:![0-9]+]]
// PPC64-NEXT:    [[TMP9:%.*]] = zext i32 [[ADD1]] to i64, !dbg [[DBG17:![0-9]+]]
// PPC64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG18:![0-9]+]]
// PPC64-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP10]], 1, !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    [[TMP11:%.*]] = zext i32 [[ADD2]] to i64, !dbg [[DBG17]]
// PPC64-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ARRAY]], ptr undef), "QUAL.OSS.CAPTURED"(i64 [[TMP9]], i64 7, i64 [[TMP11]], i64 7), "QUAL.OSS.DEP.IN"(ptr [[P_ARRAY]], [11 x i8] c"p_array[0]\00", ptr @compute_dep.2, ptr [[P_ARRAY]], i64 [[TMP9]], i64 7, i64 [[TMP11]]) ], !dbg [[DBG20:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]), !dbg [[DBG21:![0-9]+]]
// PPC64-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG22:![0-9]+]]
// PPC64-NEXT:    call void @llvm.stackrestore(ptr [[TMP13]]), !dbg [[DBG22]]
// PPC64-NEXT:    ret void, !dbg [[DBG22]]
//
// AARCH64-LABEL: @foo(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[I:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[P_ARRAY:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store i32 [[N:%.*]], ptr [[N_ADDR]], align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG9:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG10:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG11:![0-9]+]]
// AARCH64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 1, !dbg [[DBG12:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = zext i32 [[ADD]] to i64, !dbg [[DBG10]]
// AARCH64-NEXT:    [[TMP4:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG10]]
// AARCH64-NEXT:    store ptr [[TMP4]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG10]]
// AARCH64-NEXT:    [[TMP5:%.*]] = mul nuw i64 [[TMP1]], 7, !dbg [[DBG10]]
// AARCH64-NEXT:    [[TMP6:%.*]] = mul nuw i64 [[TMP5]], [[TMP3]], !dbg [[DBG10]]
// AARCH64-NEXT:    [[VLA:%.*]] = alloca [7 x i32], i64 [[TMP6]], align 4, !dbg [[DBG10]]
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG10]]
// AARCH64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG10]]
// AARCH64-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 7, i64 [[TMP3]], i64 7), "QUAL.OSS.FIRSTPRIVATE"(ptr [[I]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 7, i64 [[TMP3]], i64 7), "QUAL.OSS.DEP.IN"(ptr [[VLA]], [14 x i8] c"array[i][i+1]\00", ptr @compute_dep, ptr [[VLA]], ptr [[I]], i64 [[TMP1]], i64 7, i64 [[TMP3]]), "QUAL.OSS.DEP.IN"(ptr [[VLA]], [7 x i8] c"*array\00", ptr @compute_dep.1, ptr [[VLA]], i64 [[TMP1]], i64 7, i64 [[TMP3]]) ], !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]), !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP8]], 1, !dbg [[DBG16:![0-9]+]]
// AARCH64-NEXT:    [[TMP9:%.*]] = zext i32 [[ADD1]] to i64, !dbg [[DBG17:![0-9]+]]
// AARCH64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG18:![0-9]+]]
// AARCH64-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP10]], 1, !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    [[TMP11:%.*]] = zext i32 [[ADD2]] to i64, !dbg [[DBG17]]
// AARCH64-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ARRAY]], ptr undef), "QUAL.OSS.CAPTURED"(i64 [[TMP9]], i64 7, i64 [[TMP11]], i64 7), "QUAL.OSS.DEP.IN"(ptr [[P_ARRAY]], [11 x i8] c"p_array[0]\00", ptr @compute_dep.2, ptr [[P_ARRAY]], i64 [[TMP9]], i64 7, i64 [[TMP11]]) ], !dbg [[DBG20:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]), !dbg [[DBG21:![0-9]+]]
// AARCH64-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG22:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.stackrestore(ptr [[TMP13]]), !dbg [[DBG22]]
// AARCH64-NEXT:    ret void, !dbg [[DBG22]]
//
void foo(int n) {
    int i;
    int array[n][7][n+1][7];
    #pragma oss task depend(in: array[i][i+1], *array)
    {}
    int (*p_array)[n+1][7][n+1][7];
    #pragma oss task depend(in: p_array[0])
    {}
}

int p;
// LIN64-LABEL: @foo1(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[Y:%.*]] = alloca i32, align 4
// LIN64-NEXT:    store i32 [[X:%.*]], ptr [[X_ADDR]], align 4
// LIN64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @p, i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[X_ADDR]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[Y]], i32 undef) ], !dbg [[DBG36:![0-9]+]]
// LIN64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[__VLA_EXPR2:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG37:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = zext i32 [[TMP1]] to i64, !dbg [[DBG38:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[Y]], align 4, !dbg [[DBG39:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP3]] to i64, !dbg [[DBG38]]
// LIN64-NEXT:    [[TMP5:%.*]] = load i32, ptr @p, align 4, !dbg [[DBG40:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = zext i32 [[TMP5]] to i64, !dbg [[DBG38]]
// LIN64-NEXT:    [[TMP7:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG38]]
// LIN64-NEXT:    store ptr [[TMP7]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG38]]
// LIN64-NEXT:    [[TMP8:%.*]] = mul nuw i64 [[TMP2]], 7, !dbg [[DBG38]]
// LIN64-NEXT:    [[TMP9:%.*]] = mul nuw i64 [[TMP8]], [[TMP4]], !dbg [[DBG38]]
// LIN64-NEXT:    [[TMP10:%.*]] = mul nuw i64 [[TMP9]], [[TMP6]], !dbg [[DBG38]]
// LIN64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP10]], align 16, !dbg [[DBG38]]
// LIN64-NEXT:    store i64 [[TMP2]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG38]]
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG38]]
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[__VLA_EXPR2]], align 8, !dbg [[DBG38]]
// LIN64-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG41:![0-9]+]]
// LIN64-NEXT:    call void @llvm.stackrestore(ptr [[TMP11]]), !dbg [[DBG41]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG41]]
// LIN64-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @p, i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[X_ADDR]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[Y]], i32 undef) ], !dbg [[DBG42:![0-9]+]]
// LIN64-NEXT:    [[Z:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[TMP13:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG43:![0-9]+]]
// LIN64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[Y]], align 4, !dbg [[DBG44:![0-9]+]]
// LIN64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP13]], [[TMP14]], !dbg [[DBG45:![0-9]+]]
// LIN64-NEXT:    [[TMP15:%.*]] = load i32, ptr @p, align 4, !dbg [[DBG46:![0-9]+]]
// LIN64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[ADD]], [[TMP15]], !dbg [[DBG47:![0-9]+]]
// LIN64-NEXT:    store i32 [[ADD1]], ptr [[Z]], align 4, !dbg [[DBG48:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]), !dbg [[DBG49:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG50:![0-9]+]]
//
// PPC64-LABEL: @foo1(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[Y:%.*]] = alloca i32, align 4
// PPC64-NEXT:    store i32 [[X:%.*]], ptr [[X_ADDR]], align 4
// PPC64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @p, i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[X_ADDR]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[Y]], i32 undef) ], !dbg [[DBG36:![0-9]+]]
// PPC64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[__VLA_EXPR2:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG37:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = zext i32 [[TMP1]] to i64, !dbg [[DBG38:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[Y]], align 4, !dbg [[DBG39:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP3]] to i64, !dbg [[DBG38]]
// PPC64-NEXT:    [[TMP5:%.*]] = load i32, ptr @p, align 4, !dbg [[DBG40:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = zext i32 [[TMP5]] to i64, !dbg [[DBG38]]
// PPC64-NEXT:    [[TMP7:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG38]]
// PPC64-NEXT:    store ptr [[TMP7]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG38]]
// PPC64-NEXT:    [[TMP8:%.*]] = mul nuw i64 [[TMP2]], 7, !dbg [[DBG38]]
// PPC64-NEXT:    [[TMP9:%.*]] = mul nuw i64 [[TMP8]], [[TMP4]], !dbg [[DBG38]]
// PPC64-NEXT:    [[TMP10:%.*]] = mul nuw i64 [[TMP9]], [[TMP6]], !dbg [[DBG38]]
// PPC64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP10]], align 4, !dbg [[DBG38]]
// PPC64-NEXT:    store i64 [[TMP2]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG38]]
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG38]]
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[__VLA_EXPR2]], align 8, !dbg [[DBG38]]
// PPC64-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG41:![0-9]+]]
// PPC64-NEXT:    call void @llvm.stackrestore(ptr [[TMP11]]), !dbg [[DBG41]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG41]]
// PPC64-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @p, i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[X_ADDR]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[Y]], i32 undef) ], !dbg [[DBG42:![0-9]+]]
// PPC64-NEXT:    [[Z:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[TMP13:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG43:![0-9]+]]
// PPC64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[Y]], align 4, !dbg [[DBG44:![0-9]+]]
// PPC64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP13]], [[TMP14]], !dbg [[DBG45:![0-9]+]]
// PPC64-NEXT:    [[TMP15:%.*]] = load i32, ptr @p, align 4, !dbg [[DBG46:![0-9]+]]
// PPC64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[ADD]], [[TMP15]], !dbg [[DBG47:![0-9]+]]
// PPC64-NEXT:    store i32 [[ADD1]], ptr [[Z]], align 4, !dbg [[DBG48:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]), !dbg [[DBG49:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG50:![0-9]+]]
//
// AARCH64-LABEL: @foo1(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[Y:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    store i32 [[X:%.*]], ptr [[X_ADDR]], align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @p, i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[X_ADDR]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[Y]], i32 undef) ], !dbg [[DBG36:![0-9]+]]
// AARCH64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[__VLA_EXPR2:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG37:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = zext i32 [[TMP1]] to i64, !dbg [[DBG38:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[Y]], align 4, !dbg [[DBG39:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP3]] to i64, !dbg [[DBG38]]
// AARCH64-NEXT:    [[TMP5:%.*]] = load i32, ptr @p, align 4, !dbg [[DBG40:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = zext i32 [[TMP5]] to i64, !dbg [[DBG38]]
// AARCH64-NEXT:    [[TMP7:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG38]]
// AARCH64-NEXT:    store ptr [[TMP7]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG38]]
// AARCH64-NEXT:    [[TMP8:%.*]] = mul nuw i64 [[TMP2]], 7, !dbg [[DBG38]]
// AARCH64-NEXT:    [[TMP9:%.*]] = mul nuw i64 [[TMP8]], [[TMP4]], !dbg [[DBG38]]
// AARCH64-NEXT:    [[TMP10:%.*]] = mul nuw i64 [[TMP9]], [[TMP6]], !dbg [[DBG38]]
// AARCH64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP10]], align 4, !dbg [[DBG38]]
// AARCH64-NEXT:    store i64 [[TMP2]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG38]]
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG38]]
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[__VLA_EXPR2]], align 8, !dbg [[DBG38]]
// AARCH64-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG41:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.stackrestore(ptr [[TMP11]]), !dbg [[DBG41]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG41]]
// AARCH64-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @p, i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[X_ADDR]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[Y]], i32 undef) ], !dbg [[DBG42:![0-9]+]]
// AARCH64-NEXT:    [[Z:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[TMP13:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG43:![0-9]+]]
// AARCH64-NEXT:    [[TMP14:%.*]] = load i32, ptr [[Y]], align 4, !dbg [[DBG44:![0-9]+]]
// AARCH64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP13]], [[TMP14]], !dbg [[DBG45:![0-9]+]]
// AARCH64-NEXT:    [[TMP15:%.*]] = load i32, ptr @p, align 4, !dbg [[DBG46:![0-9]+]]
// AARCH64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[ADD]], [[TMP15]], !dbg [[DBG47:![0-9]+]]
// AARCH64-NEXT:    store i32 [[ADD1]], ptr [[Z]], align 4, !dbg [[DBG48:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]), !dbg [[DBG49:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG50:![0-9]+]]
//
void foo1(int x) {
    int y;
    #pragma oss task
    {
        int vla[x][7][y][p];
    }
    #pragma oss task
    {
        int z = x + y + p;
    }
}

// LIN64-LABEL: @foo2(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[Y:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store i32 [[X:%.*]], ptr [[X_ADDR]], align 4
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG52:![0-9]+]]
// LIN64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 1, !dbg [[DBG53:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[ADD]] to i64, !dbg [[DBG54:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 4, !dbg [[DBG55:![0-9]+]]
// LIN64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP2]], 1, !dbg [[DBG56:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = zext i32 [[ADD1]] to i64, !dbg [[DBG54]]
// LIN64-NEXT:    [[TMP4:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG54]]
// LIN64-NEXT:    store ptr [[TMP4]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG54]]
// LIN64-NEXT:    [[TMP5:%.*]] = mul nuw i64 [[TMP1]], [[TMP3]], !dbg [[DBG54]]
// LIN64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP5]], align 16, !dbg [[DBG54]]
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG54]]
// LIN64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG54]]
// LIN64-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]]), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]]) ], !dbg [[DBG57:![0-9]+]]
// LIN64-NEXT:    [[TMP7:%.*]] = mul nsw i64 0, [[TMP3]], !dbg [[DBG58:![0-9]+]]
// LIN64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 [[TMP7]], !dbg [[DBG58]]
// LIN64-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds i32, ptr [[ARRAYIDX]], i64 0, !dbg [[DBG58]]
// LIN64-NEXT:    store i32 1, ptr [[ARRAYIDX2]], align 4, !dbg [[DBG59:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]), !dbg [[DBG60:![0-9]+]]
// LIN64-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG61:![0-9]+]]
// LIN64-NEXT:    call void @llvm.stackrestore(ptr [[TMP8]]), !dbg [[DBG61]]
// LIN64-NEXT:    ret void, !dbg [[DBG61]]
//
// PPC64-LABEL: @foo2(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[Y:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store i32 [[X:%.*]], ptr [[X_ADDR]], align 4
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG52:![0-9]+]]
// PPC64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 1, !dbg [[DBG53:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[ADD]] to i64, !dbg [[DBG54:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 4, !dbg [[DBG55:![0-9]+]]
// PPC64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP2]], 1, !dbg [[DBG56:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = zext i32 [[ADD1]] to i64, !dbg [[DBG54]]
// PPC64-NEXT:    [[TMP4:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG54]]
// PPC64-NEXT:    store ptr [[TMP4]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG54]]
// PPC64-NEXT:    [[TMP5:%.*]] = mul nuw i64 [[TMP1]], [[TMP3]], !dbg [[DBG54]]
// PPC64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP5]], align 4, !dbg [[DBG54]]
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG54]]
// PPC64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG54]]
// PPC64-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]]), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]]) ], !dbg [[DBG57:![0-9]+]]
// PPC64-NEXT:    [[TMP7:%.*]] = mul nsw i64 0, [[TMP3]], !dbg [[DBG58:![0-9]+]]
// PPC64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 [[TMP7]], !dbg [[DBG58]]
// PPC64-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds i32, ptr [[ARRAYIDX]], i64 0, !dbg [[DBG58]]
// PPC64-NEXT:    store i32 1, ptr [[ARRAYIDX2]], align 4, !dbg [[DBG59:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]), !dbg [[DBG60:![0-9]+]]
// PPC64-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG61:![0-9]+]]
// PPC64-NEXT:    call void @llvm.stackrestore(ptr [[TMP8]]), !dbg [[DBG61]]
// PPC64-NEXT:    ret void, !dbg [[DBG61]]
//
// AARCH64-LABEL: @foo2(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[Y:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store i32 [[X:%.*]], ptr [[X_ADDR]], align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG52:![0-9]+]]
// AARCH64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 1, !dbg [[DBG53:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[ADD]] to i64, !dbg [[DBG54:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 4, !dbg [[DBG55:![0-9]+]]
// AARCH64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP2]], 1, !dbg [[DBG56:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = zext i32 [[ADD1]] to i64, !dbg [[DBG54]]
// AARCH64-NEXT:    [[TMP4:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG54]]
// AARCH64-NEXT:    store ptr [[TMP4]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    [[TMP5:%.*]] = mul nuw i64 [[TMP1]], [[TMP3]], !dbg [[DBG54]]
// AARCH64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP5]], align 4, !dbg [[DBG54]]
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG54]]
// AARCH64-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]]), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]]) ], !dbg [[DBG57:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = mul nsw i64 0, [[TMP3]], !dbg [[DBG58:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 [[TMP7]], !dbg [[DBG58]]
// AARCH64-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds i32, ptr [[ARRAYIDX]], i64 0, !dbg [[DBG58]]
// AARCH64-NEXT:    store i32 1, ptr [[ARRAYIDX2]], align 4, !dbg [[DBG59:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]), !dbg [[DBG60:![0-9]+]]
// AARCH64-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG61:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.stackrestore(ptr [[TMP8]]), !dbg [[DBG61]]
// AARCH64-NEXT:    ret void, !dbg [[DBG61]]
//
void foo2(int x) {
    int y;
    int array[x + 1][y + 1];
    #pragma oss task
    { array[0][0] = 1; }
}


