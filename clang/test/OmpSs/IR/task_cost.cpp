// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs
// RUN: %clang_cc1 -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics
template<typename T> T foo() { return T(); }

#pragma oss task cost(foo<int>())
void foo1() {}
#pragma oss task cost(n)
void foo2(int n) {}

void bar(int n) {
    int vla[n];
    #pragma oss task cost(foo<int>())
    {}
    #pragma oss task cost(n)
    {}
    #pragma oss task cost(vla[1])
    {}
    foo1();
    foo2(n);
}

// LIN64-LABEL: define {{[^@]+}}@_Z4foo1v
// LIN64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    ret void, !dbg [[DBG9:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@_Z4foo2i
// LIN64-SAME: (i32 noundef [[N:%.*]]) #[[ATTR0]] !dbg [[DBG10:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// LIN64-NEXT:    ret void, !dbg [[DBG11:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@_Z3bari
// LIN64-SAME: (i32 noundef [[N:%.*]]) #[[ATTR0]] !dbg [[DBG12:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// LIN64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = call ptr @llvm.stacksave.p0(), !dbg [[DBG14]]
// LIN64-NEXT:    store ptr [[TMP2]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG14]]
// LIN64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP1]], align 16, !dbg [[DBG14]]
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG14]]
// LIN64-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.COST"(ptr @compute_cost) ], !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]), !dbg [[DBG16:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N_ADDR]], i32 undef), "QUAL.OSS.COST"(ptr @compute_cost.1, ptr [[N_ADDR]]) ], !dbg [[DBG17:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]), !dbg [[DBG18:![0-9]+]]
// LIN64-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]]), "QUAL.OSS.COST"(ptr @compute_cost.2, ptr [[VLA]], i64 [[TMP1]]), "QUAL.OSS.CAPTURED"(i64 [[TMP1]]) ], !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]), !dbg [[DBG20:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.COST"(ptr @compute_cost.3), "QUAL.OSS.DECL.SOURCE"([18 x i8] c"task_cost.cpp:8:9\00") ], !dbg [[DBG21:![0-9]+]]
// LIN64-NEXT:    call void @_Z4foo1v(), !dbg [[DBG21]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]), !dbg [[DBG21]]
// LIN64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG22:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP7]], ptr [[CALL_ARG]], align 4, !dbg [[DBG22]]
// LIN64-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.COST"(ptr @compute_cost.4, ptr [[CALL_ARG]]), "QUAL.OSS.DECL.SOURCE"([19 x i8] c"task_cost.cpp:10:9\00") ], !dbg [[DBG23:![0-9]+]]
// LIN64-NEXT:    [[TMP9:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG22]]
// LIN64-NEXT:    call void @_Z4foo2i(i32 noundef [[TMP9]]), !dbg [[DBG23]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]), !dbg [[DBG23]]
// LIN64-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG24:![0-9]+]]
// LIN64-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP10]]), !dbg [[DBG24]]
// LIN64-NEXT:    ret void, !dbg [[DBG24]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_cost
// LIN64-SAME: () #[[ATTR3:[0-9]+]] !dbg [[DBG25:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooIiET_v(), !dbg [[DBG26:![0-9]+]]
// LIN64-NEXT:    ret i32 [[CALL]], !dbg [[DBG26]]
//
//
// LIN64-LABEL: define {{[^@]+}}@_Z3fooIiET_v
// LIN64-SAME: () #[[ATTR0]] comdat !dbg [[DBG28:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    ret i32 0, !dbg [[DBG29:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_cost.1
// LIN64-SAME: (ptr [[N:%.*]]) #[[ATTR3]] !dbg [[DBG30:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG31:![0-9]+]]
// LIN64-NEXT:    ret i32 [[TMP0]], !dbg [[DBG31]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_cost.2
// LIN64-SAME: (ptr [[VLA:%.*]], i64 [[TMP0:%.*]]) #[[ATTR3]] !dbg [[DBG33:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 1, !dbg [[DBG34:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[ARRAYIDX]], align 4, !dbg [[DBG34]]
// LIN64-NEXT:    ret i32 [[TMP1]], !dbg [[DBG36:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_cost.3
// LIN64-SAME: () #[[ATTR3]] !dbg [[DBG37:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooIiET_v(), !dbg [[DBG38:![0-9]+]]
// LIN64-NEXT:    ret i32 [[CALL]], !dbg [[DBG38]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_cost.4
// LIN64-SAME: (ptr [[N:%.*]]) #[[ATTR3]] !dbg [[DBG40:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG41:![0-9]+]]
// LIN64-NEXT:    ret i32 [[TMP0]], !dbg [[DBG41]]
//
//
// PPC64-LABEL: define {{[^@]+}}@_Z4foo1v
// PPC64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    ret void, !dbg [[DBG9:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@_Z4foo2i
// PPC64-SAME: (i32 noundef signext [[N:%.*]]) #[[ATTR0]] !dbg [[DBG10:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// PPC64-NEXT:    ret void, !dbg [[DBG11:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@_Z3bari
// PPC64-SAME: (i32 noundef signext [[N:%.*]]) #[[ATTR0]] !dbg [[DBG12:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// PPC64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = call ptr @llvm.stacksave.p0(), !dbg [[DBG14]]
// PPC64-NEXT:    store ptr [[TMP2]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG14]]
// PPC64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP1]], align 4, !dbg [[DBG14]]
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG14]]
// PPC64-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.COST"(ptr @compute_cost) ], !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]), !dbg [[DBG16:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N_ADDR]], i32 undef), "QUAL.OSS.COST"(ptr @compute_cost.1, ptr [[N_ADDR]]) ], !dbg [[DBG17:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]), !dbg [[DBG18:![0-9]+]]
// PPC64-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]]), "QUAL.OSS.COST"(ptr @compute_cost.2, ptr [[VLA]], i64 [[TMP1]]), "QUAL.OSS.CAPTURED"(i64 [[TMP1]]) ], !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]), !dbg [[DBG20:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.COST"(ptr @compute_cost.3), "QUAL.OSS.DECL.SOURCE"([18 x i8] c"task_cost.cpp:8:9\00") ], !dbg [[DBG21:![0-9]+]]
// PPC64-NEXT:    call void @_Z4foo1v(), !dbg [[DBG21]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]), !dbg [[DBG21]]
// PPC64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG22:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP7]], ptr [[CALL_ARG]], align 4, !dbg [[DBG22]]
// PPC64-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.COST"(ptr @compute_cost.4, ptr [[CALL_ARG]]), "QUAL.OSS.DECL.SOURCE"([19 x i8] c"task_cost.cpp:10:9\00") ], !dbg [[DBG23:![0-9]+]]
// PPC64-NEXT:    [[TMP9:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG22]]
// PPC64-NEXT:    call void @_Z4foo2i(i32 noundef signext [[TMP9]]), !dbg [[DBG23]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]), !dbg [[DBG23]]
// PPC64-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG24:![0-9]+]]
// PPC64-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP10]]), !dbg [[DBG24]]
// PPC64-NEXT:    ret void, !dbg [[DBG24]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_cost
// PPC64-SAME: () !dbg [[DBG25:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[CALL:%.*]] = call noundef signext i32 @_Z3fooIiET_v(), !dbg [[DBG26:![0-9]+]]
// PPC64-NEXT:    ret i32 [[CALL]], !dbg [[DBG26]]
//
//
// PPC64-LABEL: define {{[^@]+}}@_Z3fooIiET_v
// PPC64-SAME: () #[[ATTR0]] comdat !dbg [[DBG28:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    ret i32 0, !dbg [[DBG29:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_cost.1
// PPC64-SAME: (ptr [[N:%.*]]) !dbg [[DBG30:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG31:![0-9]+]]
// PPC64-NEXT:    ret i32 [[TMP0]], !dbg [[DBG31]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_cost.2
// PPC64-SAME: (ptr [[VLA:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG33:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 1, !dbg [[DBG34:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[ARRAYIDX]], align 4, !dbg [[DBG34]]
// PPC64-NEXT:    ret i32 [[TMP1]], !dbg [[DBG36:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_cost.3
// PPC64-SAME: () !dbg [[DBG37:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[CALL:%.*]] = call noundef signext i32 @_Z3fooIiET_v(), !dbg [[DBG38:![0-9]+]]
// PPC64-NEXT:    ret i32 [[CALL]], !dbg [[DBG38]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_cost.4
// PPC64-SAME: (ptr [[N:%.*]]) !dbg [[DBG40:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG41:![0-9]+]]
// PPC64-NEXT:    ret i32 [[TMP0]], !dbg [[DBG41]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@_Z4foo1v
// AARCH64-SAME: () #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    ret void, !dbg [[DBG9:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@_Z4foo2i
// AARCH64-SAME: (i32 noundef [[N:%.*]]) #[[ATTR0]] !dbg [[DBG10:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// AARCH64-NEXT:    ret void, !dbg [[DBG11:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@_Z3bari
// AARCH64-SAME: (i32 noundef [[N:%.*]]) #[[ATTR0]] !dbg [[DBG12:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64, !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = call ptr @llvm.stacksave.p0(), !dbg [[DBG14]]
// AARCH64-NEXT:    store ptr [[TMP2]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG14]]
// AARCH64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP1]], align 4, !dbg [[DBG14]]
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG14]]
// AARCH64-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.COST"(ptr @compute_cost) ], !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]), !dbg [[DBG16:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N_ADDR]], i32 undef), "QUAL.OSS.COST"(ptr @compute_cost.1, ptr [[N_ADDR]]) ], !dbg [[DBG17:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]), !dbg [[DBG18:![0-9]+]]
// AARCH64-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]]), "QUAL.OSS.COST"(ptr @compute_cost.2, ptr [[VLA]], i64 [[TMP1]]), "QUAL.OSS.CAPTURED"(i64 [[TMP1]]) ], !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]), !dbg [[DBG20:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.COST"(ptr @compute_cost.3), "QUAL.OSS.DECL.SOURCE"([18 x i8] c"task_cost.cpp:8:9\00") ], !dbg [[DBG21:![0-9]+]]
// AARCH64-NEXT:    call void @_Z4foo1v(), !dbg [[DBG21]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]), !dbg [[DBG21]]
// AARCH64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG22:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP7]], ptr [[CALL_ARG]], align 4, !dbg [[DBG22]]
// AARCH64-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.COST"(ptr @compute_cost.4, ptr [[CALL_ARG]]), "QUAL.OSS.DECL.SOURCE"([19 x i8] c"task_cost.cpp:10:9\00") ], !dbg [[DBG23:![0-9]+]]
// AARCH64-NEXT:    [[TMP9:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG22]]
// AARCH64-NEXT:    call void @_Z4foo2i(i32 noundef [[TMP9]]), !dbg [[DBG23]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]), !dbg [[DBG23]]
// AARCH64-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG24:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP10]]), !dbg [[DBG24]]
// AARCH64-NEXT:    ret void, !dbg [[DBG24]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_cost
// AARCH64-SAME: () !dbg [[DBG25:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooIiET_v(), !dbg [[DBG26:![0-9]+]]
// AARCH64-NEXT:    ret i32 [[CALL]], !dbg [[DBG26]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@_Z3fooIiET_v
// AARCH64-SAME: () #[[ATTR0]] comdat !dbg [[DBG28:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    ret i32 0, !dbg [[DBG29:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_cost.1
// AARCH64-SAME: (ptr [[N:%.*]]) !dbg [[DBG30:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG31:![0-9]+]]
// AARCH64-NEXT:    ret i32 [[TMP0]], !dbg [[DBG31]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_cost.2
// AARCH64-SAME: (ptr [[VLA:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG33:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 1, !dbg [[DBG34:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[ARRAYIDX]], align 4, !dbg [[DBG34]]
// AARCH64-NEXT:    ret i32 [[TMP1]], !dbg [[DBG36:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_cost.3
// AARCH64-SAME: () !dbg [[DBG37:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooIiET_v(), !dbg [[DBG38:![0-9]+]]
// AARCH64-NEXT:    ret i32 [[CALL]], !dbg [[DBG38]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_cost.4
// AARCH64-SAME: (ptr [[N:%.*]]) !dbg [[DBG40:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG41:![0-9]+]]
// AARCH64-NEXT:    ret i32 [[TMP0]], !dbg [[DBG41]]
//
