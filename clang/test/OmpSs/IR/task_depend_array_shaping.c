// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs
// RUN: %clang_cc1 -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -S -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics

int array[10][20];
void foo1(int **p, int n) {
    #pragma oss task depend(in : [n + 1]p, [n + 2]array)
    {}
    #pragma oss task depend(in : ([3]p)[2], ([4]array)[3])
    {}
    #pragma oss task depend(in : ([3]p)[2 : n], ([4]array)[3 : n])
    {}
    #pragma oss task depend(in : [3]p[2], [4]array[3])
    {}
}

void foo2() {
    int n, m;
    int vla[10];
    #pragma oss task inout(([n]vla)[m])
    {}
}

#pragma oss task in(pvla[sizeof(*pvla)])
void foo3(int n, int (*pvla)[n - 1]) {
}

void bar() {
    int mat[10][10];
    foo3(10, mat);
}

void foo4(int x, int y, int z) {
    int a, b, c;
    int vla[x + 1][y + 2][z + 3];
    #pragma oss task inout(([a](vla[2]))[b])
    {}
    #pragma oss task inout([c]([a][b]vla))
    {}
    #pragma oss task inout([c]([a][b]vla[sizeof(vla)]))
    {}
}

// LIN64-LABEL: define {{[^@]+}}@foo1
// LIN64-SAME: (ptr noundef [[P:%.*]], i32 noundef [[N:%.*]]) #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// LIN64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// LIN64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N_ADDR]], i32 undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [9 x i8] c"[n + 1]p\00", ptr @compute_dep, ptr [[P_ADDR]], ptr [[N_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [13 x i8] c"[n + 2]array\00", ptr @compute_dep.1, ptr [[N_ADDR]], ptr @array) ], !dbg [[DBG9:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG10:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [10 x i8] c"([3]p)[2]\00", ptr @compute_dep.2, ptr [[P_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [14 x i8] c"([4]array)[3]\00", ptr @compute_dep.3, ptr @array) ], !dbg [[DBG11:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]), !dbg [[DBG12:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N_ADDR]], i32 undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [14 x i8] c"([3]p)[2 : n]\00", ptr @compute_dep.4, ptr [[P_ADDR]], ptr [[N_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [18 x i8] c"([4]array)[3 : n]\00", ptr @compute_dep.5, ptr [[N_ADDR]], ptr @array) ], !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [8 x i8] c"[3]p[2]\00", ptr @compute_dep.6, ptr [[P_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [12 x i8] c"[4]array[3]\00", ptr @compute_dep.7, ptr @array) ], !dbg [[DBG15:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]), !dbg [[DBG16:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG17:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep
// LIN64-SAME: (ptr [[P:%.*]], ptr [[N:%.*]]) #[[ATTR2:[0-9]+]] !dbg [[DBG18:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 8
// LIN64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// LIN64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG19:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG21:![0-9]+]]
// LIN64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP1]], 1, !dbg [[DBG22:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = zext i32 [[ADD]] to i64
// LIN64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 8
// LIN64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 8
// LIN64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP5]], align 8
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 [[TMP3]], ptr [[TMP6]], align 8
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 8, !dbg [[DBG23:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP9]], !dbg [[DBG23]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.1
// LIN64-SAME: (ptr [[N:%.*]], ptr [[ARRAY:%.*]]) #[[ATTR2]] !dbg [[DBG24:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// LIN64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 0, !dbg [[DBG25:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG27:![0-9]+]]
// LIN64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 2, !dbg [[DBG28:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[ADD]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 80, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 80, ptr [[TMP5]], align 8
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[TMP6]], align 8
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8, !dbg [[DBG29:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP9]], !dbg [[DBG29]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.2
// LIN64-SAME: (ptr [[P:%.*]]) #[[ATTR2]] !dbg [[DBG30:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 8
// LIN64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG31:![0-9]+]]
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [3 x ptr], ptr [[TMP0]], i64 0, i64 0, !dbg [[DBG33:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 24, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 24, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 8, !dbg [[DBG31]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP5]], !dbg [[DBG31]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.3
// LIN64-SAME: (ptr [[ARRAY:%.*]]) #[[ATTR2]] !dbg [[DBG34:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_2:%.*]], align 8
// LIN64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 0, !dbg [[DBG35:![0-9]+]]
// LIN64-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [4 x [20 x i32]], ptr [[ARRAYDECAY]], i64 0, i64 0, !dbg [[DBG37:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY1]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 80, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 80, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 3, ptr [[TMP5]], align 8
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 4, ptr [[TMP6]], align 8
// LIN64-NEXT:    [[TMP7:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], align 8, !dbg [[DBG35]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_2]] [[TMP7]], !dbg [[DBG35]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.4
// LIN64-SAME: (ptr [[P:%.*]], ptr [[N:%.*]]) #[[ATTR2]] !dbg [[DBG38:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_3:%.*]], align 8
// LIN64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// LIN64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG39:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = add i64 2, [[TMP1]]
// LIN64-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG41:![0-9]+]]
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [3 x ptr], ptr [[TMP3]], i64 0, i64 0, !dbg [[DBG42:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 8
// LIN64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP5]], align 8
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 24, ptr [[TMP6]], align 8
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 16, ptr [[TMP7]], align 8
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], align 8, !dbg [[DBG41]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_3]] [[TMP9]], !dbg [[DBG41]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.5
// LIN64-SAME: (ptr [[N:%.*]], ptr [[ARRAY:%.*]]) #[[ATTR2]] !dbg [[DBG43:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_4:%.*]], align 8
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// LIN64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG44:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = add i64 3, [[TMP1]]
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 0, !dbg [[DBG46:![0-9]+]]
// LIN64-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [4 x [20 x i32]], ptr [[ARRAYDECAY]], i64 0, i64 0, !dbg [[DBG47:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY1]], ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 80, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP5]], align 8
// LIN64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 80, ptr [[TMP6]], align 8
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 4, ptr [[TMP7]], align 8
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 3, ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP2]], ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], align 8, !dbg [[DBG46]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_4]] [[TMP10]], !dbg [[DBG46]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.6
// LIN64-SAME: (ptr [[P:%.*]]) #[[ATTR2]] !dbg [[DBG48:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_5:%.*]], align 8
// LIN64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG49:![0-9]+]]
// LIN64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds ptr, ptr [[TMP0]], i64 2, !dbg [[DBG49]]
// LIN64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[ARRAYIDX]], align 8, !dbg [[DBG49]]
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP1]], ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 12, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 12, ptr [[TMP5]], align 8
// LIN64-NEXT:    [[TMP6:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], align 8, !dbg [[DBG51:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_5]] [[TMP6]], !dbg [[DBG51]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.7
// LIN64-SAME: (ptr [[ARRAY:%.*]]) #[[ATTR2]] !dbg [[DBG52:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_6:%.*]], align 8
// LIN64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// LIN64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 3, !dbg [[DBG53:![0-9]+]]
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [20 x i32], ptr [[ARRAYIDX]], i64 0, i64 0, !dbg [[DBG53]]
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 16, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], align 8, !dbg [[DBG55:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_6]] [[TMP4]], !dbg [[DBG55]]
//
//
// LIN64-LABEL: define {{[^@]+}}@foo2
// LIN64-SAME: () #[[ATTR0]] !dbg [[DBG56:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[M:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[VLA:%.*]] = alloca [10 x i32], align 16
// LIN64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], [10 x i32] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[M]], i32 undef), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [12 x i8] c"([n]vla)[m]\00", ptr @compute_dep.8, ptr [[VLA]], ptr [[N]], ptr [[M]]) ], !dbg [[DBG57:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG58:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG59:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.8
// LIN64-SAME: (ptr [[VLA:%.*]], ptr [[N:%.*]], ptr [[M:%.*]]) #[[ATTR2]] !dbg [[DBG60:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_7:%.*]], align 8
// LIN64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[M_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// LIN64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// LIN64-NEXT:    store ptr [[M]], ptr [[M_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[M]], align 4, !dbg [[DBG61:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// LIN64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x i32], ptr [[VLA]], i64 0, i64 0, !dbg [[DBG63:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG64:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP3]] to i64
// LIN64-NEXT:    [[TMP5:%.*]] = mul i64 [[TMP4]], 4
// LIN64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// LIN64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// LIN64-NEXT:    [[TMP12:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], align 8, !dbg [[DBG64]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_7]] [[TMP12]], !dbg [[DBG64]]
//
//
// LIN64-LABEL: define {{[^@]+}}@foo3
// LIN64-SAME: (i32 noundef [[N:%.*]], ptr noundef [[PVLA:%.*]]) #[[ATTR0]] !dbg [[DBG65:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[PVLA_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// LIN64-NEXT:    store ptr [[PVLA]], ptr [[PVLA_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG66:![0-9]+]]
// LIN64-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP0]], 1, !dbg [[DBG67:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[SUB]] to i64
// LIN64-NEXT:    ret void, !dbg [[DBG68:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@bar
// LIN64-SAME: () #[[ATTR0]] !dbg [[DBG69:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[MAT:%.*]] = alloca [10 x [10 x i32]], align 16
// LIN64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[CALL_ARG1:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store i32 10, ptr [[CALL_ARG]], align 4, !dbg [[DBG70:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG71:![0-9]+]]
// LIN64-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP0]], 1, !dbg [[DBG72:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[SUB]] to i64, !dbg [[DBG73:![0-9]+]]
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[MAT]], i64 0, i64 0, !dbg [[DBG74:![0-9]+]]
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[CALL_ARG1]], align 8, !dbg [[DBG74]]
// LIN64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG1]], ptr undef), "QUAL.OSS.DEVICE.DEVFUNC"([5 x i8] c"foo3\00"), "QUAL.OSS.DEP.IN"(ptr [[CALL_ARG1]], [20 x i8] c"pvla[sizeof(*pvla)]\00", ptr @compute_dep.9, ptr [[CALL_ARG1]], i64 [[TMP1]]), "QUAL.OSS.CAPTURED"(i64 [[TMP1]]), "QUAL.OSS.DECL.SOURCE"([33 x i8] c"task_depend_array_shaping.c:26:9\00") ], !dbg [[DBG73]]
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG70]]
// LIN64-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[CALL_ARG1]], align 8, !dbg [[DBG74]]
// LIN64-NEXT:    call void @foo3(i32 noundef [[TMP3]], ptr noundef [[TMP4]]), !dbg [[DBG73]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG73]]
// LIN64-NEXT:    ret void, !dbg [[DBG75:![0-9]+]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.9
// LIN64-SAME: (ptr [[PVLA:%.*]], i64 [[TMP0:%.*]]) #[[ATTR2]] !dbg [[DBG76:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_8:%.*]], align 8
// LIN64-NEXT:    [[PVLA_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[PVLA]], ptr [[PVLA_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[PVLA]], align 8, !dbg [[DBG77:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = mul nuw i64 4, [[TMP0]], !dbg [[DBG79:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = add i64 [[TMP2]], 1
// LIN64-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[PVLA]], align 8, !dbg [[DBG80:![0-9]+]]
// LIN64-NEXT:    [[TMP5:%.*]] = mul i64 [[TMP0]], 4
// LIN64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP0]], 4
// LIN64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP4]], ptr [[TMP7]], align 8
// LIN64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[TMP8]], align 8
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 1, ptr [[TMP11]], align 8
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 [[TMP2]], ptr [[TMP12]], align 8
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP3]], ptr [[TMP13]], align 8
// LIN64-NEXT:    [[TMP14:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], align 8, !dbg [[DBG80]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_8]] [[TMP14]], !dbg [[DBG80]]
//
//
// LIN64-LABEL: define {{[^@]+}}@foo4
// LIN64-SAME: (i32 noundef [[X:%.*]], i32 noundef [[Y:%.*]], i32 noundef [[Z:%.*]]) #[[ATTR0]] !dbg [[DBG81:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[Y_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[Z_ADDR:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[A:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[B:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[C:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[__VLA_EXPR2:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store i32 [[X]], ptr [[X_ADDR]], align 4
// LIN64-NEXT:    store i32 [[Y]], ptr [[Y_ADDR]], align 4
// LIN64-NEXT:    store i32 [[Z]], ptr [[Z_ADDR]], align 4
// LIN64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG82:![0-9]+]]
// LIN64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 1, !dbg [[DBG83:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = zext i32 [[ADD]] to i64, !dbg [[DBG84:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y_ADDR]], align 4, !dbg [[DBG85:![0-9]+]]
// LIN64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP2]], 2, !dbg [[DBG86:![0-9]+]]
// LIN64-NEXT:    [[TMP3:%.*]] = zext i32 [[ADD1]] to i64, !dbg [[DBG84]]
// LIN64-NEXT:    [[TMP4:%.*]] = load i32, ptr [[Z_ADDR]], align 4, !dbg [[DBG87:![0-9]+]]
// LIN64-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP4]], 3, !dbg [[DBG88:![0-9]+]]
// LIN64-NEXT:    [[TMP5:%.*]] = zext i32 [[ADD2]] to i64, !dbg [[DBG84]]
// LIN64-NEXT:    [[TMP6:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG84]]
// LIN64-NEXT:    store ptr [[TMP6]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG84]]
// LIN64-NEXT:    [[TMP7:%.*]] = mul nuw i64 [[TMP1]], [[TMP3]], !dbg [[DBG84]]
// LIN64-NEXT:    [[TMP8:%.*]] = mul nuw i64 [[TMP7]], [[TMP5]], !dbg [[DBG84]]
// LIN64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP8]], align 16, !dbg [[DBG84]]
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG84]]
// LIN64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG84]]
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[__VLA_EXPR2]], align 8, !dbg [[DBG84]]
// LIN64-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.FIRSTPRIVATE"(ptr [[A]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[B]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [17 x i8] c"([a](vla[2]))[b]\00", ptr @compute_dep.10, ptr [[VLA]], ptr [[A]], ptr [[B]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]) ], !dbg [[DBG89:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]), !dbg [[DBG90:![0-9]+]]
// LIN64-NEXT:    [[TMP10:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.FIRSTPRIVATE"(ptr [[A]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[B]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[C]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [15 x i8] c"[c]([a][b]vla)\00", ptr @compute_dep.11, ptr [[VLA]], ptr [[A]], ptr [[B]], ptr [[C]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]) ], !dbg [[DBG91:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP10]]), !dbg [[DBG92:![0-9]+]]
// LIN64-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.FIRSTPRIVATE"(ptr [[A]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[B]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[C]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [28 x i8] c"[c]([a][b]vla[sizeof(vla)])\00", ptr @compute_dep.12, ptr [[VLA]], ptr [[A]], ptr [[B]], ptr [[C]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]) ], !dbg [[DBG93:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]), !dbg [[DBG94:![0-9]+]]
// LIN64-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG95:![0-9]+]]
// LIN64-NEXT:    call void @llvm.stackrestore(ptr [[TMP12]]), !dbg [[DBG95]]
// LIN64-NEXT:    ret void, !dbg [[DBG95]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.10
// LIN64-SAME: (ptr [[VLA:%.*]], ptr [[A:%.*]], ptr [[B:%.*]], i64 [[TMP0:%.*]], i64 [[TMP1:%.*]], i64 [[TMP2:%.*]]) #[[ATTR2]] !dbg [[DBG96:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_9:%.*]], align 8
// LIN64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[DOTADDR1:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[DOTADDR2:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// LIN64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// LIN64-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[DOTADDR1]], align 8
// LIN64-NEXT:    store i64 [[TMP2]], ptr [[DOTADDR2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B]], align 4, !dbg [[DBG97:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// LIN64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// LIN64-NEXT:    [[TMP6:%.*]] = mul nuw i64 [[TMP1]], [[TMP2]], !dbg [[DBG99:![0-9]+]]
// LIN64-NEXT:    [[TMP7:%.*]] = mul nsw i64 2, [[TMP6]], !dbg [[DBG99]]
// LIN64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 [[TMP7]], !dbg [[DBG99]]
// LIN64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[A]], align 4, !dbg [[DBG100:![0-9]+]]
// LIN64-NEXT:    [[TMP9:%.*]] = zext i32 [[TMP8]] to i64
// LIN64-NEXT:    [[TMP10:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP11:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYIDX]], ptr [[TMP12]], align 8
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 [[TMP10]], ptr [[TMP13]], align 8
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP14]], align 8
// LIN64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP11]], ptr [[TMP15]], align 8
// LIN64-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 [[TMP9]], ptr [[TMP16]], align 8
// LIN64-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP17]], align 8
// LIN64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP5]], ptr [[TMP18]], align 8
// LIN64-NEXT:    [[TMP19:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], align 8, !dbg [[DBG100]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_9]] [[TMP19]], !dbg [[DBG100]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.11
// LIN64-SAME: (ptr [[VLA:%.*]], ptr [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]], i64 [[TMP0:%.*]], i64 [[TMP1:%.*]], i64 [[TMP2:%.*]]) #[[ATTR2]] !dbg [[DBG101:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_10:%.*]], align 8
// LIN64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[C_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[DOTADDR1:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[DOTADDR2:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// LIN64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// LIN64-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 8
// LIN64-NEXT:    store ptr [[C]], ptr [[C_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[DOTADDR1]], align 8
// LIN64-NEXT:    store i64 [[TMP2]], ptr [[DOTADDR2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[C]], align 4, !dbg [[DBG102:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP3]] to i64
// LIN64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[B]], align 4, !dbg [[DBG104:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = zext i32 [[TMP5]] to i64
// LIN64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP8:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[VLA]], ptr [[TMP9]], align 8
// LIN64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 [[TMP7]], ptr [[TMP10]], align 8
// LIN64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP11]], align 8
// LIN64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP8]], ptr [[TMP12]], align 8
// LIN64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[TMP13]], align 8
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 0, ptr [[TMP14]], align 8
// LIN64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[TMP15]], align 8
// LIN64-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 7
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP16]], align 8
// LIN64-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 8
// LIN64-NEXT:    store i64 0, ptr [[TMP17]], align 8
// LIN64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 9
// LIN64-NEXT:    store i64 [[TMP6]], ptr [[TMP18]], align 8
// LIN64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 10
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP19]], align 8
// LIN64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 11
// LIN64-NEXT:    store i64 0, ptr [[TMP20]], align 8
// LIN64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 12
// LIN64-NEXT:    store i64 [[TMP4]], ptr [[TMP21]], align 8
// LIN64-NEXT:    [[TMP22:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], align 8, !dbg [[DBG104]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_10]] [[TMP22]], !dbg [[DBG104]]
//
//
// LIN64-LABEL: define {{[^@]+}}@compute_dep.12
// LIN64-SAME: (ptr [[VLA:%.*]], ptr [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]], i64 [[TMP0:%.*]], i64 [[TMP1:%.*]], i64 [[TMP2:%.*]]) #[[ATTR2]] !dbg [[DBG105:![0-9]+]] {
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_11:%.*]], align 8
// LIN64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[C_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[DOTADDR1:%.*]] = alloca i64, align 8
// LIN64-NEXT:    [[DOTADDR2:%.*]] = alloca i64, align 8
// LIN64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// LIN64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// LIN64-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 8
// LIN64-NEXT:    store ptr [[C]], ptr [[C_ADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// LIN64-NEXT:    store i64 [[TMP1]], ptr [[DOTADDR1]], align 8
// LIN64-NEXT:    store i64 [[TMP2]], ptr [[DOTADDR2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = mul nuw i64 [[TMP0]], [[TMP1]], !dbg [[DBG106:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = mul nuw i64 [[TMP3]], [[TMP2]], !dbg [[DBG106]]
// LIN64-NEXT:    [[TMP5:%.*]] = mul nuw i64 4, [[TMP4]], !dbg [[DBG106]]
// LIN64-NEXT:    [[TMP6:%.*]] = mul nuw i64 [[TMP1]], [[TMP2]], !dbg [[DBG108:![0-9]+]]
// LIN64-NEXT:    [[TMP7:%.*]] = mul nsw i64 [[TMP5]], [[TMP6]], !dbg [[DBG108]]
// LIN64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 [[TMP7]], !dbg [[DBG108]]
// LIN64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[C]], align 4, !dbg [[DBG109:![0-9]+]]
// LIN64-NEXT:    [[TMP9:%.*]] = zext i32 [[TMP8]] to i64
// LIN64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[B]], align 4, !dbg [[DBG110:![0-9]+]]
// LIN64-NEXT:    [[TMP11:%.*]] = zext i32 [[TMP10]] to i64
// LIN64-NEXT:    [[TMP12:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP13:%.*]] = mul i64 [[TMP2]], 4
// LIN64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYIDX]], ptr [[TMP14]], align 8
// LIN64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 [[TMP12]], ptr [[TMP15]], align 8
// LIN64-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP16]], align 8
// LIN64-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 [[TMP13]], ptr [[TMP17]], align 8
// LIN64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 4
// LIN64-NEXT:    store i64 [[TMP11]], ptr [[TMP18]], align 8
// LIN64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 5
// LIN64-NEXT:    store i64 0, ptr [[TMP19]], align 8
// LIN64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 6
// LIN64-NEXT:    store i64 [[TMP11]], ptr [[TMP20]], align 8
// LIN64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 7
// LIN64-NEXT:    store i64 [[TMP9]], ptr [[TMP21]], align 8
// LIN64-NEXT:    [[TMP22:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 8
// LIN64-NEXT:    store i64 0, ptr [[TMP22]], align 8
// LIN64-NEXT:    [[TMP23:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 9
// LIN64-NEXT:    store i64 [[TMP9]], ptr [[TMP23]], align 8
// LIN64-NEXT:    [[TMP24:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], align 8, !dbg [[DBG110]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_11]] [[TMP24]], !dbg [[DBG110]]
//
//
// PPC64-LABEL: define {{[^@]+}}@foo1
// PPC64-SAME: (ptr noundef [[P:%.*]], i32 noundef signext [[N:%.*]]) #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// PPC64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// PPC64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N_ADDR]], i32 undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [9 x i8] c"[n + 1]p\00", ptr @compute_dep, ptr [[P_ADDR]], ptr [[N_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [13 x i8] c"[n + 2]array\00", ptr @compute_dep.1, ptr [[N_ADDR]], ptr @array) ], !dbg [[DBG9:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG10:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [10 x i8] c"([3]p)[2]\00", ptr @compute_dep.2, ptr [[P_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [14 x i8] c"([4]array)[3]\00", ptr @compute_dep.3, ptr @array) ], !dbg [[DBG11:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]), !dbg [[DBG12:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N_ADDR]], i32 undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [14 x i8] c"([3]p)[2 : n]\00", ptr @compute_dep.4, ptr [[P_ADDR]], ptr [[N_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [18 x i8] c"([4]array)[3 : n]\00", ptr @compute_dep.5, ptr [[N_ADDR]], ptr @array) ], !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [8 x i8] c"[3]p[2]\00", ptr @compute_dep.6, ptr [[P_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [12 x i8] c"[4]array[3]\00", ptr @compute_dep.7, ptr @array) ], !dbg [[DBG15:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]), !dbg [[DBG16:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG17:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep
// PPC64-SAME: (ptr [[P:%.*]], ptr [[N:%.*]]) !dbg [[DBG18:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 8
// PPC64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// PPC64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG19:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG21:![0-9]+]]
// PPC64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP1]], 1, !dbg [[DBG22:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = zext i32 [[ADD]] to i64
// PPC64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 8
// PPC64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 8
// PPC64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP5]], align 8
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 [[TMP3]], ptr [[TMP6]], align 8
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 8, !dbg [[DBG23:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP9]], !dbg [[DBG23]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.1
// PPC64-SAME: (ptr [[N:%.*]], ptr [[ARRAY:%.*]]) !dbg [[DBG24:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// PPC64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 0, !dbg [[DBG25:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG27:![0-9]+]]
// PPC64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 2, !dbg [[DBG28:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[ADD]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 80, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 80, ptr [[TMP5]], align 8
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[TMP6]], align 8
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8, !dbg [[DBG29:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP9]], !dbg [[DBG29]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.2
// PPC64-SAME: (ptr [[P:%.*]]) !dbg [[DBG30:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 8
// PPC64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG31:![0-9]+]]
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [3 x ptr], ptr [[TMP0]], i64 0, i64 0, !dbg [[DBG33:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 24, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 24, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 8, !dbg [[DBG31]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP5]], !dbg [[DBG31]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.3
// PPC64-SAME: (ptr [[ARRAY:%.*]]) !dbg [[DBG34:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_2:%.*]], align 8
// PPC64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 0, !dbg [[DBG35:![0-9]+]]
// PPC64-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [4 x [20 x i32]], ptr [[ARRAYDECAY]], i64 0, i64 0, !dbg [[DBG37:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY1]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 80, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 80, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 3, ptr [[TMP5]], align 8
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 4, ptr [[TMP6]], align 8
// PPC64-NEXT:    [[TMP7:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], align 8, !dbg [[DBG35]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_2]] [[TMP7]], !dbg [[DBG35]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.4
// PPC64-SAME: (ptr [[P:%.*]], ptr [[N:%.*]]) !dbg [[DBG38:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_3:%.*]], align 8
// PPC64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// PPC64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG39:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = add i64 2, [[TMP1]]
// PPC64-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG41:![0-9]+]]
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [3 x ptr], ptr [[TMP3]], i64 0, i64 0, !dbg [[DBG42:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 8
// PPC64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP5]], align 8
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 24, ptr [[TMP6]], align 8
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 16, ptr [[TMP7]], align 8
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], align 8, !dbg [[DBG41]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_3]] [[TMP9]], !dbg [[DBG41]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.5
// PPC64-SAME: (ptr [[N:%.*]], ptr [[ARRAY:%.*]]) !dbg [[DBG43:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_4:%.*]], align 8
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// PPC64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG44:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = add i64 3, [[TMP1]]
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 0, !dbg [[DBG46:![0-9]+]]
// PPC64-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [4 x [20 x i32]], ptr [[ARRAYDECAY]], i64 0, i64 0, !dbg [[DBG47:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY1]], ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 80, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP5]], align 8
// PPC64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 80, ptr [[TMP6]], align 8
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 4, ptr [[TMP7]], align 8
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 3, ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP2]], ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], align 8, !dbg [[DBG46]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_4]] [[TMP10]], !dbg [[DBG46]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.6
// PPC64-SAME: (ptr [[P:%.*]]) !dbg [[DBG48:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_5:%.*]], align 8
// PPC64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG49:![0-9]+]]
// PPC64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds ptr, ptr [[TMP0]], i64 2, !dbg [[DBG49]]
// PPC64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[ARRAYIDX]], align 8, !dbg [[DBG49]]
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP1]], ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 12, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 12, ptr [[TMP5]], align 8
// PPC64-NEXT:    [[TMP6:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], align 8, !dbg [[DBG51:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_5]] [[TMP6]], !dbg [[DBG51]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.7
// PPC64-SAME: (ptr [[ARRAY:%.*]]) !dbg [[DBG52:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_6:%.*]], align 8
// PPC64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// PPC64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 3, !dbg [[DBG53:![0-9]+]]
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [20 x i32], ptr [[ARRAYIDX]], i64 0, i64 0, !dbg [[DBG53]]
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 16, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], align 8, !dbg [[DBG55:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_6]] [[TMP4]], !dbg [[DBG55]]
//
//
// PPC64-LABEL: define {{[^@]+}}@foo2
// PPC64-SAME: () #[[ATTR0]] !dbg [[DBG56:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[M:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[VLA:%.*]] = alloca [10 x i32], align 4
// PPC64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], [10 x i32] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[M]], i32 undef), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [12 x i8] c"([n]vla)[m]\00", ptr @compute_dep.8, ptr [[VLA]], ptr [[N]], ptr [[M]]) ], !dbg [[DBG57:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG58:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG59:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.8
// PPC64-SAME: (ptr [[VLA:%.*]], ptr [[N:%.*]], ptr [[M:%.*]]) !dbg [[DBG60:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_7:%.*]], align 8
// PPC64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[M_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// PPC64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// PPC64-NEXT:    store ptr [[M]], ptr [[M_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[M]], align 4, !dbg [[DBG61:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// PPC64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x i32], ptr [[VLA]], i64 0, i64 0, !dbg [[DBG63:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG64:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP3]] to i64
// PPC64-NEXT:    [[TMP5:%.*]] = mul i64 [[TMP4]], 4
// PPC64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// PPC64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// PPC64-NEXT:    [[TMP12:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], align 8, !dbg [[DBG64]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_7]] [[TMP12]], !dbg [[DBG64]]
//
//
// PPC64-LABEL: define {{[^@]+}}@foo3
// PPC64-SAME: (i32 noundef signext [[N:%.*]], ptr noundef [[PVLA:%.*]]) #[[ATTR0]] !dbg [[DBG65:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[PVLA_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// PPC64-NEXT:    store ptr [[PVLA]], ptr [[PVLA_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG66:![0-9]+]]
// PPC64-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP0]], 1, !dbg [[DBG67:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[SUB]] to i64
// PPC64-NEXT:    ret void, !dbg [[DBG68:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@bar
// PPC64-SAME: () #[[ATTR0]] !dbg [[DBG69:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[MAT:%.*]] = alloca [10 x [10 x i32]], align 4
// PPC64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[CALL_ARG1:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store i32 10, ptr [[CALL_ARG]], align 4, !dbg [[DBG70:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG71:![0-9]+]]
// PPC64-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP0]], 1, !dbg [[DBG72:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[SUB]] to i64, !dbg [[DBG73:![0-9]+]]
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[MAT]], i64 0, i64 0, !dbg [[DBG74:![0-9]+]]
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[CALL_ARG1]], align 8, !dbg [[DBG74]]
// PPC64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG1]], ptr undef), "QUAL.OSS.DEVICE.DEVFUNC"([5 x i8] c"foo3\00"), "QUAL.OSS.DEP.IN"(ptr [[CALL_ARG1]], [20 x i8] c"pvla[sizeof(*pvla)]\00", ptr @compute_dep.9, ptr [[CALL_ARG1]], i64 [[TMP1]]), "QUAL.OSS.CAPTURED"(i64 [[TMP1]]), "QUAL.OSS.DECL.SOURCE"([33 x i8] c"task_depend_array_shaping.c:26:9\00") ], !dbg [[DBG73]]
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG70]]
// PPC64-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[CALL_ARG1]], align 8, !dbg [[DBG74]]
// PPC64-NEXT:    call void @foo3(i32 noundef signext [[TMP3]], ptr noundef [[TMP4]]), !dbg [[DBG73]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG73]]
// PPC64-NEXT:    ret void, !dbg [[DBG75:![0-9]+]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.9
// PPC64-SAME: (ptr [[PVLA:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG76:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_8:%.*]], align 8
// PPC64-NEXT:    [[PVLA_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[PVLA]], ptr [[PVLA_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[PVLA]], align 8, !dbg [[DBG77:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = mul nuw i64 4, [[TMP0]], !dbg [[DBG79:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = add i64 [[TMP2]], 1
// PPC64-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[PVLA]], align 8, !dbg [[DBG80:![0-9]+]]
// PPC64-NEXT:    [[TMP5:%.*]] = mul i64 [[TMP0]], 4
// PPC64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP0]], 4
// PPC64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP4]], ptr [[TMP7]], align 8
// PPC64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[TMP8]], align 8
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 1, ptr [[TMP11]], align 8
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 [[TMP2]], ptr [[TMP12]], align 8
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP3]], ptr [[TMP13]], align 8
// PPC64-NEXT:    [[TMP14:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], align 8, !dbg [[DBG80]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_8]] [[TMP14]], !dbg [[DBG80]]
//
//
// PPC64-LABEL: define {{[^@]+}}@foo4
// PPC64-SAME: (i32 noundef signext [[X:%.*]], i32 noundef signext [[Y:%.*]], i32 noundef signext [[Z:%.*]]) #[[ATTR0]] !dbg [[DBG81:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[Y_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[Z_ADDR:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[A:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[B:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[C:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[__VLA_EXPR2:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store i32 [[X]], ptr [[X_ADDR]], align 4
// PPC64-NEXT:    store i32 [[Y]], ptr [[Y_ADDR]], align 4
// PPC64-NEXT:    store i32 [[Z]], ptr [[Z_ADDR]], align 4
// PPC64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG82:![0-9]+]]
// PPC64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 1, !dbg [[DBG83:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = zext i32 [[ADD]] to i64, !dbg [[DBG84:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y_ADDR]], align 4, !dbg [[DBG85:![0-9]+]]
// PPC64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP2]], 2, !dbg [[DBG86:![0-9]+]]
// PPC64-NEXT:    [[TMP3:%.*]] = zext i32 [[ADD1]] to i64, !dbg [[DBG84]]
// PPC64-NEXT:    [[TMP4:%.*]] = load i32, ptr [[Z_ADDR]], align 4, !dbg [[DBG87:![0-9]+]]
// PPC64-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP4]], 3, !dbg [[DBG88:![0-9]+]]
// PPC64-NEXT:    [[TMP5:%.*]] = zext i32 [[ADD2]] to i64, !dbg [[DBG84]]
// PPC64-NEXT:    [[TMP6:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG84]]
// PPC64-NEXT:    store ptr [[TMP6]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG84]]
// PPC64-NEXT:    [[TMP7:%.*]] = mul nuw i64 [[TMP1]], [[TMP3]], !dbg [[DBG84]]
// PPC64-NEXT:    [[TMP8:%.*]] = mul nuw i64 [[TMP7]], [[TMP5]], !dbg [[DBG84]]
// PPC64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP8]], align 4, !dbg [[DBG84]]
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG84]]
// PPC64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG84]]
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[__VLA_EXPR2]], align 8, !dbg [[DBG84]]
// PPC64-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.FIRSTPRIVATE"(ptr [[A]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[B]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [17 x i8] c"([a](vla[2]))[b]\00", ptr @compute_dep.10, ptr [[VLA]], ptr [[A]], ptr [[B]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]) ], !dbg [[DBG89:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]), !dbg [[DBG90:![0-9]+]]
// PPC64-NEXT:    [[TMP10:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.FIRSTPRIVATE"(ptr [[A]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[B]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[C]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [15 x i8] c"[c]([a][b]vla)\00", ptr @compute_dep.11, ptr [[VLA]], ptr [[A]], ptr [[B]], ptr [[C]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]) ], !dbg [[DBG91:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP10]]), !dbg [[DBG92:![0-9]+]]
// PPC64-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.FIRSTPRIVATE"(ptr [[A]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[B]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[C]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [28 x i8] c"[c]([a][b]vla[sizeof(vla)])\00", ptr @compute_dep.12, ptr [[VLA]], ptr [[A]], ptr [[B]], ptr [[C]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]) ], !dbg [[DBG93:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]), !dbg [[DBG94:![0-9]+]]
// PPC64-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG95:![0-9]+]]
// PPC64-NEXT:    call void @llvm.stackrestore(ptr [[TMP12]]), !dbg [[DBG95]]
// PPC64-NEXT:    ret void, !dbg [[DBG95]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.10
// PPC64-SAME: (ptr [[VLA:%.*]], ptr [[A:%.*]], ptr [[B:%.*]], i64 [[TMP0:%.*]], i64 [[TMP1:%.*]], i64 [[TMP2:%.*]]) !dbg [[DBG96:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_9:%.*]], align 8
// PPC64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[DOTADDR1:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[DOTADDR2:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// PPC64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// PPC64-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[DOTADDR1]], align 8
// PPC64-NEXT:    store i64 [[TMP2]], ptr [[DOTADDR2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B]], align 4, !dbg [[DBG97:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// PPC64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// PPC64-NEXT:    [[TMP6:%.*]] = mul nuw i64 [[TMP1]], [[TMP2]], !dbg [[DBG99:![0-9]+]]
// PPC64-NEXT:    [[TMP7:%.*]] = mul nsw i64 2, [[TMP6]], !dbg [[DBG99]]
// PPC64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 [[TMP7]], !dbg [[DBG99]]
// PPC64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[A]], align 4, !dbg [[DBG100:![0-9]+]]
// PPC64-NEXT:    [[TMP9:%.*]] = zext i32 [[TMP8]] to i64
// PPC64-NEXT:    [[TMP10:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP11:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYIDX]], ptr [[TMP12]], align 8
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 [[TMP10]], ptr [[TMP13]], align 8
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP14]], align 8
// PPC64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP11]], ptr [[TMP15]], align 8
// PPC64-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 [[TMP9]], ptr [[TMP16]], align 8
// PPC64-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP17]], align 8
// PPC64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP5]], ptr [[TMP18]], align 8
// PPC64-NEXT:    [[TMP19:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], align 8, !dbg [[DBG100]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_9]] [[TMP19]], !dbg [[DBG100]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.11
// PPC64-SAME: (ptr [[VLA:%.*]], ptr [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]], i64 [[TMP0:%.*]], i64 [[TMP1:%.*]], i64 [[TMP2:%.*]]) !dbg [[DBG101:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_10:%.*]], align 8
// PPC64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[C_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[DOTADDR1:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[DOTADDR2:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// PPC64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// PPC64-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 8
// PPC64-NEXT:    store ptr [[C]], ptr [[C_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[DOTADDR1]], align 8
// PPC64-NEXT:    store i64 [[TMP2]], ptr [[DOTADDR2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[C]], align 4, !dbg [[DBG102:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP3]] to i64
// PPC64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[B]], align 4, !dbg [[DBG104:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = zext i32 [[TMP5]] to i64
// PPC64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP8:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[VLA]], ptr [[TMP9]], align 8
// PPC64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 [[TMP7]], ptr [[TMP10]], align 8
// PPC64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP11]], align 8
// PPC64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP8]], ptr [[TMP12]], align 8
// PPC64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[TMP13]], align 8
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 0, ptr [[TMP14]], align 8
// PPC64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[TMP15]], align 8
// PPC64-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 7
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP16]], align 8
// PPC64-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 8
// PPC64-NEXT:    store i64 0, ptr [[TMP17]], align 8
// PPC64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 9
// PPC64-NEXT:    store i64 [[TMP6]], ptr [[TMP18]], align 8
// PPC64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 10
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP19]], align 8
// PPC64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 11
// PPC64-NEXT:    store i64 0, ptr [[TMP20]], align 8
// PPC64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 12
// PPC64-NEXT:    store i64 [[TMP4]], ptr [[TMP21]], align 8
// PPC64-NEXT:    [[TMP22:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], align 8, !dbg [[DBG104]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_10]] [[TMP22]], !dbg [[DBG104]]
//
//
// PPC64-LABEL: define {{[^@]+}}@compute_dep.12
// PPC64-SAME: (ptr [[VLA:%.*]], ptr [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]], i64 [[TMP0:%.*]], i64 [[TMP1:%.*]], i64 [[TMP2:%.*]]) !dbg [[DBG105:![0-9]+]] {
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_11:%.*]], align 8
// PPC64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[C_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[DOTADDR1:%.*]] = alloca i64, align 8
// PPC64-NEXT:    [[DOTADDR2:%.*]] = alloca i64, align 8
// PPC64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// PPC64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// PPC64-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 8
// PPC64-NEXT:    store ptr [[C]], ptr [[C_ADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// PPC64-NEXT:    store i64 [[TMP1]], ptr [[DOTADDR1]], align 8
// PPC64-NEXT:    store i64 [[TMP2]], ptr [[DOTADDR2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = mul nuw i64 [[TMP0]], [[TMP1]], !dbg [[DBG106:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = mul nuw i64 [[TMP3]], [[TMP2]], !dbg [[DBG106]]
// PPC64-NEXT:    [[TMP5:%.*]] = mul nuw i64 4, [[TMP4]], !dbg [[DBG106]]
// PPC64-NEXT:    [[TMP6:%.*]] = mul nuw i64 [[TMP1]], [[TMP2]], !dbg [[DBG108:![0-9]+]]
// PPC64-NEXT:    [[TMP7:%.*]] = mul nsw i64 [[TMP5]], [[TMP6]], !dbg [[DBG108]]
// PPC64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 [[TMP7]], !dbg [[DBG108]]
// PPC64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[C]], align 4, !dbg [[DBG109:![0-9]+]]
// PPC64-NEXT:    [[TMP9:%.*]] = zext i32 [[TMP8]] to i64
// PPC64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[B]], align 4, !dbg [[DBG110:![0-9]+]]
// PPC64-NEXT:    [[TMP11:%.*]] = zext i32 [[TMP10]] to i64
// PPC64-NEXT:    [[TMP12:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP13:%.*]] = mul i64 [[TMP2]], 4
// PPC64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYIDX]], ptr [[TMP14]], align 8
// PPC64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 [[TMP12]], ptr [[TMP15]], align 8
// PPC64-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP16]], align 8
// PPC64-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 [[TMP13]], ptr [[TMP17]], align 8
// PPC64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 4
// PPC64-NEXT:    store i64 [[TMP11]], ptr [[TMP18]], align 8
// PPC64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 5
// PPC64-NEXT:    store i64 0, ptr [[TMP19]], align 8
// PPC64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 6
// PPC64-NEXT:    store i64 [[TMP11]], ptr [[TMP20]], align 8
// PPC64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 7
// PPC64-NEXT:    store i64 [[TMP9]], ptr [[TMP21]], align 8
// PPC64-NEXT:    [[TMP22:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 8
// PPC64-NEXT:    store i64 0, ptr [[TMP22]], align 8
// PPC64-NEXT:    [[TMP23:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 9
// PPC64-NEXT:    store i64 [[TMP9]], ptr [[TMP23]], align 8
// PPC64-NEXT:    [[TMP24:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], align 8, !dbg [[DBG110]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_11]] [[TMP24]], !dbg [[DBG110]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@foo1
// AARCH64-SAME: (ptr noundef [[P:%.*]], i32 noundef [[N:%.*]]) #[[ATTR0:[0-9]+]] !dbg [[DBG5:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// AARCH64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N_ADDR]], i32 undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [9 x i8] c"[n + 1]p\00", ptr @compute_dep, ptr [[P_ADDR]], ptr [[N_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [13 x i8] c"[n + 2]array\00", ptr @compute_dep.1, ptr [[N_ADDR]], ptr @array) ], !dbg [[DBG9:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG10:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [10 x i8] c"([3]p)[2]\00", ptr @compute_dep.2, ptr [[P_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [14 x i8] c"([4]array)[3]\00", ptr @compute_dep.3, ptr @array) ], !dbg [[DBG11:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]), !dbg [[DBG12:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N_ADDR]], i32 undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [14 x i8] c"([3]p)[2 : n]\00", ptr @compute_dep.4, ptr [[P_ADDR]], ptr [[N_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [18 x i8] c"([4]array)[3 : n]\00", ptr @compute_dep.5, ptr [[N_ADDR]], ptr @array) ], !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @array, [10 x [20 x i32]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[P_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[P_ADDR]], [8 x i8] c"[3]p[2]\00", ptr @compute_dep.6, ptr [[P_ADDR]]), "QUAL.OSS.DEP.IN"(ptr @array, [12 x i8] c"[4]array[3]\00", ptr @compute_dep.7, ptr @array) ], !dbg [[DBG15:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]), !dbg [[DBG16:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG17:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep
// AARCH64-SAME: (ptr [[P:%.*]], ptr [[N:%.*]]) !dbg [[DBG18:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 8
// AARCH64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG19:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG21:![0-9]+]]
// AARCH64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP1]], 1, !dbg [[DBG22:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = zext i32 [[ADD]] to i64
// AARCH64-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 8
// AARCH64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 8
// AARCH64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP5]], align 8
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 [[TMP3]], ptr [[TMP6]], align 8
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 8, !dbg [[DBG23:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP9]], !dbg [[DBG23]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.1
// AARCH64-SAME: (ptr [[N:%.*]], ptr [[ARRAY:%.*]]) !dbg [[DBG24:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 0, !dbg [[DBG25:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG27:![0-9]+]]
// AARCH64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 2, !dbg [[DBG28:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[ADD]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 80, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 80, ptr [[TMP5]], align 8
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[TMP6]], align 8
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 0, ptr [[TMP7]], align 8
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8, !dbg [[DBG29:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP9]], !dbg [[DBG29]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.2
// AARCH64-SAME: (ptr [[P:%.*]]) !dbg [[DBG30:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 8
// AARCH64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG31:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [3 x ptr], ptr [[TMP0]], i64 0, i64 0, !dbg [[DBG33:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 24, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 24, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 8, !dbg [[DBG31]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP5]], !dbg [[DBG31]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.3
// AARCH64-SAME: (ptr [[ARRAY:%.*]]) !dbg [[DBG34:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_2:%.*]], align 8
// AARCH64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 0, !dbg [[DBG35:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [4 x [20 x i32]], ptr [[ARRAYDECAY]], i64 0, i64 0, !dbg [[DBG37:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY1]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 80, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 80, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 3, ptr [[TMP5]], align 8
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 4, ptr [[TMP6]], align 8
// AARCH64-NEXT:    [[TMP7:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], align 8, !dbg [[DBG35]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_2]] [[TMP7]], !dbg [[DBG35]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.4
// AARCH64-SAME: (ptr [[P:%.*]], ptr [[N:%.*]]) !dbg [[DBG38:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_3:%.*]], align 8
// AARCH64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG39:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = add i64 2, [[TMP1]]
// AARCH64-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG41:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [3 x ptr], ptr [[TMP3]], i64 0, i64 0, !dbg [[DBG42:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = mul i64 [[TMP2]], 8
// AARCH64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP5]], align 8
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 24, ptr [[TMP6]], align 8
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 16, ptr [[TMP7]], align 8
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], align 8, !dbg [[DBG41]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_3]] [[TMP9]], !dbg [[DBG41]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.5
// AARCH64-SAME: (ptr [[N:%.*]], ptr [[ARRAY:%.*]]) !dbg [[DBG43:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_4:%.*]], align 8
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG44:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = add i64 3, [[TMP1]]
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 0, !dbg [[DBG46:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [4 x [20 x i32]], ptr [[ARRAYDECAY]], i64 0, i64 0, !dbg [[DBG47:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY1]], ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 80, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP5]], align 8
// AARCH64-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 80, ptr [[TMP6]], align 8
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 4, ptr [[TMP7]], align 8
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 3, ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP2]], ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], align 8, !dbg [[DBG46]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_4]] [[TMP10]], !dbg [[DBG46]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.6
// AARCH64-SAME: (ptr [[P:%.*]]) !dbg [[DBG48:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_5:%.*]], align 8
// AARCH64-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P]], align 8, !dbg [[DBG49:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds ptr, ptr [[TMP0]], i64 2, !dbg [[DBG49]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[ARRAYIDX]], align 8, !dbg [[DBG49]]
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP1]], ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 12, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 12, ptr [[TMP5]], align 8
// AARCH64-NEXT:    [[TMP6:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], align 8, !dbg [[DBG51:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_5]] [[TMP6]], !dbg [[DBG51]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.7
// AARCH64-SAME: (ptr [[ARRAY:%.*]]) !dbg [[DBG52:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_6:%.*]], align 8
// AARCH64-NEXT:    [[ARRAY_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[ARRAY]], ptr [[ARRAY_ADDR]], align 8
// AARCH64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x [20 x i32]], ptr [[ARRAY]], i64 0, i64 3, !dbg [[DBG53:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [20 x i32], ptr [[ARRAYIDX]], i64 0, i64 0, !dbg [[DBG53]]
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 16, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], align 8, !dbg [[DBG55:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_6]] [[TMP4]], !dbg [[DBG55]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@foo2
// AARCH64-SAME: () #[[ATTR0]] !dbg [[DBG56:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[M:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[VLA:%.*]] = alloca [10 x i32], align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], [10 x i32] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[N]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[M]], i32 undef), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [12 x i8] c"([n]vla)[m]\00", ptr @compute_dep.8, ptr [[VLA]], ptr [[N]], ptr [[M]]) ], !dbg [[DBG57:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG58:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG59:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.8
// AARCH64-SAME: (ptr [[VLA:%.*]], ptr [[N:%.*]], ptr [[M:%.*]]) !dbg [[DBG60:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_7:%.*]], align 8
// AARCH64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[M_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[N]], ptr [[N_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[M]], ptr [[M_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[M]], align 4, !dbg [[DBG61:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = sext i32 [[TMP0]] to i64
// AARCH64-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 1
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x i32], ptr [[VLA]], i64 0, i64 0, !dbg [[DBG63:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[N]], align 4, !dbg [[DBG64:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP3]] to i64
// AARCH64-NEXT:    [[TMP5:%.*]] = mul i64 [[TMP4]], 4
// AARCH64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP1]], 4
// AARCH64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP7]], ptr [[TMP11]], align 8
// AARCH64-NEXT:    [[TMP12:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], align 8, !dbg [[DBG64]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_7]] [[TMP12]], !dbg [[DBG64]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@foo3
// AARCH64-SAME: (i32 noundef [[N:%.*]], ptr noundef [[PVLA:%.*]]) #[[ATTR0]] !dbg [[DBG65:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[PVLA_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// AARCH64-NEXT:    store ptr [[PVLA]], ptr [[PVLA_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4, !dbg [[DBG66:![0-9]+]]
// AARCH64-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP0]], 1, !dbg [[DBG67:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[SUB]] to i64
// AARCH64-NEXT:    ret void, !dbg [[DBG68:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@bar
// AARCH64-SAME: () #[[ATTR0]] !dbg [[DBG69:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[MAT:%.*]] = alloca [10 x [10 x i32]], align 4
// AARCH64-NEXT:    [[CALL_ARG:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[CALL_ARG1:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store i32 10, ptr [[CALL_ARG]], align 4, !dbg [[DBG70:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG71:![0-9]+]]
// AARCH64-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP0]], 1, !dbg [[DBG72:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[SUB]] to i64, !dbg [[DBG73:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[MAT]], i64 0, i64 0, !dbg [[DBG74:![0-9]+]]
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[CALL_ARG1]], align 8, !dbg [[DBG74]]
// AARCH64-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CALL_ARG1]], ptr undef), "QUAL.OSS.DEVICE.DEVFUNC"([5 x i8] c"foo3\00"), "QUAL.OSS.DEP.IN"(ptr [[CALL_ARG1]], [20 x i8] c"pvla[sizeof(*pvla)]\00", ptr @compute_dep.9, ptr [[CALL_ARG1]], i64 [[TMP1]]), "QUAL.OSS.CAPTURED"(i64 [[TMP1]]), "QUAL.OSS.DECL.SOURCE"([33 x i8] c"task_depend_array_shaping.c:26:9\00") ], !dbg [[DBG73]]
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[CALL_ARG]], align 4, !dbg [[DBG70]]
// AARCH64-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[CALL_ARG1]], align 8, !dbg [[DBG74]]
// AARCH64-NEXT:    call void @foo3(i32 noundef [[TMP3]], ptr noundef [[TMP4]]), !dbg [[DBG73]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]), !dbg [[DBG73]]
// AARCH64-NEXT:    ret void, !dbg [[DBG75:![0-9]+]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.9
// AARCH64-SAME: (ptr [[PVLA:%.*]], i64 [[TMP0:%.*]]) !dbg [[DBG76:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_8:%.*]], align 8
// AARCH64-NEXT:    [[PVLA_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[PVLA]], ptr [[PVLA_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[PVLA]], align 8, !dbg [[DBG77:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = mul nuw i64 4, [[TMP0]], !dbg [[DBG79:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = add i64 [[TMP2]], 1
// AARCH64-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[PVLA]], align 8, !dbg [[DBG80:![0-9]+]]
// AARCH64-NEXT:    [[TMP5:%.*]] = mul i64 [[TMP0]], 4
// AARCH64-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP0]], 4
// AARCH64-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP4]], ptr [[TMP7]], align 8
// AARCH64-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[TMP8]], align 8
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP10]], align 8
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 1, ptr [[TMP11]], align 8
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 [[TMP2]], ptr [[TMP12]], align 8
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP3]], ptr [[TMP13]], align 8
// AARCH64-NEXT:    [[TMP14:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], align 8, !dbg [[DBG80]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_8]] [[TMP14]], !dbg [[DBG80]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@foo4
// AARCH64-SAME: (i32 noundef [[X:%.*]], i32 noundef [[Y:%.*]], i32 noundef [[Z:%.*]]) #[[ATTR0]] !dbg [[DBG81:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[Y_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[Z_ADDR:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[A:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[B:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[C:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[__VLA_EXPR1:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[__VLA_EXPR2:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store i32 [[X]], ptr [[X_ADDR]], align 4
// AARCH64-NEXT:    store i32 [[Y]], ptr [[Y_ADDR]], align 4
// AARCH64-NEXT:    store i32 [[Z]], ptr [[Z_ADDR]], align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = load i32, ptr [[X_ADDR]], align 4, !dbg [[DBG82:![0-9]+]]
// AARCH64-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 1, !dbg [[DBG83:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = zext i32 [[ADD]] to i64, !dbg [[DBG84:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y_ADDR]], align 4, !dbg [[DBG85:![0-9]+]]
// AARCH64-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP2]], 2, !dbg [[DBG86:![0-9]+]]
// AARCH64-NEXT:    [[TMP3:%.*]] = zext i32 [[ADD1]] to i64, !dbg [[DBG84]]
// AARCH64-NEXT:    [[TMP4:%.*]] = load i32, ptr [[Z_ADDR]], align 4, !dbg [[DBG87:![0-9]+]]
// AARCH64-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP4]], 3, !dbg [[DBG88:![0-9]+]]
// AARCH64-NEXT:    [[TMP5:%.*]] = zext i32 [[ADD2]] to i64, !dbg [[DBG84]]
// AARCH64-NEXT:    [[TMP6:%.*]] = call ptr @llvm.stacksave(), !dbg [[DBG84]]
// AARCH64-NEXT:    store ptr [[TMP6]], ptr [[SAVED_STACK]], align 8, !dbg [[DBG84]]
// AARCH64-NEXT:    [[TMP7:%.*]] = mul nuw i64 [[TMP1]], [[TMP3]], !dbg [[DBG84]]
// AARCH64-NEXT:    [[TMP8:%.*]] = mul nuw i64 [[TMP7]], [[TMP5]], !dbg [[DBG84]]
// AARCH64-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP8]], align 4, !dbg [[DBG84]]
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8, !dbg [[DBG84]]
// AARCH64-NEXT:    store i64 [[TMP3]], ptr [[__VLA_EXPR1]], align 8, !dbg [[DBG84]]
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[__VLA_EXPR2]], align 8, !dbg [[DBG84]]
// AARCH64-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.FIRSTPRIVATE"(ptr [[A]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[B]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [17 x i8] c"([a](vla[2]))[b]\00", ptr @compute_dep.10, ptr [[VLA]], ptr [[A]], ptr [[B]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]) ], !dbg [[DBG89:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]), !dbg [[DBG90:![0-9]+]]
// AARCH64-NEXT:    [[TMP10:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.FIRSTPRIVATE"(ptr [[A]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[B]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[C]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [15 x i8] c"[c]([a][b]vla)\00", ptr @compute_dep.11, ptr [[VLA]], ptr [[A]], ptr [[B]], ptr [[C]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]) ], !dbg [[DBG91:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP10]]), !dbg [[DBG92:![0-9]+]]
// AARCH64-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[VLA]], i32 undef), "QUAL.OSS.VLA.DIMS"(ptr [[VLA]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.FIRSTPRIVATE"(ptr [[A]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[B]], i32 undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[C]], i32 undef), "QUAL.OSS.CAPTURED"(i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]), "QUAL.OSS.DEP.INOUT"(ptr [[VLA]], [28 x i8] c"[c]([a][b]vla[sizeof(vla)])\00", ptr @compute_dep.12, ptr [[VLA]], ptr [[A]], ptr [[B]], ptr [[C]], i64 [[TMP1]], i64 [[TMP3]], i64 [[TMP5]]) ], !dbg [[DBG93:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]), !dbg [[DBG94:![0-9]+]]
// AARCH64-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8, !dbg [[DBG95:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.stackrestore(ptr [[TMP12]]), !dbg [[DBG95]]
// AARCH64-NEXT:    ret void, !dbg [[DBG95]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.10
// AARCH64-SAME: (ptr [[VLA:%.*]], ptr [[A:%.*]], ptr [[B:%.*]], i64 [[TMP0:%.*]], i64 [[TMP1:%.*]], i64 [[TMP2:%.*]]) !dbg [[DBG96:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_9:%.*]], align 8
// AARCH64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[DOTADDR1:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[DOTADDR2:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[DOTADDR1]], align 8
// AARCH64-NEXT:    store i64 [[TMP2]], ptr [[DOTADDR2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B]], align 4, !dbg [[DBG97:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = sext i32 [[TMP3]] to i64
// AARCH64-NEXT:    [[TMP5:%.*]] = add i64 [[TMP4]], 1
// AARCH64-NEXT:    [[TMP6:%.*]] = mul nuw i64 [[TMP1]], [[TMP2]], !dbg [[DBG99:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = mul nsw i64 2, [[TMP6]], !dbg [[DBG99]]
// AARCH64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 [[TMP7]], !dbg [[DBG99]]
// AARCH64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[A]], align 4, !dbg [[DBG100:![0-9]+]]
// AARCH64-NEXT:    [[TMP9:%.*]] = zext i32 [[TMP8]] to i64
// AARCH64-NEXT:    [[TMP10:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP11:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYIDX]], ptr [[TMP12]], align 8
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 [[TMP10]], ptr [[TMP13]], align 8
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP14]], align 8
// AARCH64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP11]], ptr [[TMP15]], align 8
// AARCH64-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 [[TMP9]], ptr [[TMP16]], align 8
// AARCH64-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP17]], align 8
// AARCH64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP5]], ptr [[TMP18]], align 8
// AARCH64-NEXT:    [[TMP19:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], align 8, !dbg [[DBG100]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_9]] [[TMP19]], !dbg [[DBG100]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.11
// AARCH64-SAME: (ptr [[VLA:%.*]], ptr [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]], i64 [[TMP0:%.*]], i64 [[TMP1:%.*]], i64 [[TMP2:%.*]]) !dbg [[DBG101:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_10:%.*]], align 8
// AARCH64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[C_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[DOTADDR1:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[DOTADDR2:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[C]], ptr [[C_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[DOTADDR1]], align 8
// AARCH64-NEXT:    store i64 [[TMP2]], ptr [[DOTADDR2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[C]], align 4, !dbg [[DBG102:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP3]] to i64
// AARCH64-NEXT:    [[TMP5:%.*]] = load i32, ptr [[B]], align 4, !dbg [[DBG104:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = zext i32 [[TMP5]] to i64
// AARCH64-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP8:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP9:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[VLA]], ptr [[TMP9]], align 8
// AARCH64-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 [[TMP7]], ptr [[TMP10]], align 8
// AARCH64-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP11]], align 8
// AARCH64-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP8]], ptr [[TMP12]], align 8
// AARCH64-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[TMP13]], align 8
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 0, ptr [[TMP14]], align 8
// AARCH64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[TMP15]], align 8
// AARCH64-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 7
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP16]], align 8
// AARCH64-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 8
// AARCH64-NEXT:    store i64 0, ptr [[TMP17]], align 8
// AARCH64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 9
// AARCH64-NEXT:    store i64 [[TMP6]], ptr [[TMP18]], align 8
// AARCH64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 10
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP19]], align 8
// AARCH64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 11
// AARCH64-NEXT:    store i64 0, ptr [[TMP20]], align 8
// AARCH64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 12
// AARCH64-NEXT:    store i64 [[TMP4]], ptr [[TMP21]], align 8
// AARCH64-NEXT:    [[TMP22:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], align 8, !dbg [[DBG104]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_10]] [[TMP22]], !dbg [[DBG104]]
//
//
// AARCH64-LABEL: define {{[^@]+}}@compute_dep.12
// AARCH64-SAME: (ptr [[VLA:%.*]], ptr [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]], i64 [[TMP0:%.*]], i64 [[TMP1:%.*]], i64 [[TMP2:%.*]]) !dbg [[DBG105:![0-9]+]] {
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_11:%.*]], align 8
// AARCH64-NEXT:    [[VLA_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[C_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[DOTADDR:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[DOTADDR1:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    [[DOTADDR2:%.*]] = alloca i64, align 8
// AARCH64-NEXT:    store ptr [[VLA]], ptr [[VLA_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[C]], ptr [[C_ADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP0]], ptr [[DOTADDR]], align 8
// AARCH64-NEXT:    store i64 [[TMP1]], ptr [[DOTADDR1]], align 8
// AARCH64-NEXT:    store i64 [[TMP2]], ptr [[DOTADDR2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = mul nuw i64 [[TMP0]], [[TMP1]], !dbg [[DBG106:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = mul nuw i64 [[TMP3]], [[TMP2]], !dbg [[DBG106]]
// AARCH64-NEXT:    [[TMP5:%.*]] = mul nuw i64 4, [[TMP4]], !dbg [[DBG106]]
// AARCH64-NEXT:    [[TMP6:%.*]] = mul nuw i64 [[TMP1]], [[TMP2]], !dbg [[DBG108:![0-9]+]]
// AARCH64-NEXT:    [[TMP7:%.*]] = mul nsw i64 [[TMP5]], [[TMP6]], !dbg [[DBG108]]
// AARCH64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[VLA]], i64 [[TMP7]], !dbg [[DBG108]]
// AARCH64-NEXT:    [[TMP8:%.*]] = load i32, ptr [[C]], align 4, !dbg [[DBG109:![0-9]+]]
// AARCH64-NEXT:    [[TMP9:%.*]] = zext i32 [[TMP8]] to i64
// AARCH64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[B]], align 4, !dbg [[DBG110:![0-9]+]]
// AARCH64-NEXT:    [[TMP11:%.*]] = zext i32 [[TMP10]] to i64
// AARCH64-NEXT:    [[TMP12:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP13:%.*]] = mul i64 [[TMP2]], 4
// AARCH64-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYIDX]], ptr [[TMP14]], align 8
// AARCH64-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 [[TMP12]], ptr [[TMP15]], align 8
// AARCH64-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP16]], align 8
// AARCH64-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 [[TMP13]], ptr [[TMP17]], align 8
// AARCH64-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 4
// AARCH64-NEXT:    store i64 [[TMP11]], ptr [[TMP18]], align 8
// AARCH64-NEXT:    [[TMP19:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 5
// AARCH64-NEXT:    store i64 0, ptr [[TMP19]], align 8
// AARCH64-NEXT:    [[TMP20:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 6
// AARCH64-NEXT:    store i64 [[TMP11]], ptr [[TMP20]], align 8
// AARCH64-NEXT:    [[TMP21:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 7
// AARCH64-NEXT:    store i64 [[TMP9]], ptr [[TMP21]], align 8
// AARCH64-NEXT:    [[TMP22:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 8
// AARCH64-NEXT:    store i64 0, ptr [[TMP22]], align 8
// AARCH64-NEXT:    [[TMP23:%.*]] = getelementptr inbounds [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 9
// AARCH64-NEXT:    store i64 [[TMP9]], ptr [[TMP23]], align 8
// AARCH64-NEXT:    [[TMP24:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], align 8, !dbg [[DBG110]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_11]] [[TMP24]], !dbg [[DBG110]]
//
