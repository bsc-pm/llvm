// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --include-generated-funcs
// RUN: %clang_cc1 -triple x86_64-gnu-linux -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=LIN64
// RUN: %clang_cc1 -triple ppc64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=PPC64
// RUN: %clang_cc1 -triple aarch64 -verify -fompss-2 -disable-llvm-passes -ferror-limit 100 %s -emit-llvm -o - | FileCheck %s --check-prefixes=AARCH64
// expected-no-diagnostics

void foo(void) {
  int i;
  int *pi;
  int ai[5];
  #pragma oss task depend(in: i, pi, ai[3])
  { i = *pi = ai[2]; }
}





struct Foo1_struct {
    int x;
} foo1_s;

int foo1_array[5];
int foo1_var;
int *foo1_ptr;

void foo1(void) {
  #pragma oss task depend(in: foo1_var, *foo1_ptr, foo1_array[3], foo1_array[-2], foo1_s.x)
  { foo1_var = *foo1_ptr = foo1_array[3] = foo1_s.x; }
}







void foo2(int *iptr, char *cptr) {
  #pragma oss task depend(in: iptr[3], iptr[-3], cptr[3], cptr[-3])
  { iptr[3] = cptr[3]; }
  #pragma oss task depend(in: *iptr, *cptr)
  { *iptr = *cptr; }
}









struct Foo3_struct {
    int x;
};

void foo3() {
  struct Foo3_struct foo3_struct;
  int a;
  #pragma oss task depend(in: foo3_struct.x, a)
  { foo3_struct.x = a; }
}


// LIN64-LABEL: @foo(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[I:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[PI:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[AI:%.*]] = alloca [5 x i32], align 16
// LIN64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[I]], i32 undef), "QUAL.OSS.SHARED"(ptr [[PI]], ptr undef), "QUAL.OSS.SHARED"(ptr [[AI]], [5 x i32] undef), "QUAL.OSS.DEP.IN"(ptr [[I]], [2 x i8] c"i\00", ptr @compute_dep, ptr [[I]]), "QUAL.OSS.DEP.IN"(ptr [[PI]], [3 x i8] c"pi\00", ptr @compute_dep.1, ptr [[PI]]), "QUAL.OSS.DEP.IN"(ptr [[AI]], [6 x i8] c"ai[3]\00", ptr @compute_dep.2, ptr [[AI]]) ], !dbg [[DBG9:![0-9]+]]
// LIN64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [5 x i32], ptr [[AI]], i64 0, i64 2, !dbg [[DBG10:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[ARRAYIDX]], align 8, !dbg [[DBG10]]
// LIN64-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[PI]], align 8, !dbg [[DBG11:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP1]], ptr [[TMP2]], align 4, !dbg [[DBG12:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP1]], ptr [[I]], align 4, !dbg [[DBG13:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG14:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG15:![0-9]+]]
//
//
// LIN64-LABEL: @compute_dep(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 8
// LIN64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[I:%.*]], ptr [[I_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[I]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 8
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP4]]
//
//
// LIN64-LABEL: @compute_dep.1(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// LIN64-NEXT:    [[PI_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[PI:%.*]], ptr [[PI_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[PI]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 8, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 8, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP4]]
//
//
// LIN64-LABEL: @compute_dep.2(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 8
// LIN64-NEXT:    [[AI_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[AI:%.*]], ptr [[AI_ADDR]], align 8
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [5 x i32], ptr [[AI]], i64 0, i64 0, !dbg [[DBG20:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 20, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 12, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 8, !dbg [[DBG20]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP4]], !dbg [[DBG20]]
//
//
// LIN64-LABEL: @foo1(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @foo1_var, i32 undef), "QUAL.OSS.SHARED"(ptr @foo1_array, [5 x i32] undef), "QUAL.OSS.SHARED"(ptr @foo1_s, [[STRUCT_FOO1_STRUCT:%.*]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr @foo1_ptr, ptr undef), "QUAL.OSS.DEP.IN"(ptr @foo1_var, [9 x i8] c"foo1_var\00", ptr @compute_dep.3, ptr @foo1_var), "QUAL.OSS.DEP.IN"(ptr @foo1_ptr, [10 x i8] c"*foo1_ptr\00", ptr @compute_dep.4, ptr @foo1_ptr), "QUAL.OSS.DEP.IN"(ptr @foo1_array, [14 x i8] c"foo1_array[3]\00", ptr @compute_dep.5, ptr @foo1_array), "QUAL.OSS.DEP.IN"(ptr @foo1_array, [15 x i8] c"foo1_array[-2]\00", ptr @compute_dep.6, ptr @foo1_array), "QUAL.OSS.DEP.IN"(ptr @foo1_s, [9 x i8] c"foo1_s.x\00", ptr @compute_dep.7, ptr @foo1_s) ], !dbg [[DBG23:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i32, ptr @foo1_s, align 4, !dbg [[DBG24:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP1]], ptr getelementptr inbounds ([5 x i32], ptr @foo1_array, i64 0, i64 3), align 4, !dbg [[DBG25:![0-9]+]]
// LIN64-NEXT:    [[TMP2:%.*]] = load ptr, ptr @foo1_ptr, align 8, !dbg [[DBG26:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP1]], ptr [[TMP2]], align 4, !dbg [[DBG27:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP1]], ptr @foo1_var, align 4, !dbg [[DBG28:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG29:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG30:![0-9]+]]
//
//
// LIN64-LABEL: @compute_dep.3(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_2:%.*]], align 8
// LIN64-NEXT:    [[FOO1_VAR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[FOO1_VAR:%.*]], ptr [[FOO1_VAR_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[FOO1_VAR]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], align 8
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_2]] [[TMP4]]
//
//
// LIN64-LABEL: @compute_dep.4(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_3:%.*]], align 8
// LIN64-NEXT:    [[FOO1_PTR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[FOO1_PTR:%.*]], ptr [[FOO1_PTR_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[FOO1_PTR]], align 8, !dbg [[DBG33:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], align 8, !dbg [[DBG33]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_3]] [[TMP5]], !dbg [[DBG33]]
//
//
// LIN64-LABEL: @compute_dep.5(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_4:%.*]], align 8
// LIN64-NEXT:    [[FOO1_ARRAY_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[FOO1_ARRAY:%.*]], ptr [[FOO1_ARRAY_ADDR]], align 8
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [5 x i32], ptr [[FOO1_ARRAY]], i64 0, i64 0, !dbg [[DBG36:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 20, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 12, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], align 8, !dbg [[DBG36]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_4]] [[TMP4]], !dbg [[DBG36]]
//
//
// LIN64-LABEL: @compute_dep.6(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_5:%.*]], align 8
// LIN64-NEXT:    [[FOO1_ARRAY_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[FOO1_ARRAY:%.*]], ptr [[FOO1_ARRAY_ADDR]], align 8
// LIN64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [5 x i32], ptr [[FOO1_ARRAY]], i64 0, i64 0, !dbg [[DBG39:![0-9]+]]
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 20, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 -8, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 -4, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], align 8, !dbg [[DBG39]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_5]] [[TMP4]], !dbg [[DBG39]]
//
//
// LIN64-LABEL: @compute_dep.7(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_6:%.*]], align 8
// LIN64-NEXT:    [[FOO1_S_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[FOO1_S:%.*]], ptr [[FOO1_S_ADDR]], align 8
// LIN64-NEXT:    [[X:%.*]] = getelementptr inbounds nuw [[STRUCT_FOO1_STRUCT:%.*]], ptr [[FOO1_S]], i32 0, i32 0
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[X]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], align 8, !dbg [[DBG42:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_6]] [[TMP4]], !dbg [[DBG42]]
//
//
// LIN64-LABEL: @foo2(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// LIN64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[IPTR_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CPTR_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[IPTR_ADDR]], [8 x i8] c"iptr[3]\00", ptr @compute_dep.8, ptr [[IPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[IPTR_ADDR]], [9 x i8] c"iptr[-3]\00", ptr @compute_dep.9, ptr [[IPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[CPTR_ADDR]], [8 x i8] c"cptr[3]\00", ptr @compute_dep.10, ptr [[CPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[CPTR_ADDR]], [9 x i8] c"cptr[-3]\00", ptr @compute_dep.11, ptr [[CPTR_ADDR]]) ], !dbg [[DBG45:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[CPTR_ADDR]], align 8, !dbg [[DBG46:![0-9]+]]
// LIN64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[TMP1]], i64 3, !dbg [[DBG46]]
// LIN64-NEXT:    [[TMP2:%.*]] = load i8, ptr [[ARRAYIDX]], align 1, !dbg [[DBG46]]
// LIN64-NEXT:    [[CONV:%.*]] = sext i8 [[TMP2]] to i32, !dbg [[DBG46]]
// LIN64-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[IPTR_ADDR]], align 8, !dbg [[DBG47:![0-9]+]]
// LIN64-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i32, ptr [[TMP3]], i64 3, !dbg [[DBG47]]
// LIN64-NEXT:    store i32 [[CONV]], ptr [[ARRAYIDX1]], align 4, !dbg [[DBG48:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG49:![0-9]+]]
// LIN64-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[IPTR_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CPTR_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[IPTR_ADDR]], [6 x i8] c"*iptr\00", ptr @compute_dep.12, ptr [[IPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[CPTR_ADDR]], [6 x i8] c"*cptr\00", ptr @compute_dep.13, ptr [[CPTR_ADDR]]) ], !dbg [[DBG50:![0-9]+]]
// LIN64-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[CPTR_ADDR]], align 8, !dbg [[DBG51:![0-9]+]]
// LIN64-NEXT:    [[TMP6:%.*]] = load i8, ptr [[TMP5]], align 1, !dbg [[DBG52:![0-9]+]]
// LIN64-NEXT:    [[CONV2:%.*]] = sext i8 [[TMP6]] to i32, !dbg [[DBG52]]
// LIN64-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[IPTR_ADDR]], align 8, !dbg [[DBG53:![0-9]+]]
// LIN64-NEXT:    store i32 [[CONV2]], ptr [[TMP7]], align 4, !dbg [[DBG54:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]), !dbg [[DBG55:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG56:![0-9]+]]
//
//
// LIN64-LABEL: @compute_dep.8(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_7:%.*]], align 8
// LIN64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[IPTR]], align 8, !dbg [[DBG58:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 12, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 16, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], align 8, !dbg [[DBG58]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_7]] [[TMP5]], !dbg [[DBG58]]
//
//
// LIN64-LABEL: @compute_dep.9(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_8:%.*]], align 8
// LIN64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[IPTR]], align 8, !dbg [[DBG61:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 -12, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 -8, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], align 8, !dbg [[DBG61]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_8]] [[TMP5]], !dbg [[DBG61]]
//
//
// LIN64-LABEL: @compute_dep.10(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_9:%.*]], align 8
// LIN64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CPTR]], align 8, !dbg [[DBG64:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 1, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 3, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], align 8, !dbg [[DBG64]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_9]] [[TMP5]], !dbg [[DBG64]]
//
//
// LIN64-LABEL: @compute_dep.11(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_10:%.*]], align 8
// LIN64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CPTR]], align 8, !dbg [[DBG67:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 1, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 -3, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 -2, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], align 8, !dbg [[DBG67]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_10]] [[TMP5]], !dbg [[DBG67]]
//
//
// LIN64-LABEL: @compute_dep.12(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_11:%.*]], align 8
// LIN64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[IPTR]], align 8, !dbg [[DBG70:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], align 8, !dbg [[DBG70]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_11]] [[TMP5]], !dbg [[DBG70]]
//
//
// LIN64-LABEL: @compute_dep.13(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_12:%.*]], align 8
// LIN64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CPTR]], align 8, !dbg [[DBG73:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 1, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 1, ptr [[TMP4]], align 8
// LIN64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], align 8, !dbg [[DBG73]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_12]] [[TMP5]], !dbg [[DBG73]]
//
//
// LIN64-LABEL: @foo3(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[FOO3_STRUCT:%.*]] = alloca [[STRUCT_FOO3_STRUCT:%.*]], align 4
// LIN64-NEXT:    [[A:%.*]] = alloca i32, align 4
// LIN64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[FOO3_STRUCT]], [[STRUCT_FOO3_STRUCT]] undef), "QUAL.OSS.SHARED"(ptr [[A]], i32 undef), "QUAL.OSS.DEP.IN"(ptr [[FOO3_STRUCT]], [14 x i8] c"foo3_struct.x\00", ptr @compute_dep.14, ptr [[FOO3_STRUCT]]), "QUAL.OSS.DEP.IN"(ptr [[A]], [2 x i8] c"a\00", ptr @compute_dep.15, ptr [[A]]) ], !dbg [[DBG76:![0-9]+]]
// LIN64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[A]], align 4, !dbg [[DBG77:![0-9]+]]
// LIN64-NEXT:    [[X:%.*]] = getelementptr inbounds nuw [[STRUCT_FOO3_STRUCT]], ptr [[FOO3_STRUCT]], i32 0, i32 0, !dbg [[DBG78:![0-9]+]]
// LIN64-NEXT:    store i32 [[TMP1]], ptr [[X]], align 4, !dbg [[DBG79:![0-9]+]]
// LIN64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG80:![0-9]+]]
// LIN64-NEXT:    ret void, !dbg [[DBG81:![0-9]+]]
//
//
// LIN64-LABEL: @compute_dep.14(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_13:%.*]], align 8
// LIN64-NEXT:    [[FOO3_STRUCT_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[FOO3_STRUCT:%.*]], ptr [[FOO3_STRUCT_ADDR]], align 8
// LIN64-NEXT:    [[X:%.*]] = getelementptr inbounds nuw [[STRUCT_FOO3_STRUCT:%.*]], ptr [[FOO3_STRUCT]], i32 0, i32 0
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[X]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], align 8, !dbg [[DBG83:![0-9]+]]
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_13]] [[TMP4]], !dbg [[DBG83]]
//
//
// LIN64-LABEL: @compute_dep.15(
// LIN64-NEXT:  entry:
// LIN64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_14:%.*]], align 8
// LIN64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// LIN64-NEXT:    store ptr [[A:%.*]], ptr [[A_ADDR]], align 8
// LIN64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 0
// LIN64-NEXT:    store ptr [[A]], ptr [[TMP0]], align 8
// LIN64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 1
// LIN64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// LIN64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 2
// LIN64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// LIN64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 3
// LIN64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// LIN64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], align 8
// LIN64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_14]] [[TMP4]]
//
//
// PPC64-LABEL: @foo(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[I:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[PI:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[AI:%.*]] = alloca [5 x i32], align 4
// PPC64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[I]], i32 undef), "QUAL.OSS.SHARED"(ptr [[PI]], ptr undef), "QUAL.OSS.SHARED"(ptr [[AI]], [5 x i32] undef), "QUAL.OSS.DEP.IN"(ptr [[I]], [2 x i8] c"i\00", ptr @compute_dep, ptr [[I]]), "QUAL.OSS.DEP.IN"(ptr [[PI]], [3 x i8] c"pi\00", ptr @compute_dep.1, ptr [[PI]]), "QUAL.OSS.DEP.IN"(ptr [[AI]], [6 x i8] c"ai[3]\00", ptr @compute_dep.2, ptr [[AI]]) ], !dbg [[DBG9:![0-9]+]]
// PPC64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [5 x i32], ptr [[AI]], i64 0, i64 2, !dbg [[DBG10:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[ARRAYIDX]], align 4, !dbg [[DBG10]]
// PPC64-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[PI]], align 8, !dbg [[DBG11:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP1]], ptr [[TMP2]], align 4, !dbg [[DBG12:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP1]], ptr [[I]], align 4, !dbg [[DBG13:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG14:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG15:![0-9]+]]
//
//
// PPC64-LABEL: @compute_dep(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 8
// PPC64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[I:%.*]], ptr [[I_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[I]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 8
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP4]]
//
//
// PPC64-LABEL: @compute_dep.1(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// PPC64-NEXT:    [[PI_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[PI:%.*]], ptr [[PI_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[PI]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 8, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 8, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP4]]
//
//
// PPC64-LABEL: @compute_dep.2(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 8
// PPC64-NEXT:    [[AI_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[AI:%.*]], ptr [[AI_ADDR]], align 8
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [5 x i32], ptr [[AI]], i64 0, i64 0, !dbg [[DBG20:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 20, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 12, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 8, !dbg [[DBG20]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP4]], !dbg [[DBG20]]
//
//
// PPC64-LABEL: @foo1(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @foo1_var, i32 undef), "QUAL.OSS.SHARED"(ptr @foo1_array, [5 x i32] undef), "QUAL.OSS.SHARED"(ptr @foo1_s, [[STRUCT_FOO1_STRUCT:%.*]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr @foo1_ptr, ptr undef), "QUAL.OSS.DEP.IN"(ptr @foo1_var, [9 x i8] c"foo1_var\00", ptr @compute_dep.3, ptr @foo1_var), "QUAL.OSS.DEP.IN"(ptr @foo1_ptr, [10 x i8] c"*foo1_ptr\00", ptr @compute_dep.4, ptr @foo1_ptr), "QUAL.OSS.DEP.IN"(ptr @foo1_array, [14 x i8] c"foo1_array[3]\00", ptr @compute_dep.5, ptr @foo1_array), "QUAL.OSS.DEP.IN"(ptr @foo1_array, [15 x i8] c"foo1_array[-2]\00", ptr @compute_dep.6, ptr @foo1_array), "QUAL.OSS.DEP.IN"(ptr @foo1_s, [9 x i8] c"foo1_s.x\00", ptr @compute_dep.7, ptr @foo1_s) ], !dbg [[DBG23:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i32, ptr @foo1_s, align 4, !dbg [[DBG24:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP1]], ptr getelementptr inbounds ([5 x i32], ptr @foo1_array, i64 0, i64 3), align 4, !dbg [[DBG25:![0-9]+]]
// PPC64-NEXT:    [[TMP2:%.*]] = load ptr, ptr @foo1_ptr, align 8, !dbg [[DBG26:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP1]], ptr [[TMP2]], align 4, !dbg [[DBG27:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP1]], ptr @foo1_var, align 4, !dbg [[DBG28:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG29:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG30:![0-9]+]]
//
//
// PPC64-LABEL: @compute_dep.3(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_2:%.*]], align 8
// PPC64-NEXT:    [[FOO1_VAR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[FOO1_VAR:%.*]], ptr [[FOO1_VAR_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[FOO1_VAR]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], align 8
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_2]] [[TMP4]]
//
//
// PPC64-LABEL: @compute_dep.4(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_3:%.*]], align 8
// PPC64-NEXT:    [[FOO1_PTR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[FOO1_PTR:%.*]], ptr [[FOO1_PTR_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[FOO1_PTR]], align 8, !dbg [[DBG33:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], align 8, !dbg [[DBG33]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_3]] [[TMP5]], !dbg [[DBG33]]
//
//
// PPC64-LABEL: @compute_dep.5(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_4:%.*]], align 8
// PPC64-NEXT:    [[FOO1_ARRAY_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[FOO1_ARRAY:%.*]], ptr [[FOO1_ARRAY_ADDR]], align 8
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [5 x i32], ptr [[FOO1_ARRAY]], i64 0, i64 0, !dbg [[DBG36:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 20, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 12, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], align 8, !dbg [[DBG36]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_4]] [[TMP4]], !dbg [[DBG36]]
//
//
// PPC64-LABEL: @compute_dep.6(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_5:%.*]], align 8
// PPC64-NEXT:    [[FOO1_ARRAY_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[FOO1_ARRAY:%.*]], ptr [[FOO1_ARRAY_ADDR]], align 8
// PPC64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [5 x i32], ptr [[FOO1_ARRAY]], i64 0, i64 0, !dbg [[DBG39:![0-9]+]]
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 20, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 -8, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 -4, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], align 8, !dbg [[DBG39]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_5]] [[TMP4]], !dbg [[DBG39]]
//
//
// PPC64-LABEL: @compute_dep.7(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_6:%.*]], align 8
// PPC64-NEXT:    [[FOO1_S_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[FOO1_S:%.*]], ptr [[FOO1_S_ADDR]], align 8
// PPC64-NEXT:    [[X:%.*]] = getelementptr inbounds nuw [[STRUCT_FOO1_STRUCT:%.*]], ptr [[FOO1_S]], i32 0, i32 0
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[X]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], align 8, !dbg [[DBG42:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_6]] [[TMP4]], !dbg [[DBG42]]
//
//
// PPC64-LABEL: @foo2(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// PPC64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[IPTR_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CPTR_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[IPTR_ADDR]], [8 x i8] c"iptr[3]\00", ptr @compute_dep.8, ptr [[IPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[IPTR_ADDR]], [9 x i8] c"iptr[-3]\00", ptr @compute_dep.9, ptr [[IPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[CPTR_ADDR]], [8 x i8] c"cptr[3]\00", ptr @compute_dep.10, ptr [[CPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[CPTR_ADDR]], [9 x i8] c"cptr[-3]\00", ptr @compute_dep.11, ptr [[CPTR_ADDR]]) ], !dbg [[DBG45:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[CPTR_ADDR]], align 8, !dbg [[DBG46:![0-9]+]]
// PPC64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[TMP1]], i64 3, !dbg [[DBG46]]
// PPC64-NEXT:    [[TMP2:%.*]] = load i8, ptr [[ARRAYIDX]], align 1, !dbg [[DBG46]]
// PPC64-NEXT:    [[CONV:%.*]] = sext i8 [[TMP2]] to i32, !dbg [[DBG46]]
// PPC64-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[IPTR_ADDR]], align 8, !dbg [[DBG47:![0-9]+]]
// PPC64-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i32, ptr [[TMP3]], i64 3, !dbg [[DBG47]]
// PPC64-NEXT:    store i32 [[CONV]], ptr [[ARRAYIDX1]], align 4, !dbg [[DBG48:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG49:![0-9]+]]
// PPC64-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[IPTR_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CPTR_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[IPTR_ADDR]], [6 x i8] c"*iptr\00", ptr @compute_dep.12, ptr [[IPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[CPTR_ADDR]], [6 x i8] c"*cptr\00", ptr @compute_dep.13, ptr [[CPTR_ADDR]]) ], !dbg [[DBG50:![0-9]+]]
// PPC64-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[CPTR_ADDR]], align 8, !dbg [[DBG51:![0-9]+]]
// PPC64-NEXT:    [[TMP6:%.*]] = load i8, ptr [[TMP5]], align 1, !dbg [[DBG52:![0-9]+]]
// PPC64-NEXT:    [[CONV2:%.*]] = sext i8 [[TMP6]] to i32, !dbg [[DBG52]]
// PPC64-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[IPTR_ADDR]], align 8, !dbg [[DBG53:![0-9]+]]
// PPC64-NEXT:    store i32 [[CONV2]], ptr [[TMP7]], align 4, !dbg [[DBG54:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]), !dbg [[DBG55:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG56:![0-9]+]]
//
//
// PPC64-LABEL: @compute_dep.8(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_7:%.*]], align 8
// PPC64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[IPTR]], align 8, !dbg [[DBG58:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 12, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 16, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], align 8, !dbg [[DBG58]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_7]] [[TMP5]], !dbg [[DBG58]]
//
//
// PPC64-LABEL: @compute_dep.9(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_8:%.*]], align 8
// PPC64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[IPTR]], align 8, !dbg [[DBG61:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 -12, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 -8, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], align 8, !dbg [[DBG61]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_8]] [[TMP5]], !dbg [[DBG61]]
//
//
// PPC64-LABEL: @compute_dep.10(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_9:%.*]], align 8
// PPC64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CPTR]], align 8, !dbg [[DBG64:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 1, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 3, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], align 8, !dbg [[DBG64]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_9]] [[TMP5]], !dbg [[DBG64]]
//
//
// PPC64-LABEL: @compute_dep.11(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_10:%.*]], align 8
// PPC64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CPTR]], align 8, !dbg [[DBG67:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 1, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 -3, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 -2, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], align 8, !dbg [[DBG67]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_10]] [[TMP5]], !dbg [[DBG67]]
//
//
// PPC64-LABEL: @compute_dep.12(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_11:%.*]], align 8
// PPC64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[IPTR]], align 8, !dbg [[DBG70:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], align 8, !dbg [[DBG70]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_11]] [[TMP5]], !dbg [[DBG70]]
//
//
// PPC64-LABEL: @compute_dep.13(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_12:%.*]], align 8
// PPC64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CPTR]], align 8, !dbg [[DBG73:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 1, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 1, ptr [[TMP4]], align 8
// PPC64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], align 8, !dbg [[DBG73]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_12]] [[TMP5]], !dbg [[DBG73]]
//
//
// PPC64-LABEL: @foo3(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[FOO3_STRUCT:%.*]] = alloca [[STRUCT_FOO3_STRUCT:%.*]], align 4
// PPC64-NEXT:    [[A:%.*]] = alloca i32, align 4
// PPC64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[FOO3_STRUCT]], [[STRUCT_FOO3_STRUCT]] undef), "QUAL.OSS.SHARED"(ptr [[A]], i32 undef), "QUAL.OSS.DEP.IN"(ptr [[FOO3_STRUCT]], [14 x i8] c"foo3_struct.x\00", ptr @compute_dep.14, ptr [[FOO3_STRUCT]]), "QUAL.OSS.DEP.IN"(ptr [[A]], [2 x i8] c"a\00", ptr @compute_dep.15, ptr [[A]]) ], !dbg [[DBG76:![0-9]+]]
// PPC64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[A]], align 4, !dbg [[DBG77:![0-9]+]]
// PPC64-NEXT:    [[X:%.*]] = getelementptr inbounds nuw [[STRUCT_FOO3_STRUCT]], ptr [[FOO3_STRUCT]], i32 0, i32 0, !dbg [[DBG78:![0-9]+]]
// PPC64-NEXT:    store i32 [[TMP1]], ptr [[X]], align 4, !dbg [[DBG79:![0-9]+]]
// PPC64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG80:![0-9]+]]
// PPC64-NEXT:    ret void, !dbg [[DBG81:![0-9]+]]
//
//
// PPC64-LABEL: @compute_dep.14(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_13:%.*]], align 8
// PPC64-NEXT:    [[FOO3_STRUCT_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[FOO3_STRUCT:%.*]], ptr [[FOO3_STRUCT_ADDR]], align 8
// PPC64-NEXT:    [[X:%.*]] = getelementptr inbounds nuw [[STRUCT_FOO3_STRUCT:%.*]], ptr [[FOO3_STRUCT]], i32 0, i32 0
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[X]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], align 8, !dbg [[DBG83:![0-9]+]]
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_13]] [[TMP4]], !dbg [[DBG83]]
//
//
// PPC64-LABEL: @compute_dep.15(
// PPC64-NEXT:  entry:
// PPC64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_14:%.*]], align 8
// PPC64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// PPC64-NEXT:    store ptr [[A:%.*]], ptr [[A_ADDR]], align 8
// PPC64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 0
// PPC64-NEXT:    store ptr [[A]], ptr [[TMP0]], align 8
// PPC64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 1
// PPC64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// PPC64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 2
// PPC64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// PPC64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 3
// PPC64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// PPC64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], align 8
// PPC64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_14]] [[TMP4]]
//
//
// AARCH64-LABEL: @foo(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[I:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[PI:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[AI:%.*]] = alloca [5 x i32], align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[I]], i32 undef), "QUAL.OSS.SHARED"(ptr [[PI]], ptr undef), "QUAL.OSS.SHARED"(ptr [[AI]], [5 x i32] undef), "QUAL.OSS.DEP.IN"(ptr [[I]], [2 x i8] c"i\00", ptr @compute_dep, ptr [[I]]), "QUAL.OSS.DEP.IN"(ptr [[PI]], [3 x i8] c"pi\00", ptr @compute_dep.1, ptr [[PI]]), "QUAL.OSS.DEP.IN"(ptr [[AI]], [6 x i8] c"ai[3]\00", ptr @compute_dep.2, ptr [[AI]]) ], !dbg [[DBG9:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [5 x i32], ptr [[AI]], i64 0, i64 2, !dbg [[DBG10:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[ARRAYIDX]], align 4, !dbg [[DBG10]]
// AARCH64-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[PI]], align 8, !dbg [[DBG11:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP1]], ptr [[TMP2]], align 4, !dbg [[DBG12:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP1]], ptr [[I]], align 4, !dbg [[DBG13:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG14:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG15:![0-9]+]]
//
//
// AARCH64-LABEL: @compute_dep(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T:%.*]], align 8
// AARCH64-NEXT:    [[I_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[I:%.*]], ptr [[I_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[I]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T]], ptr [[RETVAL]], align 8
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T]] [[TMP4]]
//
//
// AARCH64-LABEL: @compute_dep.1(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_0:%.*]], align 8
// AARCH64-NEXT:    [[PI_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[PI:%.*]], ptr [[PI_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[PI]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 8, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 8, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_0]], ptr [[RETVAL]], align 8
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_0]] [[TMP4]]
//
//
// AARCH64-LABEL: @compute_dep.2(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_1:%.*]], align 8
// AARCH64-NEXT:    [[AI_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[AI:%.*]], ptr [[AI_ADDR]], align 8
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [5 x i32], ptr [[AI]], i64 0, i64 0, !dbg [[DBG20:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 20, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 12, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_1]], ptr [[RETVAL]], align 8, !dbg [[DBG20]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_1]] [[TMP4]], !dbg [[DBG20]]
//
//
// AARCH64-LABEL: @foo1(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr @foo1_var, i32 undef), "QUAL.OSS.SHARED"(ptr @foo1_array, [5 x i32] undef), "QUAL.OSS.SHARED"(ptr @foo1_s, [[STRUCT_FOO1_STRUCT:%.*]] undef), "QUAL.OSS.FIRSTPRIVATE"(ptr @foo1_ptr, ptr undef), "QUAL.OSS.DEP.IN"(ptr @foo1_var, [9 x i8] c"foo1_var\00", ptr @compute_dep.3, ptr @foo1_var), "QUAL.OSS.DEP.IN"(ptr @foo1_ptr, [10 x i8] c"*foo1_ptr\00", ptr @compute_dep.4, ptr @foo1_ptr), "QUAL.OSS.DEP.IN"(ptr @foo1_array, [14 x i8] c"foo1_array[3]\00", ptr @compute_dep.5, ptr @foo1_array), "QUAL.OSS.DEP.IN"(ptr @foo1_array, [15 x i8] c"foo1_array[-2]\00", ptr @compute_dep.6, ptr @foo1_array), "QUAL.OSS.DEP.IN"(ptr @foo1_s, [9 x i8] c"foo1_s.x\00", ptr @compute_dep.7, ptr @foo1_s) ], !dbg [[DBG23:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i32, ptr @foo1_s, align 4, !dbg [[DBG24:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP1]], ptr getelementptr inbounds ([5 x i32], ptr @foo1_array, i64 0, i64 3), align 4, !dbg [[DBG25:![0-9]+]]
// AARCH64-NEXT:    [[TMP2:%.*]] = load ptr, ptr @foo1_ptr, align 8, !dbg [[DBG26:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP1]], ptr [[TMP2]], align 4, !dbg [[DBG27:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP1]], ptr @foo1_var, align 4, !dbg [[DBG28:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG29:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG30:![0-9]+]]
//
//
// AARCH64-LABEL: @compute_dep.3(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_2:%.*]], align 8
// AARCH64-NEXT:    [[FOO1_VAR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[FOO1_VAR:%.*]], ptr [[FOO1_VAR_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[FOO1_VAR]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_2]], ptr [[RETVAL]], align 8
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_2]] [[TMP4]]
//
//
// AARCH64-LABEL: @compute_dep.4(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_3:%.*]], align 8
// AARCH64-NEXT:    [[FOO1_PTR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[FOO1_PTR:%.*]], ptr [[FOO1_PTR_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[FOO1_PTR]], align 8, !dbg [[DBG33:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_3]], ptr [[RETVAL]], align 8, !dbg [[DBG33]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_3]] [[TMP5]], !dbg [[DBG33]]
//
//
// AARCH64-LABEL: @compute_dep.5(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_4:%.*]], align 8
// AARCH64-NEXT:    [[FOO1_ARRAY_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[FOO1_ARRAY:%.*]], ptr [[FOO1_ARRAY_ADDR]], align 8
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [5 x i32], ptr [[FOO1_ARRAY]], i64 0, i64 0, !dbg [[DBG36:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 20, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 12, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 16, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_4]], ptr [[RETVAL]], align 8, !dbg [[DBG36]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_4]] [[TMP4]], !dbg [[DBG36]]
//
//
// AARCH64-LABEL: @compute_dep.6(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_5:%.*]], align 8
// AARCH64-NEXT:    [[FOO1_ARRAY_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[FOO1_ARRAY:%.*]], ptr [[FOO1_ARRAY_ADDR]], align 8
// AARCH64-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [5 x i32], ptr [[FOO1_ARRAY]], i64 0, i64 0, !dbg [[DBG39:![0-9]+]]
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[ARRAYDECAY]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 20, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 -8, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 -4, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_5]], ptr [[RETVAL]], align 8, !dbg [[DBG39]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_5]] [[TMP4]], !dbg [[DBG39]]
//
//
// AARCH64-LABEL: @compute_dep.7(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_6:%.*]], align 8
// AARCH64-NEXT:    [[FOO1_S_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[FOO1_S:%.*]], ptr [[FOO1_S_ADDR]], align 8
// AARCH64-NEXT:    [[X:%.*]] = getelementptr inbounds nuw [[STRUCT_FOO1_STRUCT:%.*]], ptr [[FOO1_S]], i32 0, i32 0
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[X]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_6]], ptr [[RETVAL]], align 8, !dbg [[DBG42:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_6]] [[TMP4]], !dbg [[DBG42]]
//
//
// AARCH64-LABEL: @foo2(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// AARCH64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[IPTR_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CPTR_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[IPTR_ADDR]], [8 x i8] c"iptr[3]\00", ptr @compute_dep.8, ptr [[IPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[IPTR_ADDR]], [9 x i8] c"iptr[-3]\00", ptr @compute_dep.9, ptr [[IPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[CPTR_ADDR]], [8 x i8] c"cptr[3]\00", ptr @compute_dep.10, ptr [[CPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[CPTR_ADDR]], [9 x i8] c"cptr[-3]\00", ptr @compute_dep.11, ptr [[CPTR_ADDR]]) ], !dbg [[DBG45:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[CPTR_ADDR]], align 8, !dbg [[DBG46:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[TMP1]], i64 3, !dbg [[DBG46]]
// AARCH64-NEXT:    [[TMP2:%.*]] = load i8, ptr [[ARRAYIDX]], align 1, !dbg [[DBG46]]
// AARCH64-NEXT:    [[CONV:%.*]] = sext i8 [[TMP2]] to i32, !dbg [[DBG46]]
// AARCH64-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[IPTR_ADDR]], align 8, !dbg [[DBG47:![0-9]+]]
// AARCH64-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i32, ptr [[TMP3]], i64 3, !dbg [[DBG47]]
// AARCH64-NEXT:    store i32 [[CONV]], ptr [[ARRAYIDX1]], align 4, !dbg [[DBG48:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG49:![0-9]+]]
// AARCH64-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.FIRSTPRIVATE"(ptr [[IPTR_ADDR]], ptr undef), "QUAL.OSS.FIRSTPRIVATE"(ptr [[CPTR_ADDR]], ptr undef), "QUAL.OSS.DEP.IN"(ptr [[IPTR_ADDR]], [6 x i8] c"*iptr\00", ptr @compute_dep.12, ptr [[IPTR_ADDR]]), "QUAL.OSS.DEP.IN"(ptr [[CPTR_ADDR]], [6 x i8] c"*cptr\00", ptr @compute_dep.13, ptr [[CPTR_ADDR]]) ], !dbg [[DBG50:![0-9]+]]
// AARCH64-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[CPTR_ADDR]], align 8, !dbg [[DBG51:![0-9]+]]
// AARCH64-NEXT:    [[TMP6:%.*]] = load i8, ptr [[TMP5]], align 1, !dbg [[DBG52:![0-9]+]]
// AARCH64-NEXT:    [[CONV2:%.*]] = sext i8 [[TMP6]] to i32, !dbg [[DBG52]]
// AARCH64-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[IPTR_ADDR]], align 8, !dbg [[DBG53:![0-9]+]]
// AARCH64-NEXT:    store i32 [[CONV2]], ptr [[TMP7]], align 4, !dbg [[DBG54:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]), !dbg [[DBG55:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG56:![0-9]+]]
//
//
// AARCH64-LABEL: @compute_dep.8(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_7:%.*]], align 8
// AARCH64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[IPTR]], align 8, !dbg [[DBG58:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 12, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 16, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_7]], ptr [[RETVAL]], align 8, !dbg [[DBG58]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_7]] [[TMP5]], !dbg [[DBG58]]
//
//
// AARCH64-LABEL: @compute_dep.9(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_8:%.*]], align 8
// AARCH64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[IPTR]], align 8, !dbg [[DBG61:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 -12, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 -8, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_8]], ptr [[RETVAL]], align 8, !dbg [[DBG61]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_8]] [[TMP5]], !dbg [[DBG61]]
//
//
// AARCH64-LABEL: @compute_dep.10(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_9:%.*]], align 8
// AARCH64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CPTR]], align 8, !dbg [[DBG64:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 1, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 3, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_9]], ptr [[RETVAL]], align 8, !dbg [[DBG64]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_9]] [[TMP5]], !dbg [[DBG64]]
//
//
// AARCH64-LABEL: @compute_dep.11(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_10:%.*]], align 8
// AARCH64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CPTR]], align 8, !dbg [[DBG67:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 1, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 -3, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 -2, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_10]], ptr [[RETVAL]], align 8, !dbg [[DBG67]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_10]] [[TMP5]], !dbg [[DBG67]]
//
//
// AARCH64-LABEL: @compute_dep.12(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_11:%.*]], align 8
// AARCH64-NEXT:    [[IPTR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[IPTR:%.*]], ptr [[IPTR_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[IPTR]], align 8, !dbg [[DBG70:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 4, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 4, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_11]], ptr [[RETVAL]], align 8, !dbg [[DBG70]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_11]] [[TMP5]], !dbg [[DBG70]]
//
//
// AARCH64-LABEL: @compute_dep.13(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_12:%.*]], align 8
// AARCH64-NEXT:    [[CPTR_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[CPTR:%.*]], ptr [[CPTR_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CPTR]], align 8, !dbg [[DBG73:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[TMP0]], ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 1, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 1, ptr [[TMP4]], align 8
// AARCH64-NEXT:    [[TMP5:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_12]], ptr [[RETVAL]], align 8, !dbg [[DBG73]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_12]] [[TMP5]], !dbg [[DBG73]]
//
//
// AARCH64-LABEL: @foo3(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[FOO3_STRUCT:%.*]] = alloca [[STRUCT_FOO3_STRUCT:%.*]], align 4
// AARCH64-NEXT:    [[A:%.*]] = alloca i32, align 4
// AARCH64-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OSS"([5 x i8] c"TASK\00"), "QUAL.OSS.SHARED"(ptr [[FOO3_STRUCT]], [[STRUCT_FOO3_STRUCT]] undef), "QUAL.OSS.SHARED"(ptr [[A]], i32 undef), "QUAL.OSS.DEP.IN"(ptr [[FOO3_STRUCT]], [14 x i8] c"foo3_struct.x\00", ptr @compute_dep.14, ptr [[FOO3_STRUCT]]), "QUAL.OSS.DEP.IN"(ptr [[A]], [2 x i8] c"a\00", ptr @compute_dep.15, ptr [[A]]) ], !dbg [[DBG76:![0-9]+]]
// AARCH64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[A]], align 4, !dbg [[DBG77:![0-9]+]]
// AARCH64-NEXT:    [[X:%.*]] = getelementptr inbounds nuw [[STRUCT_FOO3_STRUCT]], ptr [[FOO3_STRUCT]], i32 0, i32 0, !dbg [[DBG78:![0-9]+]]
// AARCH64-NEXT:    store i32 [[TMP1]], ptr [[X]], align 4, !dbg [[DBG79:![0-9]+]]
// AARCH64-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]), !dbg [[DBG80:![0-9]+]]
// AARCH64-NEXT:    ret void, !dbg [[DBG81:![0-9]+]]
//
//
// AARCH64-LABEL: @compute_dep.14(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_13:%.*]], align 8
// AARCH64-NEXT:    [[FOO3_STRUCT_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[FOO3_STRUCT:%.*]], ptr [[FOO3_STRUCT_ADDR]], align 8
// AARCH64-NEXT:    [[X:%.*]] = getelementptr inbounds nuw [[STRUCT_FOO3_STRUCT:%.*]], ptr [[FOO3_STRUCT]], i32 0, i32 0
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[X]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_13]], ptr [[RETVAL]], align 8, !dbg [[DBG83:![0-9]+]]
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_13]] [[TMP4]], !dbg [[DBG83]]
//
//
// AARCH64-LABEL: @compute_dep.15(
// AARCH64-NEXT:  entry:
// AARCH64-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__DEPEND_UNPACK_T_14:%.*]], align 8
// AARCH64-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// AARCH64-NEXT:    store ptr [[A:%.*]], ptr [[A_ADDR]], align 8
// AARCH64-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 0
// AARCH64-NEXT:    store ptr [[A]], ptr [[TMP0]], align 8
// AARCH64-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 1
// AARCH64-NEXT:    store i64 4, ptr [[TMP1]], align 8
// AARCH64-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 2
// AARCH64-NEXT:    store i64 0, ptr [[TMP2]], align 8
// AARCH64-NEXT:    [[TMP3:%.*]] = getelementptr inbounds nuw [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], i32 0, i32 3
// AARCH64-NEXT:    store i64 4, ptr [[TMP3]], align 8
// AARCH64-NEXT:    [[TMP4:%.*]] = load [[STRUCT__DEPEND_UNPACK_T_14]], ptr [[RETVAL]], align 8
// AARCH64-NEXT:    ret [[STRUCT__DEPEND_UNPACK_T_14]] [[TMP4]]
//
