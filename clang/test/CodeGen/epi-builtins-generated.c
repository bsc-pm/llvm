// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang -mepi -O2 -S -emit-llvm -o - %s | FileCheck --check-prefix=CHECK-O2 %s



// CHECK-O2-LABEL: @test_vreadvl(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vreadvl()
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vreadvl()
{
    return __builtin_epi_vreadvl();
}


// CHECK-O2-LABEL: @test_vsetvl__epi_e64__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 3, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e64__epi_m1(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e64, __epi_m1);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e32__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 2, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e32__epi_m1(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e32, __epi_m1);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e16__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 1, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e16__epi_m1(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e16, __epi_m1);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e8__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 0, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e8__epi_m1(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e8, __epi_m1);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e64__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 3, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e64__epi_m2(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e64, __epi_m2);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e32__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 2, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e32__epi_m2(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e32, __epi_m2);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e16__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 1, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e16__epi_m2(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e16, __epi_m2);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e8__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 0, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e8__epi_m2(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e8, __epi_m2);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e64__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 3, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e64__epi_m4(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e64, __epi_m4);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e32__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 2, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e32__epi_m4(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e32, __epi_m4);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e16__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 1, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e16__epi_m4(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e16, __epi_m4);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e8__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 0, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e8__epi_m4(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e8, __epi_m4);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e64__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 3, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e64__epi_m8(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e64, __epi_m8);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e32__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 2, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e32__epi_m8(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e32, __epi_m8);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e16__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 1, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e16__epi_m8(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e16, __epi_m8);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e8__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 0, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e8__epi_m8(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e8, __epi_m8);
    return gvl;
}


// CHECK-O2-LABEL: @test_vload_8xi8_vstore_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.v8i8(<vscale x 8 x i8>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_8xi8_vstore_8xi8(signed char*  addr, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vload_8xi8(addr, gvl);
  __builtin_epi_vstore_8xi8(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_4xi16_vstore_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.v4i16(<vscale x 4 x i16>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_4xi16_vstore_4xi16(signed short int*  addr, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vload_4xi16(addr, gvl);
  __builtin_epi_vstore_4xi16(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_2xi32_vstore_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.v2i32(<vscale x 2 x i32>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_2xi32_vstore_2xi32(signed int*  addr, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vload_2xi32(addr, gvl);
  __builtin_epi_vstore_2xi32(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_1xi64_vstore_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.v1i64(<vscale x 1 x i64>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_1xi64_vstore_1xi64(signed long int*  addr, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vload_1xi64(addr, gvl);
  __builtin_epi_vstore_1xi64(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_8xi8_vstore_strided_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.strided.v8i8(<vscale x 8 x i8>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_8xi8_vstore_strided_8xi8(signed char*  addr, signed long stride, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vload_strided_8xi8(addr, stride, gvl);
  __builtin_epi_vstore_strided_8xi8(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_4xi16_vstore_strided_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.strided.v4i16(<vscale x 4 x i16>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_4xi16_vstore_strided_4xi16(signed short int*  addr, signed long stride, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vload_strided_4xi16(addr, stride, gvl);
  __builtin_epi_vstore_strided_4xi16(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_2xi32_vstore_strided_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.strided.v2i32(<vscale x 2 x i32>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_2xi32_vstore_strided_2xi32(signed int*  addr, signed long stride, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vload_strided_2xi32(addr, stride, gvl);
  __builtin_epi_vstore_strided_2xi32(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_1xi64_vstore_strided_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.strided.v1i64(<vscale x 1 x i64>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_1xi64_vstore_strided_1xi64(signed long int*  addr, signed long stride, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vload_strided_1xi64(addr, stride, gvl);
  __builtin_epi_vstore_strided_1xi64(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_8xi8_vstore_indexed_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.indexed.v8i8.v8i8(<vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v8i8.v8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_8xi8_vstore_indexed_8xi8(signed char*  addr, unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 index;
  result = __builtin_epi_vload_indexed_8xi8(addr, index, gvl);
  __builtin_epi_vstore_indexed_8xi8(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_4xi16_vstore_indexed_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.indexed.v4i16.v4i16(<vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v4i16.v4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_4xi16_vstore_indexed_4xi16(signed short int*  addr, unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 index;
  result = __builtin_epi_vload_indexed_4xi16(addr, index, gvl);
  __builtin_epi_vstore_indexed_4xi16(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_2xi32_vstore_indexed_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.indexed.v2i32.v2i32(<vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v2i32.v2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_2xi32_vstore_indexed_2xi32(signed int*  addr, unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 index;
  result = __builtin_epi_vload_indexed_2xi32(addr, index, gvl);
  __builtin_epi_vstore_indexed_2xi32(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_1xi64_vstore_indexed_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.indexed.v1i64.v1i64(<vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v1i64.v1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_1xi64_vstore_indexed_1xi64(signed long int*  addr, unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 index;
  result = __builtin_epi_vload_indexed_1xi64(addr, index, gvl);
  __builtin_epi_vstore_indexed_1xi64(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_unsigned_8xi8_vstore_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.v8i8(<vscale x 8 x i8>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_unsigned_8xi8_vstore_unsigned_8xi8(unsigned char*  addr, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vload_unsigned_8xi8(addr, gvl);
  __builtin_epi_vstore_unsigned_8xi8(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_unsigned_4xi16_vstore_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.v4i16(<vscale x 4 x i16>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_unsigned_4xi16_vstore_unsigned_4xi16(unsigned short int*  addr, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vload_unsigned_4xi16(addr, gvl);
  __builtin_epi_vstore_unsigned_4xi16(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_unsigned_2xi32_vstore_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.v2i32(<vscale x 2 x i32>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_unsigned_2xi32_vstore_unsigned_2xi32(unsigned int*  addr, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vload_unsigned_2xi32(addr, gvl);
  __builtin_epi_vstore_unsigned_2xi32(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_unsigned_1xi64_vstore_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.v1i64(<vscale x 1 x i64>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_unsigned_1xi64_vstore_unsigned_1xi64(unsigned long int*  addr, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vload_unsigned_1xi64(addr, gvl);
  __builtin_epi_vstore_unsigned_1xi64(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_unsigned_8xi8_vstore_strided_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.strided.v8i8(<vscale x 8 x i8>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_unsigned_8xi8_vstore_strided_unsigned_8xi8(unsigned char*  addr, signed long stride, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vload_strided_unsigned_8xi8(addr, stride, gvl);
  __builtin_epi_vstore_strided_unsigned_8xi8(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_unsigned_4xi16_vstore_strided_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.strided.v4i16(<vscale x 4 x i16>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_unsigned_4xi16_vstore_strided_unsigned_4xi16(unsigned short int*  addr, signed long stride, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vload_strided_unsigned_4xi16(addr, stride, gvl);
  __builtin_epi_vstore_strided_unsigned_4xi16(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_unsigned_2xi32_vstore_strided_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.strided.v2i32(<vscale x 2 x i32>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_unsigned_2xi32_vstore_strided_unsigned_2xi32(unsigned int*  addr, signed long stride, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vload_strided_unsigned_2xi32(addr, stride, gvl);
  __builtin_epi_vstore_strided_unsigned_2xi32(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_unsigned_1xi64_vstore_strided_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.strided.v1i64(<vscale x 1 x i64>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_unsigned_1xi64_vstore_strided_unsigned_1xi64(unsigned long int*  addr, signed long stride, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vload_strided_unsigned_1xi64(addr, stride, gvl);
  __builtin_epi_vstore_strided_unsigned_1xi64(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_unsigned_8xi8_vstore_indexed_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.indexed.v8i8.v8i8(<vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v8i8.v8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_unsigned_8xi8_vstore_indexed_unsigned_8xi8(unsigned char*  addr, unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 index;
  result = __builtin_epi_vload_indexed_unsigned_8xi8(addr, index, gvl);
  __builtin_epi_vstore_indexed_unsigned_8xi8(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_unsigned_4xi16_vstore_indexed_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.indexed.v4i16.v4i16(<vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v4i16.v4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_unsigned_4xi16_vstore_indexed_unsigned_4xi16(unsigned short int*  addr, unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 index;
  result = __builtin_epi_vload_indexed_unsigned_4xi16(addr, index, gvl);
  __builtin_epi_vstore_indexed_unsigned_4xi16(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_unsigned_2xi32_vstore_indexed_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.indexed.v2i32.v2i32(<vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v2i32.v2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_unsigned_2xi32_vstore_indexed_unsigned_2xi32(unsigned int*  addr, unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 index;
  result = __builtin_epi_vload_indexed_unsigned_2xi32(addr, index, gvl);
  __builtin_epi_vstore_indexed_unsigned_2xi32(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_unsigned_1xi64_vstore_indexed_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.indexed.v1i64.v1i64(<vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v1i64.v1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_unsigned_1xi64_vstore_indexed_unsigned_1xi64(unsigned long int*  addr, unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 index;
  result = __builtin_epi_vload_indexed_unsigned_1xi64(addr, index, gvl);
  __builtin_epi_vstore_indexed_unsigned_1xi64(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_2xf32_vstore_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ADDR:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vload.v2f32(<vscale x 2 x float>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP1]], <vscale x 2 x float>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_2xf32_vstore_2xf32(float*  addr, unsigned long gvl)
{
  __epi_2xf32 result;
  result = __builtin_epi_vload_2xf32(addr, gvl);
  __builtin_epi_vstore_2xf32(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_1xf64_vstore_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ADDR:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vload.v1f64(<vscale x 1 x double>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP1]], <vscale x 1 x double>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_1xf64_vstore_1xf64(double*  addr, unsigned long gvl)
{
  __epi_1xf64 result;
  result = __builtin_epi_vload_1xf64(addr, gvl);
  __builtin_epi_vstore_1xf64(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_2xf64_vstore_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ADDR:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x double> @llvm.epi.vload.v2f64(<vscale x 2 x double>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f64(<vscale x 2 x double> [[TMP1]], <vscale x 2 x double>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_2xf64_vstore_2xf64(double*  addr, unsigned long gvl)
{
  __epi_2xf64 result;
  result = __builtin_epi_vload_2xf64(addr, gvl);
  __builtin_epi_vstore_2xf64(addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_2xf32_vstore_strided_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ADDR:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vload.strided.v2f32(<vscale x 2 x float>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v2f32(<vscale x 2 x float> [[TMP1]], <vscale x 2 x float>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_2xf32_vstore_strided_2xf32(float*  addr, signed long stride, unsigned long gvl)
{
  __epi_2xf32 result;
  result = __builtin_epi_vload_strided_2xf32(addr, stride, gvl);
  __builtin_epi_vstore_strided_2xf32(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_1xf64_vstore_strided_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ADDR:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vload.strided.v1f64(<vscale x 1 x double>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.v1f64(<vscale x 1 x double> [[TMP1]], <vscale x 1 x double>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_1xf64_vstore_strided_1xf64(double*  addr, signed long stride, unsigned long gvl)
{
  __epi_1xf64 result;
  result = __builtin_epi_vload_strided_1xf64(addr, stride, gvl);
  __builtin_epi_vstore_strided_1xf64(addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_2xf32_vstore_indexed_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ADDR:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vload.indexed.v2f32.v2i32(<vscale x 2 x float>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v2f32.v2i32(<vscale x 2 x float> [[TMP1]], <vscale x 2 x float>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_2xf32_vstore_indexed_2xf32(float*  addr, unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xi32 index;
  result = __builtin_epi_vload_indexed_2xf32(addr, index, gvl);
  __builtin_epi_vstore_indexed_2xf32(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_1xf64_vstore_indexed_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ADDR:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.v1f64.v1i64(<vscale x 1 x double>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.v1f64.v1i64(<vscale x 1 x double> [[TMP1]], <vscale x 1 x double>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_1xf64_vstore_indexed_1xf64(double*  addr, unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xi64 index;
  result = __builtin_epi_vload_indexed_1xf64(addr, index, gvl);
  __builtin_epi_vstore_indexed_1xf64(addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_1xi1_vstore_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    ret void
//
void test_vload_1xi1_vstore_1xi1(unsigned long int*  addr)
{
  __epi_1xi1 result;
  result = __builtin_epi_vload_1xi1(addr);
  __builtin_epi_vstore_1xi1(addr, result);
}


// CHECK-O2-LABEL: @test_vload_2xi1_vstore_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    ret void
//
void test_vload_2xi1_vstore_2xi1(unsigned int*  addr)
{
  __epi_2xi1 result;
  result = __builtin_epi_vload_2xi1(addr);
  __builtin_epi_vstore_2xi1(addr, result);
}


// CHECK-O2-LABEL: @test_vload_4xi1_vstore_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    ret void
//
void test_vload_4xi1_vstore_4xi1(unsigned short int*  addr)
{
  __epi_4xi1 result;
  result = __builtin_epi_vload_4xi1(addr);
  __builtin_epi_vstore_4xi1(addr, result);
}


// CHECK-O2-LABEL: @test_vload_8xi1_vstore_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    ret void
//
void test_vload_8xi1_vstore_8xi1(unsigned char*  addr)
{
  __epi_8xi1 result;
  result = __builtin_epi_vload_8xi1(addr);
  __builtin_epi_vstore_8xi1(addr, result);
}


signed char* p0;
// CHECK-O2-LABEL: @test_vadd_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vadd.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p0 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vadd_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p0, result, gvl);
}


short* p1;
// CHECK-O2-LABEL: @test_vadd_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vadd.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p1 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vadd_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p1, result, gvl);
}


int* p2;
// CHECK-O2-LABEL: @test_vadd_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vadd.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p2 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vadd_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p2, result, gvl);
}


long* p3;
// CHECK-O2-LABEL: @test_vadd_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vadd.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p3 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vadd_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p3, result, gvl);
}


signed char* p4;
// CHECK-O2-LABEL: @test_vadd_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vadd.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p4 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vadd_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p4, result, gvl);
}


short* p5;
// CHECK-O2-LABEL: @test_vadd_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vadd.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p5 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vadd_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p5, result, gvl);
}


int* p6;
// CHECK-O2-LABEL: @test_vadd_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vadd.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p6 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vadd_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p6, result, gvl);
}


long* p7;
// CHECK-O2-LABEL: @test_vadd_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vadd.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p7 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vadd_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p7, result, gvl);
}


signed char* p8;
// CHECK-O2-LABEL: @test_vsub_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsub.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p8 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsub_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p8, result, gvl);
}


short* p9;
// CHECK-O2-LABEL: @test_vsub_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsub.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p9 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsub_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p9, result, gvl);
}


int* p10;
// CHECK-O2-LABEL: @test_vsub_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsub.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p10 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsub_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p10, result, gvl);
}


long* p11;
// CHECK-O2-LABEL: @test_vsub_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsub.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p11 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsub_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p11, result, gvl);
}


signed char* p12;
// CHECK-O2-LABEL: @test_vsub_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsub.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p12 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsub_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p12, result, gvl);
}


short* p13;
// CHECK-O2-LABEL: @test_vsub_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsub.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p13 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsub_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p13, result, gvl);
}


int* p14;
// CHECK-O2-LABEL: @test_vsub_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsub.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p14 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsub_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p14, result, gvl);
}


long* p15;
// CHECK-O2-LABEL: @test_vsub_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsub.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p15 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsub_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p15, result, gvl);
}


signed char* p16;
// CHECK-O2-LABEL: @test_vrsub_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrsub.v8i8.i8(<vscale x 8 x i8> undef, i8 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p16 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  signed char rhs;
  result = __builtin_epi_vrsub_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p16, result, gvl);
}


short* p17;
// CHECK-O2-LABEL: @test_vrsub_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrsub.v4i16.i16(<vscale x 4 x i16> undef, i16 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p17 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  signed short int rhs;
  result = __builtin_epi_vrsub_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p17, result, gvl);
}


int* p18;
// CHECK-O2-LABEL: @test_vrsub_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrsub.v2i32.i32(<vscale x 2 x i32> undef, i32 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p18 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  signed int rhs;
  result = __builtin_epi_vrsub_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p18, result, gvl);
}


long* p19;
// CHECK-O2-LABEL: @test_vrsub_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrsub.v1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p19 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  signed long int rhs;
  result = __builtin_epi_vrsub_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p19, result, gvl);
}


signed char* p20;
// CHECK-O2-LABEL: @test_vrsub_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrsub.mask.v8i8.i8.v8i1(<vscale x 8 x i8> undef, i8 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p20 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  signed char rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vrsub_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p20, result, gvl);
}


short* p21;
// CHECK-O2-LABEL: @test_vrsub_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrsub.mask.v4i16.i16.v4i1(<vscale x 4 x i16> undef, i16 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p21 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  signed short int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vrsub_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p21, result, gvl);
}


int* p22;
// CHECK-O2-LABEL: @test_vrsub_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrsub.mask.v2i32.i32.v2i1(<vscale x 2 x i32> undef, i32 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p22 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  signed int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vrsub_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p22, result, gvl);
}


long* p23;
// CHECK-O2-LABEL: @test_vrsub_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrsub.mask.v1i64.i64.v1i1(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p23 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  signed long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vrsub_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p23, result, gvl);
}


signed char* p24;
// CHECK-O2-LABEL: @test_vand_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vand.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p24 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vand_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p24, result, gvl);
}


short* p25;
// CHECK-O2-LABEL: @test_vand_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vand.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p25 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vand_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p25, result, gvl);
}


int* p26;
// CHECK-O2-LABEL: @test_vand_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vand.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p26 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vand_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p26, result, gvl);
}


long* p27;
// CHECK-O2-LABEL: @test_vand_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vand.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p27 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vand_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p27, result, gvl);
}


signed char* p28;
// CHECK-O2-LABEL: @test_vand_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vand.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p28 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vand_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p28, result, gvl);
}


short* p29;
// CHECK-O2-LABEL: @test_vand_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vand.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p29 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vand_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p29, result, gvl);
}


int* p30;
// CHECK-O2-LABEL: @test_vand_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vand.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p30 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vand_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p30, result, gvl);
}


long* p31;
// CHECK-O2-LABEL: @test_vand_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vand.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p31 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vand_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p31, result, gvl);
}


signed char* p32;
// CHECK-O2-LABEL: @test_vor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vor.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p32 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vor_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p32, result, gvl);
}


short* p33;
// CHECK-O2-LABEL: @test_vor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vor.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p33 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vor_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p33, result, gvl);
}


int* p34;
// CHECK-O2-LABEL: @test_vor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vor.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p34 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vor_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p34, result, gvl);
}


long* p35;
// CHECK-O2-LABEL: @test_vor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vor.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p35 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vor_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p35, result, gvl);
}


signed char* p36;
// CHECK-O2-LABEL: @test_vor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vor.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p36 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vor_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p36, result, gvl);
}


short* p37;
// CHECK-O2-LABEL: @test_vor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vor.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p37 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vor_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p37, result, gvl);
}


int* p38;
// CHECK-O2-LABEL: @test_vor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vor.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p38 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vor_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p38, result, gvl);
}


long* p39;
// CHECK-O2-LABEL: @test_vor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vor.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p39 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vor_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p39, result, gvl);
}


signed char* p40;
// CHECK-O2-LABEL: @test_vxor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vxor.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p40 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vxor_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p40, result, gvl);
}


short* p41;
// CHECK-O2-LABEL: @test_vxor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vxor.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p41 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vxor_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p41, result, gvl);
}


int* p42;
// CHECK-O2-LABEL: @test_vxor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vxor.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p42 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vxor_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p42, result, gvl);
}


long* p43;
// CHECK-O2-LABEL: @test_vxor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vxor.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p43 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vxor_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p43, result, gvl);
}


signed char* p44;
// CHECK-O2-LABEL: @test_vxor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vxor.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p44 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vxor_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p44, result, gvl);
}


short* p45;
// CHECK-O2-LABEL: @test_vxor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vxor.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p45 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vxor_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p45, result, gvl);
}


int* p46;
// CHECK-O2-LABEL: @test_vxor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vxor.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p46 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vxor_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p46, result, gvl);
}


long* p47;
// CHECK-O2-LABEL: @test_vxor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vxor.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p47 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vxor_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p47, result, gvl);
}


signed char* p48;
// CHECK-O2-LABEL: @test_vsll_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsll.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p48 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsll_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p48, result, gvl);
}


short* p49;
// CHECK-O2-LABEL: @test_vsll_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsll.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p49 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsll_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p49, result, gvl);
}


int* p50;
// CHECK-O2-LABEL: @test_vsll_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsll.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p50 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsll_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p50, result, gvl);
}


long* p51;
// CHECK-O2-LABEL: @test_vsll_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsll.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p51 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsll_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p51, result, gvl);
}


signed char* p52;
// CHECK-O2-LABEL: @test_vsll_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsll.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p52 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsll_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p52, result, gvl);
}


short* p53;
// CHECK-O2-LABEL: @test_vsll_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsll.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p53 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsll_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p53, result, gvl);
}


int* p54;
// CHECK-O2-LABEL: @test_vsll_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsll.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p54 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsll_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p54, result, gvl);
}


long* p55;
// CHECK-O2-LABEL: @test_vsll_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsll.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p55 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsll_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p55, result, gvl);
}


signed char* p56;
// CHECK-O2-LABEL: @test_vsrl_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsrl.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p56 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsrl_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p56, result, gvl);
}


short* p57;
// CHECK-O2-LABEL: @test_vsrl_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsrl.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p57 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsrl_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p57, result, gvl);
}


int* p58;
// CHECK-O2-LABEL: @test_vsrl_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsrl.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p58 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsrl_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p58, result, gvl);
}


long* p59;
// CHECK-O2-LABEL: @test_vsrl_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsrl.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p59 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsrl_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p59, result, gvl);
}


signed char* p60;
// CHECK-O2-LABEL: @test_vsrl_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsrl.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p60 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsrl_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p60, result, gvl);
}


short* p61;
// CHECK-O2-LABEL: @test_vsrl_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsrl.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p61 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsrl_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p61, result, gvl);
}


int* p62;
// CHECK-O2-LABEL: @test_vsrl_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsrl.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p62 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsrl_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p62, result, gvl);
}


long* p63;
// CHECK-O2-LABEL: @test_vsrl_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsrl.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p63 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsrl_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p63, result, gvl);
}


signed char* p64;
// CHECK-O2-LABEL: @test_vsra_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsra.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p64 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsra_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p64, result, gvl);
}


short* p65;
// CHECK-O2-LABEL: @test_vsra_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsra.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p65 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsra_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p65, result, gvl);
}


int* p66;
// CHECK-O2-LABEL: @test_vsra_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsra.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p66 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsra_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p66, result, gvl);
}


long* p67;
// CHECK-O2-LABEL: @test_vsra_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsra.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p67 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsra_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p67, result, gvl);
}


signed char* p68;
// CHECK-O2-LABEL: @test_vsra_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsra.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p68 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsra_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p68, result, gvl);
}


short* p69;
// CHECK-O2-LABEL: @test_vsra_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsra.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p69 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsra_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p69, result, gvl);
}


int* p70;
// CHECK-O2-LABEL: @test_vsra_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsra.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p70 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsra_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p70, result, gvl);
}


long* p71;
// CHECK-O2-LABEL: @test_vsra_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsra.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p71 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsra_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p71, result, gvl);
}


unsigned char* p72;
// CHECK-O2-LABEL: @test_vseq_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vseq.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p72 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vseq_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vseq_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p72, result);
}


unsigned short* p73;
// CHECK-O2-LABEL: @test_vseq_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vseq.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p73 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vseq_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vseq_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p73, result);
}


unsigned int* p74;
// CHECK-O2-LABEL: @test_vseq_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vseq.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p74 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vseq_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vseq_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p74, result);
}


unsigned long* p75;
// CHECK-O2-LABEL: @test_vseq_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vseq.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p75 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vseq_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vseq_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p75, result);
}


unsigned char* p76;
// CHECK-O2-LABEL: @test_vseq_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vseq.mask.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p76 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vseq_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vseq_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p76, result);
}


unsigned short* p77;
// CHECK-O2-LABEL: @test_vseq_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vseq.mask.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p77 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vseq_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vseq_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p77, result);
}


unsigned int* p78;
// CHECK-O2-LABEL: @test_vseq_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vseq.mask.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p78 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vseq_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vseq_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p78, result);
}


unsigned long* p79;
// CHECK-O2-LABEL: @test_vseq_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vseq.mask.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p79 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vseq_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vseq_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p79, result);
}


unsigned char* p80;
// CHECK-O2-LABEL: @test_vsne_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsne.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p80 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsne_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsne_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p80, result);
}


unsigned short* p81;
// CHECK-O2-LABEL: @test_vsne_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsne.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p81 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsne_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsne_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p81, result);
}


unsigned int* p82;
// CHECK-O2-LABEL: @test_vsne_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsne.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p82 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsne_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsne_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p82, result);
}


unsigned long* p83;
// CHECK-O2-LABEL: @test_vsne_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsne.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p83 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsne_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsne_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p83, result);
}


unsigned char* p84;
// CHECK-O2-LABEL: @test_vsne_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsne.mask.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p84 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsne_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsne_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p84, result);
}


unsigned short* p85;
// CHECK-O2-LABEL: @test_vsne_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsne.mask.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p85 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsne_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsne_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p85, result);
}


unsigned int* p86;
// CHECK-O2-LABEL: @test_vsne_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsne.mask.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p86 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsne_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsne_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p86, result);
}


unsigned long* p87;
// CHECK-O2-LABEL: @test_vsne_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsne.mask.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p87 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsne_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsne_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p87, result);
}


unsigned char* p88;
// CHECK-O2-LABEL: @test_vsltu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsltu.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p88 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsltu_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsltu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p88, result);
}


unsigned short* p89;
// CHECK-O2-LABEL: @test_vsltu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsltu.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p89 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsltu_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsltu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p89, result);
}


unsigned int* p90;
// CHECK-O2-LABEL: @test_vsltu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsltu.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p90 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsltu_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsltu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p90, result);
}


unsigned long* p91;
// CHECK-O2-LABEL: @test_vsltu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsltu.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p91 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsltu_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsltu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p91, result);
}


unsigned char* p92;
// CHECK-O2-LABEL: @test_vsltu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsltu.mask.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p92 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsltu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsltu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p92, result);
}


unsigned short* p93;
// CHECK-O2-LABEL: @test_vsltu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsltu.mask.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p93 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsltu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsltu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p93, result);
}


unsigned int* p94;
// CHECK-O2-LABEL: @test_vsltu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsltu.mask.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p94 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsltu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsltu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p94, result);
}


unsigned long* p95;
// CHECK-O2-LABEL: @test_vsltu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsltu.mask.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p95 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsltu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsltu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p95, result);
}


unsigned char* p96;
// CHECK-O2-LABEL: @test_vslt_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vslt.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p96 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vslt_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vslt_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p96, result);
}


unsigned short* p97;
// CHECK-O2-LABEL: @test_vslt_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vslt.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p97 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vslt_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vslt_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p97, result);
}


unsigned int* p98;
// CHECK-O2-LABEL: @test_vslt_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vslt.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p98 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vslt_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vslt_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p98, result);
}


unsigned long* p99;
// CHECK-O2-LABEL: @test_vslt_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vslt.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p99 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vslt_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vslt_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p99, result);
}


unsigned char* p100;
// CHECK-O2-LABEL: @test_vslt_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vslt.mask.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p100 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vslt_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vslt_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p100, result);
}


unsigned short* p101;
// CHECK-O2-LABEL: @test_vslt_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vslt.mask.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p101 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vslt_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vslt_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p101, result);
}


unsigned int* p102;
// CHECK-O2-LABEL: @test_vslt_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vslt.mask.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p102 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vslt_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslt_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p102, result);
}


unsigned long* p103;
// CHECK-O2-LABEL: @test_vslt_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vslt.mask.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p103 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vslt_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslt_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p103, result);
}


unsigned char* p104;
// CHECK-O2-LABEL: @test_vsleu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsleu.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p104 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsleu_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsleu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p104, result);
}


unsigned short* p105;
// CHECK-O2-LABEL: @test_vsleu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsleu.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p105 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsleu_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsleu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p105, result);
}


unsigned int* p106;
// CHECK-O2-LABEL: @test_vsleu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsleu.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p106 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsleu_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsleu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p106, result);
}


unsigned long* p107;
// CHECK-O2-LABEL: @test_vsleu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsleu.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p107 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsleu_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsleu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p107, result);
}


unsigned char* p108;
// CHECK-O2-LABEL: @test_vsleu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsleu.mask.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p108 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsleu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsleu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p108, result);
}


unsigned short* p109;
// CHECK-O2-LABEL: @test_vsleu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsleu.mask.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p109 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsleu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsleu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p109, result);
}


unsigned int* p110;
// CHECK-O2-LABEL: @test_vsleu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsleu.mask.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p110 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsleu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsleu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p110, result);
}


unsigned long* p111;
// CHECK-O2-LABEL: @test_vsleu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsleu.mask.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p111 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsleu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsleu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p111, result);
}


unsigned char* p112;
// CHECK-O2-LABEL: @test_vsle_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsle.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p112 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsle_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsle_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p112, result);
}


unsigned short* p113;
// CHECK-O2-LABEL: @test_vsle_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsle.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p113 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsle_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsle_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p113, result);
}


unsigned int* p114;
// CHECK-O2-LABEL: @test_vsle_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsle.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p114 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsle_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsle_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p114, result);
}


unsigned long* p115;
// CHECK-O2-LABEL: @test_vsle_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsle.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p115 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsle_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsle_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p115, result);
}


unsigned char* p116;
// CHECK-O2-LABEL: @test_vsle_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsle.mask.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p116 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsle_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsle_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p116, result);
}


unsigned short* p117;
// CHECK-O2-LABEL: @test_vsle_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsle.mask.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p117 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsle_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsle_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p117, result);
}


unsigned int* p118;
// CHECK-O2-LABEL: @test_vsle_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsle.mask.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p118 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsle_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsle_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p118, result);
}


unsigned long* p119;
// CHECK-O2-LABEL: @test_vsle_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsle.mask.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p119 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsle_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsle_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p119, result);
}


unsigned char* p120;
// CHECK-O2-LABEL: @test_vsgtu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsgtu.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p120 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsgtu_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsgtu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p120, result);
}


unsigned short* p121;
// CHECK-O2-LABEL: @test_vsgtu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsgtu.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p121 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsgtu_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsgtu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p121, result);
}


unsigned int* p122;
// CHECK-O2-LABEL: @test_vsgtu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsgtu.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p122 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsgtu_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsgtu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p122, result);
}


unsigned long* p123;
// CHECK-O2-LABEL: @test_vsgtu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsgtu.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p123 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsgtu_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsgtu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p123, result);
}


unsigned char* p124;
// CHECK-O2-LABEL: @test_vsgtu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsgtu.mask.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p124 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsgtu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsgtu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p124, result);
}


unsigned short* p125;
// CHECK-O2-LABEL: @test_vsgtu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsgtu.mask.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p125 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsgtu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsgtu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p125, result);
}


unsigned int* p126;
// CHECK-O2-LABEL: @test_vsgtu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsgtu.mask.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p126 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsgtu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsgtu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p126, result);
}


unsigned long* p127;
// CHECK-O2-LABEL: @test_vsgtu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsgtu.mask.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p127 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsgtu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsgtu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p127, result);
}


unsigned char* p128;
// CHECK-O2-LABEL: @test_vsgt_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsgt.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p128 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsgt_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsgt_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p128, result);
}


unsigned short* p129;
// CHECK-O2-LABEL: @test_vsgt_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsgt.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p129 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsgt_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsgt_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p129, result);
}


unsigned int* p130;
// CHECK-O2-LABEL: @test_vsgt_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsgt.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p130 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsgt_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsgt_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p130, result);
}


unsigned long* p131;
// CHECK-O2-LABEL: @test_vsgt_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsgt.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p131 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsgt_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsgt_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p131, result);
}


unsigned char* p132;
// CHECK-O2-LABEL: @test_vsgt_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vsgt.mask.v8i1.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p132 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vsgt_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsgt_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p132, result);
}


unsigned short* p133;
// CHECK-O2-LABEL: @test_vsgt_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vsgt.mask.v4i1.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p133 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vsgt_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsgt_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p133, result);
}


unsigned int* p134;
// CHECK-O2-LABEL: @test_vsgt_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vsgt.mask.v2i1.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p134 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vsgt_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsgt_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p134, result);
}


unsigned long* p135;
// CHECK-O2-LABEL: @test_vsgt_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vsgt.mask.v1i1.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p135 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vsgt_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsgt_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p135, result);
}


signed char* p136;
// CHECK-O2-LABEL: @test_vminu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vminu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p136 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vminu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p136, result, gvl);
}


short* p137;
// CHECK-O2-LABEL: @test_vminu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vminu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p137 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vminu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p137, result, gvl);
}


int* p138;
// CHECK-O2-LABEL: @test_vminu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vminu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p138 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vminu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p138, result, gvl);
}


long* p139;
// CHECK-O2-LABEL: @test_vminu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vminu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p139 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vminu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p139, result, gvl);
}


signed char* p140;
// CHECK-O2-LABEL: @test_vminu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vminu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p140 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vminu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p140, result, gvl);
}


short* p141;
// CHECK-O2-LABEL: @test_vminu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vminu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p141 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vminu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p141, result, gvl);
}


int* p142;
// CHECK-O2-LABEL: @test_vminu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vminu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p142 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vminu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p142, result, gvl);
}


long* p143;
// CHECK-O2-LABEL: @test_vminu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vminu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p143 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vminu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p143, result, gvl);
}


signed char* p144;
// CHECK-O2-LABEL: @test_vmin_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmin.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p144 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmin_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p144, result, gvl);
}


short* p145;
// CHECK-O2-LABEL: @test_vmin_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmin.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p145 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmin_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p145, result, gvl);
}


int* p146;
// CHECK-O2-LABEL: @test_vmin_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmin.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p146 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmin_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p146, result, gvl);
}


long* p147;
// CHECK-O2-LABEL: @test_vmin_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmin.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p147 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmin_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p147, result, gvl);
}


signed char* p148;
// CHECK-O2-LABEL: @test_vmin_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmin.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p148 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmin_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p148, result, gvl);
}


short* p149;
// CHECK-O2-LABEL: @test_vmin_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmin.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p149 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmin_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p149, result, gvl);
}


int* p150;
// CHECK-O2-LABEL: @test_vmin_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmin.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p150 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmin_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p150, result, gvl);
}


long* p151;
// CHECK-O2-LABEL: @test_vmin_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmin.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p151 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmin_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p151, result, gvl);
}


signed char* p152;
// CHECK-O2-LABEL: @test_vmaxu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmaxu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p152 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmaxu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p152, result, gvl);
}


short* p153;
// CHECK-O2-LABEL: @test_vmaxu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmaxu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p153 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmaxu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p153, result, gvl);
}


int* p154;
// CHECK-O2-LABEL: @test_vmaxu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmaxu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p154 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmaxu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p154, result, gvl);
}


long* p155;
// CHECK-O2-LABEL: @test_vmaxu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmaxu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p155 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmaxu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p155, result, gvl);
}


signed char* p156;
// CHECK-O2-LABEL: @test_vmaxu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmaxu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p156 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmaxu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p156, result, gvl);
}


short* p157;
// CHECK-O2-LABEL: @test_vmaxu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmaxu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p157 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmaxu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p157, result, gvl);
}


int* p158;
// CHECK-O2-LABEL: @test_vmaxu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmaxu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p158 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmaxu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p158, result, gvl);
}


long* p159;
// CHECK-O2-LABEL: @test_vmaxu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmaxu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p159 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmaxu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p159, result, gvl);
}


signed char* p160;
// CHECK-O2-LABEL: @test_vmax_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmax.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p160 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmax_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p160, result, gvl);
}


short* p161;
// CHECK-O2-LABEL: @test_vmax_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmax.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p161 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmax_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p161, result, gvl);
}


int* p162;
// CHECK-O2-LABEL: @test_vmax_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmax.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p162 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmax_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p162, result, gvl);
}


long* p163;
// CHECK-O2-LABEL: @test_vmax_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmax.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p163 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmax_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p163, result, gvl);
}


signed char* p164;
// CHECK-O2-LABEL: @test_vmax_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmax.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p164 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmax_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p164, result, gvl);
}


short* p165;
// CHECK-O2-LABEL: @test_vmax_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmax.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p165 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmax_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p165, result, gvl);
}


int* p166;
// CHECK-O2-LABEL: @test_vmax_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmax.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p166 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmax_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p166, result, gvl);
}


long* p167;
// CHECK-O2-LABEL: @test_vmax_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmax.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p167 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmax_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p167, result, gvl);
}


signed char* p168;
// CHECK-O2-LABEL: @test_vmul_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmul.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p168 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmul_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p168, result, gvl);
}


short* p169;
// CHECK-O2-LABEL: @test_vmul_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmul.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p169 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmul_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p169, result, gvl);
}


int* p170;
// CHECK-O2-LABEL: @test_vmul_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmul.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p170 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmul_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p170, result, gvl);
}


long* p171;
// CHECK-O2-LABEL: @test_vmul_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmul.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p171 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmul_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p171, result, gvl);
}


signed char* p172;
// CHECK-O2-LABEL: @test_vmul_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmul.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p172 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmul_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p172, result, gvl);
}


short* p173;
// CHECK-O2-LABEL: @test_vmul_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmul.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p173 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmul_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p173, result, gvl);
}


int* p174;
// CHECK-O2-LABEL: @test_vmul_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmul.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p174 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmul_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p174, result, gvl);
}


long* p175;
// CHECK-O2-LABEL: @test_vmul_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmul.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p175 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmul_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p175, result, gvl);
}


signed char* p176;
// CHECK-O2-LABEL: @test_vmulh_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulh.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p176 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmulh_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p176, result, gvl);
}


short* p177;
// CHECK-O2-LABEL: @test_vmulh_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulh.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p177 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmulh_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p177, result, gvl);
}


int* p178;
// CHECK-O2-LABEL: @test_vmulh_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulh.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p178 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmulh_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p178, result, gvl);
}


long* p179;
// CHECK-O2-LABEL: @test_vmulh_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulh.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p179 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmulh_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p179, result, gvl);
}


signed char* p180;
// CHECK-O2-LABEL: @test_vmulh_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulh.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p180 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmulh_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p180, result, gvl);
}


short* p181;
// CHECK-O2-LABEL: @test_vmulh_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulh.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p181 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmulh_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p181, result, gvl);
}


int* p182;
// CHECK-O2-LABEL: @test_vmulh_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulh.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p182 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmulh_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p182, result, gvl);
}


long* p183;
// CHECK-O2-LABEL: @test_vmulh_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulh.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p183 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmulh_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p183, result, gvl);
}


signed char* p184;
// CHECK-O2-LABEL: @test_vmulhu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulhu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p184 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmulhu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p184, result, gvl);
}


short* p185;
// CHECK-O2-LABEL: @test_vmulhu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulhu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p185 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmulhu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p185, result, gvl);
}


int* p186;
// CHECK-O2-LABEL: @test_vmulhu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulhu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p186 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmulhu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p186, result, gvl);
}


long* p187;
// CHECK-O2-LABEL: @test_vmulhu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulhu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p187 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmulhu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p187, result, gvl);
}


signed char* p188;
// CHECK-O2-LABEL: @test_vmulhu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulhu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p188 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmulhu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p188, result, gvl);
}


short* p189;
// CHECK-O2-LABEL: @test_vmulhu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulhu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p189 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmulhu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p189, result, gvl);
}


int* p190;
// CHECK-O2-LABEL: @test_vmulhu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulhu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p190 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmulhu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p190, result, gvl);
}


long* p191;
// CHECK-O2-LABEL: @test_vmulhu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulhu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p191 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmulhu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p191, result, gvl);
}


signed char* p192;
// CHECK-O2-LABEL: @test_vmulhsu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulhsu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p192 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmulhsu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p192, result, gvl);
}


short* p193;
// CHECK-O2-LABEL: @test_vmulhsu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulhsu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p193 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmulhsu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p193, result, gvl);
}


int* p194;
// CHECK-O2-LABEL: @test_vmulhsu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulhsu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p194 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmulhsu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p194, result, gvl);
}


long* p195;
// CHECK-O2-LABEL: @test_vmulhsu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulhsu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p195 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmulhsu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p195, result, gvl);
}


signed char* p196;
// CHECK-O2-LABEL: @test_vmulhsu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p196 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmulhsu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p196, result, gvl);
}


short* p197;
// CHECK-O2-LABEL: @test_vmulhsu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p197 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmulhsu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p197, result, gvl);
}


int* p198;
// CHECK-O2-LABEL: @test_vmulhsu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p198 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmulhsu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p198, result, gvl);
}


long* p199;
// CHECK-O2-LABEL: @test_vmulhsu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p199 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmulhsu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p199, result, gvl);
}


signed char* p200;
// CHECK-O2-LABEL: @test_vdivu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdivu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p200 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vdivu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p200, result, gvl);
}


short* p201;
// CHECK-O2-LABEL: @test_vdivu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdivu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p201 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vdivu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p201, result, gvl);
}


int* p202;
// CHECK-O2-LABEL: @test_vdivu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdivu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p202 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vdivu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p202, result, gvl);
}


long* p203;
// CHECK-O2-LABEL: @test_vdivu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdivu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p203 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vdivu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p203, result, gvl);
}


signed char* p204;
// CHECK-O2-LABEL: @test_vdivu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdivu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p204 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vdivu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p204, result, gvl);
}


short* p205;
// CHECK-O2-LABEL: @test_vdivu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdivu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p205 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vdivu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p205, result, gvl);
}


int* p206;
// CHECK-O2-LABEL: @test_vdivu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdivu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p206 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vdivu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p206, result, gvl);
}


long* p207;
// CHECK-O2-LABEL: @test_vdivu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdivu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p207 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vdivu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p207, result, gvl);
}


signed char* p208;
// CHECK-O2-LABEL: @test_vdiv_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdiv.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p208 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vdiv_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p208, result, gvl);
}


short* p209;
// CHECK-O2-LABEL: @test_vdiv_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdiv.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p209 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vdiv_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p209, result, gvl);
}


int* p210;
// CHECK-O2-LABEL: @test_vdiv_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdiv.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p210 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vdiv_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p210, result, gvl);
}


long* p211;
// CHECK-O2-LABEL: @test_vdiv_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdiv.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p211 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vdiv_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p211, result, gvl);
}


signed char* p212;
// CHECK-O2-LABEL: @test_vdiv_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdiv.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p212 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vdiv_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p212, result, gvl);
}


short* p213;
// CHECK-O2-LABEL: @test_vdiv_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdiv.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p213 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vdiv_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p213, result, gvl);
}


int* p214;
// CHECK-O2-LABEL: @test_vdiv_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdiv.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p214 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vdiv_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p214, result, gvl);
}


long* p215;
// CHECK-O2-LABEL: @test_vdiv_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdiv.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p215 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vdiv_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p215, result, gvl);
}


signed char* p216;
// CHECK-O2-LABEL: @test_vremu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vremu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p216 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vremu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p216, result, gvl);
}


short* p217;
// CHECK-O2-LABEL: @test_vremu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vremu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p217 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vremu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p217, result, gvl);
}


int* p218;
// CHECK-O2-LABEL: @test_vremu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vremu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p218 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vremu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p218, result, gvl);
}


long* p219;
// CHECK-O2-LABEL: @test_vremu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vremu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p219 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vremu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p219, result, gvl);
}


signed char* p220;
// CHECK-O2-LABEL: @test_vremu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vremu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p220 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vremu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p220, result, gvl);
}


short* p221;
// CHECK-O2-LABEL: @test_vremu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vremu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p221 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vremu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p221, result, gvl);
}


int* p222;
// CHECK-O2-LABEL: @test_vremu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vremu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p222 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vremu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p222, result, gvl);
}


long* p223;
// CHECK-O2-LABEL: @test_vremu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vremu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p223 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vremu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p223, result, gvl);
}


signed char* p224;
// CHECK-O2-LABEL: @test_vrem_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrem.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p224 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vrem_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p224, result, gvl);
}


short* p225;
// CHECK-O2-LABEL: @test_vrem_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrem.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p225 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vrem_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p225, result, gvl);
}


int* p226;
// CHECK-O2-LABEL: @test_vrem_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrem.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p226 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vrem_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p226, result, gvl);
}


long* p227;
// CHECK-O2-LABEL: @test_vrem_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrem.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p227 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vrem_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p227, result, gvl);
}


signed char* p228;
// CHECK-O2-LABEL: @test_vrem_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrem.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p228 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vrem_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p228, result, gvl);
}


short* p229;
// CHECK-O2-LABEL: @test_vrem_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrem.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p229 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vrem_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p229, result, gvl);
}


int* p230;
// CHECK-O2-LABEL: @test_vrem_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrem.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p230 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vrem_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p230, result, gvl);
}


long* p231;
// CHECK-O2-LABEL: @test_vrem_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrem.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p231 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vrem_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p231, result, gvl);
}


signed char* p232;
// CHECK-O2-LABEL: @test_vmerge_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmerge.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p232 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmerge_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p232, result, gvl);
}


short* p233;
// CHECK-O2-LABEL: @test_vmerge_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmerge.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p233 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmerge_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p233, result, gvl);
}


int* p234;
// CHECK-O2-LABEL: @test_vmerge_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmerge.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p234 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmerge_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p234, result, gvl);
}


long* p235;
// CHECK-O2-LABEL: @test_vmerge_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmerge.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p235 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmerge_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p235, result, gvl);
}


signed char* p236;
// CHECK-O2-LABEL: @test_vmerge_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmerge.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p236 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmerge_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p236, result, gvl);
}


short* p237;
// CHECK-O2-LABEL: @test_vmerge_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmerge.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p237 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmerge_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p237, result, gvl);
}


int* p238;
// CHECK-O2-LABEL: @test_vmerge_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmerge.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p238 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmerge_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p238, result, gvl);
}


long* p239;
// CHECK-O2-LABEL: @test_vmerge_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmerge.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p239 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmerge_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p239, result, gvl);
}


signed char* p240;
// CHECK-O2-LABEL: @test_vsaddu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsaddu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p240 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsaddu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p240, result, gvl);
}


short* p241;
// CHECK-O2-LABEL: @test_vsaddu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsaddu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p241 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsaddu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p241, result, gvl);
}


int* p242;
// CHECK-O2-LABEL: @test_vsaddu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsaddu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p242 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsaddu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p242, result, gvl);
}


long* p243;
// CHECK-O2-LABEL: @test_vsaddu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsaddu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p243 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsaddu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p243, result, gvl);
}


signed char* p244;
// CHECK-O2-LABEL: @test_vsaddu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsaddu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p244 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsaddu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p244, result, gvl);
}


short* p245;
// CHECK-O2-LABEL: @test_vsaddu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsaddu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p245 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsaddu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p245, result, gvl);
}


int* p246;
// CHECK-O2-LABEL: @test_vsaddu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsaddu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p246 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsaddu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p246, result, gvl);
}


long* p247;
// CHECK-O2-LABEL: @test_vsaddu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsaddu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p247 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsaddu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p247, result, gvl);
}


signed char* p248;
// CHECK-O2-LABEL: @test_vsadd_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsadd.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p248 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsadd_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p248, result, gvl);
}


short* p249;
// CHECK-O2-LABEL: @test_vsadd_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsadd.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p249 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsadd_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p249, result, gvl);
}


int* p250;
// CHECK-O2-LABEL: @test_vsadd_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsadd.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p250 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsadd_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p250, result, gvl);
}


long* p251;
// CHECK-O2-LABEL: @test_vsadd_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsadd.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p251 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsadd_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p251, result, gvl);
}


signed char* p252;
// CHECK-O2-LABEL: @test_vsadd_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsadd.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p252 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsadd_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p252, result, gvl);
}


short* p253;
// CHECK-O2-LABEL: @test_vsadd_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsadd.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p253 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsadd_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p253, result, gvl);
}


int* p254;
// CHECK-O2-LABEL: @test_vsadd_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsadd.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p254 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsadd_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p254, result, gvl);
}


long* p255;
// CHECK-O2-LABEL: @test_vsadd_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsadd.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p255 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsadd_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p255, result, gvl);
}


signed char* p256;
// CHECK-O2-LABEL: @test_vssub_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssub.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p256 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vssub_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p256, result, gvl);
}


short* p257;
// CHECK-O2-LABEL: @test_vssub_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssub.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p257 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vssub_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p257, result, gvl);
}


int* p258;
// CHECK-O2-LABEL: @test_vssub_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssub.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p258 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vssub_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p258, result, gvl);
}


long* p259;
// CHECK-O2-LABEL: @test_vssub_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssub.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p259 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vssub_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p259, result, gvl);
}


signed char* p260;
// CHECK-O2-LABEL: @test_vssub_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssub.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p260 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vssub_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p260, result, gvl);
}


short* p261;
// CHECK-O2-LABEL: @test_vssub_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssub.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p261 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vssub_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p261, result, gvl);
}


int* p262;
// CHECK-O2-LABEL: @test_vssub_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssub.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p262 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vssub_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p262, result, gvl);
}


long* p263;
// CHECK-O2-LABEL: @test_vssub_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssub.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p263 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vssub_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p263, result, gvl);
}


signed char* p264;
// CHECK-O2-LABEL: @test_vssubu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssubu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p264 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vssubu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p264, result, gvl);
}


short* p265;
// CHECK-O2-LABEL: @test_vssubu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssubu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p265 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vssubu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p265, result, gvl);
}


int* p266;
// CHECK-O2-LABEL: @test_vssubu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssubu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p266 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vssubu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p266, result, gvl);
}


long* p267;
// CHECK-O2-LABEL: @test_vssubu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssubu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p267 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vssubu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p267, result, gvl);
}


signed char* p268;
// CHECK-O2-LABEL: @test_vssubu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssubu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p268 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vssubu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p268, result, gvl);
}


short* p269;
// CHECK-O2-LABEL: @test_vssubu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssubu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p269 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vssubu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p269, result, gvl);
}


int* p270;
// CHECK-O2-LABEL: @test_vssubu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssubu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p270 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vssubu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p270, result, gvl);
}


long* p271;
// CHECK-O2-LABEL: @test_vssubu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssubu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p271 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vssubu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p271, result, gvl);
}


signed char* p272;
// CHECK-O2-LABEL: @test_vaadd_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vaadd.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p272 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vaadd_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p272, result, gvl);
}


short* p273;
// CHECK-O2-LABEL: @test_vaadd_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vaadd.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p273 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vaadd_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p273, result, gvl);
}


int* p274;
// CHECK-O2-LABEL: @test_vaadd_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vaadd.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p274 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vaadd_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p274, result, gvl);
}


long* p275;
// CHECK-O2-LABEL: @test_vaadd_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vaadd.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p275 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vaadd_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p275, result, gvl);
}


signed char* p276;
// CHECK-O2-LABEL: @test_vaadd_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vaadd.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p276 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vaadd_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p276, result, gvl);
}


short* p277;
// CHECK-O2-LABEL: @test_vaadd_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vaadd.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p277 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vaadd_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p277, result, gvl);
}


int* p278;
// CHECK-O2-LABEL: @test_vaadd_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vaadd.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p278 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vaadd_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p278, result, gvl);
}


long* p279;
// CHECK-O2-LABEL: @test_vaadd_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vaadd.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p279 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vaadd_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p279, result, gvl);
}


signed char* p280;
// CHECK-O2-LABEL: @test_vasub_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vasub.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p280 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vasub_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p280, result, gvl);
}


short* p281;
// CHECK-O2-LABEL: @test_vasub_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vasub.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p281 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vasub_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p281, result, gvl);
}


int* p282;
// CHECK-O2-LABEL: @test_vasub_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vasub.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p282 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vasub_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p282, result, gvl);
}


long* p283;
// CHECK-O2-LABEL: @test_vasub_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vasub.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p283 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vasub_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p283, result, gvl);
}


signed char* p284;
// CHECK-O2-LABEL: @test_vasub_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vasub.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p284 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vasub_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p284, result, gvl);
}


short* p285;
// CHECK-O2-LABEL: @test_vasub_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vasub.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p285 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vasub_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p285, result, gvl);
}


int* p286;
// CHECK-O2-LABEL: @test_vasub_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vasub.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p286 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vasub_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p286, result, gvl);
}


long* p287;
// CHECK-O2-LABEL: @test_vasub_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vasub.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p287 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vasub_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p287, result, gvl);
}


signed char* p288;
// CHECK-O2-LABEL: @test_vsmul_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsmul.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p288 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsmul_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p288, result, gvl);
}


short* p289;
// CHECK-O2-LABEL: @test_vsmul_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsmul.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p289 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsmul_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p289, result, gvl);
}


int* p290;
// CHECK-O2-LABEL: @test_vsmul_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsmul.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p290 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsmul_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p290, result, gvl);
}


long* p291;
// CHECK-O2-LABEL: @test_vsmul_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsmul.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p291 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsmul_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p291, result, gvl);
}


signed char* p292;
// CHECK-O2-LABEL: @test_vsmul_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsmul.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p292 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsmul_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p292, result, gvl);
}


short* p293;
// CHECK-O2-LABEL: @test_vsmul_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsmul.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p293 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsmul_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p293, result, gvl);
}


int* p294;
// CHECK-O2-LABEL: @test_vsmul_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsmul.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p294 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsmul_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p294, result, gvl);
}


long* p295;
// CHECK-O2-LABEL: @test_vsmul_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsmul.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p295 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsmul_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p295, result, gvl);
}


signed char* p296;
// CHECK-O2-LABEL: @test_vssrl_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssrl.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p296 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vssrl_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p296, result, gvl);
}


short* p297;
// CHECK-O2-LABEL: @test_vssrl_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssrl.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p297 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vssrl_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p297, result, gvl);
}


int* p298;
// CHECK-O2-LABEL: @test_vssrl_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssrl.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p298 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vssrl_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p298, result, gvl);
}


long* p299;
// CHECK-O2-LABEL: @test_vssrl_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssrl.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p299 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vssrl_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p299, result, gvl);
}


signed char* p300;
// CHECK-O2-LABEL: @test_vssrl_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssrl.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p300 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vssrl_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p300, result, gvl);
}


short* p301;
// CHECK-O2-LABEL: @test_vssrl_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssrl.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p301 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vssrl_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p301, result, gvl);
}


int* p302;
// CHECK-O2-LABEL: @test_vssrl_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssrl.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p302 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vssrl_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p302, result, gvl);
}


long* p303;
// CHECK-O2-LABEL: @test_vssrl_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssrl.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p303 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vssrl_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p303, result, gvl);
}


signed char* p304;
// CHECK-O2-LABEL: @test_vssra_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssra.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p304 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vssra_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p304, result, gvl);
}


short* p305;
// CHECK-O2-LABEL: @test_vssra_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssra.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p305 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vssra_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p305, result, gvl);
}


int* p306;
// CHECK-O2-LABEL: @test_vssra_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssra.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p306 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vssra_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p306, result, gvl);
}


long* p307;
// CHECK-O2-LABEL: @test_vssra_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssra.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p307 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vssra_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p307, result, gvl);
}


signed char* p308;
// CHECK-O2-LABEL: @test_vssra_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssra.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p308 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vssra_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p308, result, gvl);
}


short* p309;
// CHECK-O2-LABEL: @test_vssra_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssra.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p309 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vssra_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p309, result, gvl);
}


int* p310;
// CHECK-O2-LABEL: @test_vssra_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssra.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p310 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vssra_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p310, result, gvl);
}


long* p311;
// CHECK-O2-LABEL: @test_vssra_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssra.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p311 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vssra_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p311, result, gvl);
}


float* p312;
// CHECK-O2-LABEL: @test_vfadd_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfadd.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p312 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfadd_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfadd_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p312, result, gvl);
}


double* p313;
// CHECK-O2-LABEL: @test_vfadd_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfadd.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p313 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfadd_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfadd_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p313, result, gvl);
}


float* p314;
// CHECK-O2-LABEL: @test_vfadd_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfadd.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p314 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfadd_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfadd_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p314, result, gvl);
}


double* p315;
// CHECK-O2-LABEL: @test_vfadd_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfadd.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p315 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfadd_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfadd_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p315, result, gvl);
}


float* p316;
// CHECK-O2-LABEL: @test_vfsub_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsub.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p316 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsub_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfsub_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p316, result, gvl);
}


double* p317;
// CHECK-O2-LABEL: @test_vfsub_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsub.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p317 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsub_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfsub_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p317, result, gvl);
}


float* p318;
// CHECK-O2-LABEL: @test_vfsub_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsub.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p318 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsub_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsub_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p318, result, gvl);
}


double* p319;
// CHECK-O2-LABEL: @test_vfsub_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsub.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p319 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsub_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsub_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p319, result, gvl);
}


float* p320;
// CHECK-O2-LABEL: @test_vfmul_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmul.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p320 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmul_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfmul_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p320, result, gvl);
}


double* p321;
// CHECK-O2-LABEL: @test_vfmul_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmul.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p321 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmul_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfmul_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p321, result, gvl);
}


float* p322;
// CHECK-O2-LABEL: @test_vfmul_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmul.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p322 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmul_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmul_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p322, result, gvl);
}


double* p323;
// CHECK-O2-LABEL: @test_vfmul_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmul.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p323 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmul_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmul_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p323, result, gvl);
}


float* p324;
// CHECK-O2-LABEL: @test_vfdiv_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfdiv.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p324 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdiv_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfdiv_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p324, result, gvl);
}


double* p325;
// CHECK-O2-LABEL: @test_vfdiv_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfdiv.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p325 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdiv_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfdiv_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p325, result, gvl);
}


float* p326;
// CHECK-O2-LABEL: @test_vfdiv_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfdiv.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p326 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdiv_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfdiv_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p326, result, gvl);
}


double* p327;
// CHECK-O2-LABEL: @test_vfdiv_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfdiv.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p327 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdiv_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfdiv_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p327, result, gvl);
}


float* p328;
// CHECK-O2-LABEL: @test_vfmadd_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmadd.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p328 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmadd_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfmadd_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p328, result, gvl);
}


double* p329;
// CHECK-O2-LABEL: @test_vfmadd_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmadd.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p329 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmadd_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfmadd_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p329, result, gvl);
}


float* p330;
// CHECK-O2-LABEL: @test_vfmadd_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmadd.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p330 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmadd_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmadd_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p330, result, gvl);
}


double* p331;
// CHECK-O2-LABEL: @test_vfmadd_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmadd.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p331 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmadd_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmadd_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p331, result, gvl);
}


float* p332;
// CHECK-O2-LABEL: @test_vfnmadd_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmadd.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p332 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmadd_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfnmadd_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p332, result, gvl);
}


double* p333;
// CHECK-O2-LABEL: @test_vfnmadd_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmadd.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p333 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmadd_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfnmadd_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p333, result, gvl);
}


float* p334;
// CHECK-O2-LABEL: @test_vfnmadd_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmadd.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p334 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmadd_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfnmadd_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p334, result, gvl);
}


double* p335;
// CHECK-O2-LABEL: @test_vfnmadd_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmadd.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p335 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmadd_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfnmadd_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p335, result, gvl);
}


float* p336;
// CHECK-O2-LABEL: @test_vfmsub_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmsub.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p336 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsub_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfmsub_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p336, result, gvl);
}


double* p337;
// CHECK-O2-LABEL: @test_vfmsub_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmsub.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p337 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsub_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfmsub_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p337, result, gvl);
}


float* p338;
// CHECK-O2-LABEL: @test_vfmsub_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmsub.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p338 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsub_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmsub_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p338, result, gvl);
}


double* p339;
// CHECK-O2-LABEL: @test_vfmsub_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmsub.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p339 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsub_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmsub_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p339, result, gvl);
}


float* p340;
// CHECK-O2-LABEL: @test_vfnmsub_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmsub.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p340 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsub_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfnmsub_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p340, result, gvl);
}


double* p341;
// CHECK-O2-LABEL: @test_vfnmsub_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmsub.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p341 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsub_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfnmsub_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p341, result, gvl);
}


float* p342;
// CHECK-O2-LABEL: @test_vfnmsub_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmsub.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p342 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsub_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfnmsub_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p342, result, gvl);
}


double* p343;
// CHECK-O2-LABEL: @test_vfnmsub_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmsub.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p343 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsub_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfnmsub_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p343, result, gvl);
}


float* p344;
// CHECK-O2-LABEL: @test_vfmacc_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmacc.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p344 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmacc_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfmacc_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p344, result, gvl);
}


double* p345;
// CHECK-O2-LABEL: @test_vfmacc_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmacc.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p345 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmacc_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfmacc_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p345, result, gvl);
}


float* p346;
// CHECK-O2-LABEL: @test_vfmacc_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmacc.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p346 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmacc_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmacc_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p346, result, gvl);
}


double* p347;
// CHECK-O2-LABEL: @test_vfmacc_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmacc.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p347 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmacc_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmacc_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p347, result, gvl);
}


float* p348;
// CHECK-O2-LABEL: @test_vfnmacc_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmacc.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p348 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmacc_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfnmacc_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p348, result, gvl);
}


double* p349;
// CHECK-O2-LABEL: @test_vfnmacc_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmacc.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p349 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmacc_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfnmacc_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p349, result, gvl);
}


float* p350;
// CHECK-O2-LABEL: @test_vfnmacc_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmacc.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p350 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmacc_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfnmacc_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p350, result, gvl);
}


double* p351;
// CHECK-O2-LABEL: @test_vfnmacc_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmacc.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p351 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmacc_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfnmacc_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p351, result, gvl);
}


float* p352;
// CHECK-O2-LABEL: @test_vfmsac_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmsac.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p352 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsac_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfmsac_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p352, result, gvl);
}


double* p353;
// CHECK-O2-LABEL: @test_vfmsac_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmsac.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p353 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsac_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfmsac_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p353, result, gvl);
}


float* p354;
// CHECK-O2-LABEL: @test_vfmsac_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmsac.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p354 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsac_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmsac_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p354, result, gvl);
}


double* p355;
// CHECK-O2-LABEL: @test_vfmsac_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmsac.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p355 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsac_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmsac_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p355, result, gvl);
}


float* p356;
// CHECK-O2-LABEL: @test_vfnmsac_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmsac.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p356 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsac_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfnmsac_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p356, result, gvl);
}


double* p357;
// CHECK-O2-LABEL: @test_vfnmsac_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p357 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsac_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfnmsac_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p357, result, gvl);
}


float* p358;
// CHECK-O2-LABEL: @test_vfnmsac_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmsac.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p358 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsac_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfnmsac_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p358, result, gvl);
}


double* p359;
// CHECK-O2-LABEL: @test_vfnmsac_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p359 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsac_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfnmsac_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p359, result, gvl);
}


float* p360;
// CHECK-O2-LABEL: @test_vfsqrt_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsqrt.v2f32.v2f32(<vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p360 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsqrt_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vfsqrt_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xf32(p360, result, gvl);
}


double* p361;
// CHECK-O2-LABEL: @test_vfsqrt_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsqrt.v1f64.v1f64(<vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p361 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsqrt_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  result = __builtin_epi_vfsqrt_1xf64(lhs, gvl);
  __builtin_epi_vstore_1xf64(p361, result, gvl);
}


float* p362;
// CHECK-O2-LABEL: @test_vfsqrt_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsqrt.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p362 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsqrt_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsqrt_2xf32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p362, result, gvl);
}


double* p363;
// CHECK-O2-LABEL: @test_vfsqrt_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsqrt.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p363 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsqrt_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsqrt_1xf64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p363, result, gvl);
}


float* p364;
// CHECK-O2-LABEL: @test_vfmin_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmin.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p364 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmin_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfmin_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p364, result, gvl);
}


double* p365;
// CHECK-O2-LABEL: @test_vfmin_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmin.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p365 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmin_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfmin_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p365, result, gvl);
}


float* p366;
// CHECK-O2-LABEL: @test_vfmin_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmin.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p366 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmin_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmin_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p366, result, gvl);
}


double* p367;
// CHECK-O2-LABEL: @test_vfmin_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmin.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p367 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmin_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmin_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p367, result, gvl);
}


float* p368;
// CHECK-O2-LABEL: @test_vfmax_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmax.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p368 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmax_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfmax_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p368, result, gvl);
}


double* p369;
// CHECK-O2-LABEL: @test_vfmax_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmax.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p369 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmax_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfmax_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p369, result, gvl);
}


float* p370;
// CHECK-O2-LABEL: @test_vfmax_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmax.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p370 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmax_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmax_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p370, result, gvl);
}


double* p371;
// CHECK-O2-LABEL: @test_vfmax_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmax.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p371 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmax_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmax_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p371, result, gvl);
}


float* p372;
// CHECK-O2-LABEL: @test_vfsgnj_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnj.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p372 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnj_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfsgnj_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p372, result, gvl);
}


double* p373;
// CHECK-O2-LABEL: @test_vfsgnj_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnj.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p373 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnj_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfsgnj_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p373, result, gvl);
}


float* p374;
// CHECK-O2-LABEL: @test_vfsgnj_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnj.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p374 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnj_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsgnj_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p374, result, gvl);
}


double* p375;
// CHECK-O2-LABEL: @test_vfsgnj_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnj.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p375 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnj_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsgnj_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p375, result, gvl);
}


float* p376;
// CHECK-O2-LABEL: @test_vfsgnjn_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnjn.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p376 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjn_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfsgnjn_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p376, result, gvl);
}


double* p377;
// CHECK-O2-LABEL: @test_vfsgnjn_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p377 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjn_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfsgnjn_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p377, result, gvl);
}


float* p378;
// CHECK-O2-LABEL: @test_vfsgnjn_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p378 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjn_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsgnjn_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p378, result, gvl);
}


double* p379;
// CHECK-O2-LABEL: @test_vfsgnjn_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p379 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjn_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsgnjn_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p379, result, gvl);
}


float* p380;
// CHECK-O2-LABEL: @test_vfsgnjx_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnjx.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p380 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjx_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfsgnjx_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p380, result, gvl);
}


double* p381;
// CHECK-O2-LABEL: @test_vfsgnjx_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnjx.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p381 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjx_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfsgnjx_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p381, result, gvl);
}


float* p382;
// CHECK-O2-LABEL: @test_vfsgnjx_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p382 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjx_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsgnjx_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p382, result, gvl);
}


double* p383;
// CHECK-O2-LABEL: @test_vfsgnjx_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p383 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjx_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsgnjx_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p383, result, gvl);
}


unsigned int* p384;
// CHECK-O2-LABEL: @test_vfeq_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfeq.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p384 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfeq_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfeq_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p384, result);
}


unsigned long* p385;
// CHECK-O2-LABEL: @test_vfeq_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfeq.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p385 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfeq_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfeq_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p385, result);
}


unsigned int* p386;
// CHECK-O2-LABEL: @test_vfeq_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfeq.mask.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p386 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfeq_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfeq_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p386, result);
}


unsigned long* p387;
// CHECK-O2-LABEL: @test_vfeq_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfeq.mask.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p387 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfeq_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfeq_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p387, result);
}


unsigned int* p388;
// CHECK-O2-LABEL: @test_vfne_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfne.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p388 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfne_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfne_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p388, result);
}


unsigned long* p389;
// CHECK-O2-LABEL: @test_vfne_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfne.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p389 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfne_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfne_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p389, result);
}


unsigned int* p390;
// CHECK-O2-LABEL: @test_vfne_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfne.mask.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p390 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfne_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfne_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p390, result);
}


unsigned long* p391;
// CHECK-O2-LABEL: @test_vfne_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfne.mask.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p391 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfne_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfne_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p391, result);
}


unsigned int* p392;
// CHECK-O2-LABEL: @test_vflt_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vflt.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p392 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vflt_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vflt_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p392, result);
}


unsigned long* p393;
// CHECK-O2-LABEL: @test_vflt_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vflt.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p393 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vflt_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vflt_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p393, result);
}


unsigned int* p394;
// CHECK-O2-LABEL: @test_vflt_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vflt.mask.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p394 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vflt_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vflt_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p394, result);
}


unsigned long* p395;
// CHECK-O2-LABEL: @test_vflt_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vflt.mask.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p395 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vflt_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vflt_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p395, result);
}


unsigned int* p396;
// CHECK-O2-LABEL: @test_vfle_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfle.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p396 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfle_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfle_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p396, result);
}


unsigned long* p397;
// CHECK-O2-LABEL: @test_vfle_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfle.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p397 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfle_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfle_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p397, result);
}


unsigned int* p398;
// CHECK-O2-LABEL: @test_vfle_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfle.mask.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p398 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfle_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfle_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p398, result);
}


unsigned long* p399;
// CHECK-O2-LABEL: @test_vfle_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfle.mask.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p399 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfle_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfle_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p399, result);
}


unsigned int* p400;
// CHECK-O2-LABEL: @test_vfgt_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfgt.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p400 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfgt_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfgt_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p400, result);
}


unsigned long* p401;
// CHECK-O2-LABEL: @test_vfgt_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfgt.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p401 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfgt_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfgt_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p401, result);
}


unsigned int* p402;
// CHECK-O2-LABEL: @test_vfgt_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfgt.mask.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p402 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfgt_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfgt_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p402, result);
}


unsigned long* p403;
// CHECK-O2-LABEL: @test_vfgt_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfgt.mask.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p403 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfgt_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfgt_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p403, result);
}


unsigned int* p404;
// CHECK-O2-LABEL: @test_vfge_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfge.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p404 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfge_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfge_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p404, result);
}


unsigned long* p405;
// CHECK-O2-LABEL: @test_vfge_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfge.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p405 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfge_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfge_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p405, result);
}


unsigned int* p406;
// CHECK-O2-LABEL: @test_vfge_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vfge.mask.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p406 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vfge_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfge_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p406, result);
}


unsigned long* p407;
// CHECK-O2-LABEL: @test_vfge_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vfge.mask.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p407 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vfge_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfge_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p407, result);
}


unsigned int* p408;
// CHECK-O2-LABEL: @test_vford_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vford.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p408 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vford_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vford_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p408, result);
}


unsigned long* p409;
// CHECK-O2-LABEL: @test_vford_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vford.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p409 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vford_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vford_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p409, result);
}


unsigned int* p410;
// CHECK-O2-LABEL: @test_vford_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vford.mask.v2i1.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p410 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vford_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vford_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p410, result);
}


unsigned long* p411;
// CHECK-O2-LABEL: @test_vford_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vford.mask.v1i1.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p411 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vford_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vford_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p411, result);
}


float* p412;
// CHECK-O2-LABEL: @test_vfmerge_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmerge.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p412 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmerge_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfmerge_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p412, result, gvl);
}


double* p413;
// CHECK-O2-LABEL: @test_vfmerge_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmerge.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p413 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmerge_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfmerge_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p413, result, gvl);
}


float* p414;
// CHECK-O2-LABEL: @test_vfmerge_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmerge.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p414 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmerge_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmerge_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p414, result, gvl);
}


double* p415;
// CHECK-O2-LABEL: @test_vfmerge_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmerge.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p415 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmerge_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmerge_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p415, result, gvl);
}


int* p416;
// CHECK-O2-LABEL: @test_vfcvt_xu_f_2xi32_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vfcvt.xu.f.v2i32.v2f32(<vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p416 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_xu_f_2xi32_2xf32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vfcvt_xu_f_2xi32_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xi32(p416, result, gvl);
}


long* p417;
// CHECK-O2-LABEL: @test_vfcvt_xu_f_1xi64_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.v1i64.v1f64(<vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p417 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_xu_f_1xi64_1xf64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xf64 lhs;
  result = __builtin_epi_vfcvt_xu_f_1xi64_1xf64(lhs, gvl);
  __builtin_epi_vstore_1xi64(p417, result, gvl);
}


int* p418;
// CHECK-O2-LABEL: @test_vfcvt_xu_f_2xi32_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vfcvt.xu.f.mask.v2i32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p418 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_xu_f_2xi32_2xf32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xf32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfcvt_xu_f_2xi32_2xf32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p418, result, gvl);
}


long* p419;
// CHECK-O2-LABEL: @test_vfcvt_xu_f_1xi64_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.mask.v1i64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p419 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_xu_f_1xi64_1xf64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xf64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfcvt_xu_f_1xi64_1xf64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p419, result, gvl);
}


int* p420;
// CHECK-O2-LABEL: @test_vfcvt_x_f_2xi32_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vfcvt.x.f.v2i32.v2f32(<vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p420 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_x_f_2xi32_2xf32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vfcvt_x_f_2xi32_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xi32(p420, result, gvl);
}


long* p421;
// CHECK-O2-LABEL: @test_vfcvt_x_f_1xi64_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.v1i64.v1f64(<vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p421 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_x_f_1xi64_1xf64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xf64 lhs;
  result = __builtin_epi_vfcvt_x_f_1xi64_1xf64(lhs, gvl);
  __builtin_epi_vstore_1xi64(p421, result, gvl);
}


int* p422;
// CHECK-O2-LABEL: @test_vfcvt_x_f_2xi32_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vfcvt.x.f.mask.v2i32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p422 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_x_f_2xi32_2xf32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xf32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfcvt_x_f_2xi32_2xf32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p422, result, gvl);
}


long* p423;
// CHECK-O2-LABEL: @test_vfcvt_x_f_1xi64_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.mask.v1i64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p423 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_x_f_1xi64_1xf64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xf64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfcvt_x_f_1xi64_1xf64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p423, result, gvl);
}


float* p424;
// CHECK-O2-LABEL: @test_vfcvt_f_xu_2xf32_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfcvt.f.xu.v2f32.v2i32(<vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p424 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_xu_2xf32_2xi32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xi32 lhs;
  result = __builtin_epi_vfcvt_f_xu_2xf32_2xi32(lhs, gvl);
  __builtin_epi_vstore_2xf32(p424, result, gvl);
}


double* p425;
// CHECK-O2-LABEL: @test_vfcvt_f_xu_1xf64_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.v1f64.v1i64(<vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p425 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_xu_1xf64_1xi64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xi64 lhs;
  result = __builtin_epi_vfcvt_f_xu_1xf64_1xi64(lhs, gvl);
  __builtin_epi_vstore_1xf64(p425, result, gvl);
}


float* p426;
// CHECK-O2-LABEL: @test_vfcvt_f_xu_2xf32_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfcvt.f.xu.mask.v2f32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p426 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_xu_2xf32_2xi32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xi32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfcvt_f_xu_2xf32_2xi32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p426, result, gvl);
}


double* p427;
// CHECK-O2-LABEL: @test_vfcvt_f_xu_1xf64_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.mask.v1f64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p427 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_xu_1xf64_1xi64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xi64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfcvt_f_xu_1xf64_1xi64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p427, result, gvl);
}


float* p428;
// CHECK-O2-LABEL: @test_vfcvt_f_x_2xf32_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfcvt.f.x.v2f32.v2i32(<vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p428 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_x_2xf32_2xi32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xi32 lhs;
  result = __builtin_epi_vfcvt_f_x_2xf32_2xi32(lhs, gvl);
  __builtin_epi_vstore_2xf32(p428, result, gvl);
}


double* p429;
// CHECK-O2-LABEL: @test_vfcvt_f_x_1xf64_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfcvt.f.x.v1f64.v1i64(<vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p429 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_x_1xf64_1xi64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xi64 lhs;
  result = __builtin_epi_vfcvt_f_x_1xf64_1xi64(lhs, gvl);
  __builtin_epi_vstore_1xf64(p429, result, gvl);
}


float* p430;
// CHECK-O2-LABEL: @test_vfcvt_f_x_2xf32_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfcvt.f.x.mask.v2f32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p430 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_x_2xf32_2xi32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xi32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfcvt_f_x_2xf32_2xi32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p430, result, gvl);
}


double* p431;
// CHECK-O2-LABEL: @test_vfcvt_f_x_1xf64_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfcvt.f.x.mask.v1f64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p431 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_x_1xf64_1xi64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xi64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfcvt_f_x_1xf64_1xi64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p431, result, gvl);
}


double* p432;
// CHECK-O2-LABEL: @test_vfwcvt_f_f_2xf64_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.epi.vfwcvt.f.f.v2f64.v2f32(<vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x double>*, <vscale x 2 x double>** bitcast (double** @p432 to <vscale x 2 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f64(<vscale x 2 x double> [[TMP0]], <vscale x 2 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfwcvt_f_f_2xf64_2xf32(unsigned long gvl)
{
  __epi_2xf64 result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vfwcvt_f_f_2xf64_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xf64(p432, result, gvl);
}


double* p433;
// CHECK-O2-LABEL: @test_vfwcvt_f_f_2xf64_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.epi.vfwcvt.f.f.mask.v2f64.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x double>*, <vscale x 2 x double>** bitcast (double** @p433 to <vscale x 2 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f64(<vscale x 2 x double> [[TMP0]], <vscale x 2 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfwcvt_f_f_2xf64_2xf32_mask(unsigned long gvl)
{
  __epi_2xf64 result;
  __epi_2xf32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfwcvt_f_f_2xf64_2xf32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xf64(p433, result, gvl);
}


float* p434;
// CHECK-O2-LABEL: @test_vfncvt_f_f_2xf32_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfncvt.f.f.v2f32.v2f64(<vscale x 2 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p434 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfncvt_f_f_2xf32_2xf64(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf64 lhs;
  result = __builtin_epi_vfncvt_f_f_2xf32_2xf64(lhs, gvl);
  __builtin_epi_vstore_2xf32(p434, result, gvl);
}


float* p435;
// CHECK-O2-LABEL: @test_vfncvt_f_f_2xf32_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfncvt.f.f.mask.v2f32.v2f64.v2i1(<vscale x 2 x double> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p435 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfncvt_f_f_2xf32_2xf64_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf64 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfncvt_f_f_2xf32_2xf64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p435, result, gvl);
}


signed char* p436;
// CHECK-O2-LABEL: @test_vredsum_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredsum.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p436 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredsum_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p436, result, gvl);
}


short* p437;
// CHECK-O2-LABEL: @test_vredsum_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredsum.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p437 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredsum_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p437, result, gvl);
}


int* p438;
// CHECK-O2-LABEL: @test_vredsum_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredsum.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p438 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredsum_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p438, result, gvl);
}


long* p439;
// CHECK-O2-LABEL: @test_vredsum_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredsum.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p439 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredsum_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p439, result, gvl);
}


signed char* p440;
// CHECK-O2-LABEL: @test_vredsum_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredsum.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p440 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredsum_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p440, result, gvl);
}


short* p441;
// CHECK-O2-LABEL: @test_vredsum_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredsum.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p441 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredsum_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p441, result, gvl);
}


int* p442;
// CHECK-O2-LABEL: @test_vredsum_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredsum.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p442 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredsum_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p442, result, gvl);
}


long* p443;
// CHECK-O2-LABEL: @test_vredsum_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredsum.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p443 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredsum_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p443, result, gvl);
}


signed char* p444;
// CHECK-O2-LABEL: @test_vredmaxu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmaxu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p444 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredmaxu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p444, result, gvl);
}


short* p445;
// CHECK-O2-LABEL: @test_vredmaxu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmaxu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p445 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredmaxu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p445, result, gvl);
}


int* p446;
// CHECK-O2-LABEL: @test_vredmaxu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmaxu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p446 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredmaxu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p446, result, gvl);
}


long* p447;
// CHECK-O2-LABEL: @test_vredmaxu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmaxu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p447 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredmaxu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p447, result, gvl);
}


signed char* p448;
// CHECK-O2-LABEL: @test_vredmaxu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmaxu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p448 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredmaxu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p448, result, gvl);
}


short* p449;
// CHECK-O2-LABEL: @test_vredmaxu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmaxu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p449 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredmaxu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p449, result, gvl);
}


int* p450;
// CHECK-O2-LABEL: @test_vredmaxu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmaxu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p450 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredmaxu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p450, result, gvl);
}


long* p451;
// CHECK-O2-LABEL: @test_vredmaxu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmaxu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p451 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredmaxu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p451, result, gvl);
}


signed char* p452;
// CHECK-O2-LABEL: @test_vredmax_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmax.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p452 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredmax_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p452, result, gvl);
}


short* p453;
// CHECK-O2-LABEL: @test_vredmax_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmax.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p453 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredmax_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p453, result, gvl);
}


int* p454;
// CHECK-O2-LABEL: @test_vredmax_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmax.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p454 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredmax_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p454, result, gvl);
}


long* p455;
// CHECK-O2-LABEL: @test_vredmax_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmax.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p455 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredmax_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p455, result, gvl);
}


signed char* p456;
// CHECK-O2-LABEL: @test_vredmax_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmax.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p456 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredmax_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p456, result, gvl);
}


short* p457;
// CHECK-O2-LABEL: @test_vredmax_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmax.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p457 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredmax_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p457, result, gvl);
}


int* p458;
// CHECK-O2-LABEL: @test_vredmax_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmax.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p458 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredmax_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p458, result, gvl);
}


long* p459;
// CHECK-O2-LABEL: @test_vredmax_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmax.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p459 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredmax_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p459, result, gvl);
}


signed char* p460;
// CHECK-O2-LABEL: @test_vredmin_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmin.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p460 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredmin_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p460, result, gvl);
}


short* p461;
// CHECK-O2-LABEL: @test_vredmin_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmin.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p461 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredmin_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p461, result, gvl);
}


int* p462;
// CHECK-O2-LABEL: @test_vredmin_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmin.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p462 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredmin_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p462, result, gvl);
}


long* p463;
// CHECK-O2-LABEL: @test_vredmin_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmin.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p463 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredmin_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p463, result, gvl);
}


signed char* p464;
// CHECK-O2-LABEL: @test_vredmin_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmin.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p464 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredmin_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p464, result, gvl);
}


short* p465;
// CHECK-O2-LABEL: @test_vredmin_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmin.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p465 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredmin_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p465, result, gvl);
}


int* p466;
// CHECK-O2-LABEL: @test_vredmin_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmin.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p466 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredmin_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p466, result, gvl);
}


long* p467;
// CHECK-O2-LABEL: @test_vredmin_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmin.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p467 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredmin_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p467, result, gvl);
}


signed char* p468;
// CHECK-O2-LABEL: @test_vredminu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredminu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p468 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredminu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p468, result, gvl);
}


short* p469;
// CHECK-O2-LABEL: @test_vredminu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredminu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p469 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredminu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p469, result, gvl);
}


int* p470;
// CHECK-O2-LABEL: @test_vredminu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredminu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p470 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredminu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p470, result, gvl);
}


long* p471;
// CHECK-O2-LABEL: @test_vredminu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredminu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p471 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredminu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p471, result, gvl);
}


signed char* p472;
// CHECK-O2-LABEL: @test_vredminu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredminu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p472 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredminu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p472, result, gvl);
}


short* p473;
// CHECK-O2-LABEL: @test_vredminu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredminu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p473 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredminu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p473, result, gvl);
}


int* p474;
// CHECK-O2-LABEL: @test_vredminu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredminu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p474 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredminu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p474, result, gvl);
}


long* p475;
// CHECK-O2-LABEL: @test_vredminu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredminu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p475 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredminu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p475, result, gvl);
}


signed char* p476;
// CHECK-O2-LABEL: @test_vredand_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredand.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p476 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredand_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p476, result, gvl);
}


short* p477;
// CHECK-O2-LABEL: @test_vredand_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredand.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p477 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredand_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p477, result, gvl);
}


int* p478;
// CHECK-O2-LABEL: @test_vredand_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredand.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p478 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredand_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p478, result, gvl);
}


long* p479;
// CHECK-O2-LABEL: @test_vredand_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredand.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p479 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredand_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p479, result, gvl);
}


signed char* p480;
// CHECK-O2-LABEL: @test_vredand_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredand.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p480 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredand_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p480, result, gvl);
}


short* p481;
// CHECK-O2-LABEL: @test_vredand_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredand.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p481 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredand_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p481, result, gvl);
}


int* p482;
// CHECK-O2-LABEL: @test_vredand_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredand.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p482 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredand_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p482, result, gvl);
}


long* p483;
// CHECK-O2-LABEL: @test_vredand_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredand.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p483 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredand_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p483, result, gvl);
}


signed char* p484;
// CHECK-O2-LABEL: @test_vredor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredor.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p484 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredor_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p484, result, gvl);
}


short* p485;
// CHECK-O2-LABEL: @test_vredor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredor.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p485 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredor_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p485, result, gvl);
}


int* p486;
// CHECK-O2-LABEL: @test_vredor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredor.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p486 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredor_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p486, result, gvl);
}


long* p487;
// CHECK-O2-LABEL: @test_vredor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredor.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p487 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredor_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p487, result, gvl);
}


signed char* p488;
// CHECK-O2-LABEL: @test_vredor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredor.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p488 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredor_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p488, result, gvl);
}


short* p489;
// CHECK-O2-LABEL: @test_vredor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredor.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p489 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredor_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p489, result, gvl);
}


int* p490;
// CHECK-O2-LABEL: @test_vredor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredor.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p490 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredor_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p490, result, gvl);
}


long* p491;
// CHECK-O2-LABEL: @test_vredor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredor.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p491 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredor_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p491, result, gvl);
}


signed char* p492;
// CHECK-O2-LABEL: @test_vredxor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredxor.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p492 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredxor_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p492, result, gvl);
}


short* p493;
// CHECK-O2-LABEL: @test_vredxor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredxor.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p493 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredxor_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p493, result, gvl);
}


int* p494;
// CHECK-O2-LABEL: @test_vredxor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredxor.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p494 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredxor_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p494, result, gvl);
}


long* p495;
// CHECK-O2-LABEL: @test_vredxor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredxor.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p495 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredxor_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p495, result, gvl);
}


signed char* p496;
// CHECK-O2-LABEL: @test_vredxor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredxor.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p496 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredxor_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p496, result, gvl);
}


short* p497;
// CHECK-O2-LABEL: @test_vredxor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredxor.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p497 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredxor_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p497, result, gvl);
}


int* p498;
// CHECK-O2-LABEL: @test_vredxor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredxor.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p498 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredxor_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p498, result, gvl);
}


long* p499;
// CHECK-O2-LABEL: @test_vredxor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredxor.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p499 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredxor_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p499, result, gvl);
}


float* p500;
// CHECK-O2-LABEL: @test_vfredosum_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredosum.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p500 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredosum_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfredosum_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p500, result, gvl);
}


double* p501;
// CHECK-O2-LABEL: @test_vfredosum_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredosum.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p501 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredosum_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfredosum_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p501, result, gvl);
}


float* p502;
// CHECK-O2-LABEL: @test_vfredosum_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredosum.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p502 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredosum_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfredosum_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p502, result, gvl);
}


double* p503;
// CHECK-O2-LABEL: @test_vfredosum_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredosum.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p503 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredosum_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfredosum_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p503, result, gvl);
}


float* p504;
// CHECK-O2-LABEL: @test_vfredsum_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredsum.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p504 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredsum_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfredsum_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p504, result, gvl);
}


double* p505;
// CHECK-O2-LABEL: @test_vfredsum_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredsum.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p505 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredsum_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfredsum_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p505, result, gvl);
}


float* p506;
// CHECK-O2-LABEL: @test_vfredsum_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredsum.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p506 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredsum_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfredsum_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p506, result, gvl);
}


double* p507;
// CHECK-O2-LABEL: @test_vfredsum_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredsum.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p507 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredsum_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfredsum_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p507, result, gvl);
}


float* p508;
// CHECK-O2-LABEL: @test_vfredmax_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredmax.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p508 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmax_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfredmax_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p508, result, gvl);
}


double* p509;
// CHECK-O2-LABEL: @test_vfredmax_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredmax.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p509 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmax_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfredmax_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p509, result, gvl);
}


float* p510;
// CHECK-O2-LABEL: @test_vfredmax_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredmax.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p510 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmax_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfredmax_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p510, result, gvl);
}


double* p511;
// CHECK-O2-LABEL: @test_vfredmax_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredmax.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p511 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmax_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfredmax_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p511, result, gvl);
}


float* p512;
// CHECK-O2-LABEL: @test_vfredmin_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredmin.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p512 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmin_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfredmin_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p512, result, gvl);
}


double* p513;
// CHECK-O2-LABEL: @test_vfredmin_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredmin.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p513 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmin_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfredmin_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p513, result, gvl);
}


float* p514;
// CHECK-O2-LABEL: @test_vfredmin_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredmin.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p514 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmin_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfredmin_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p514, result, gvl);
}


double* p515;
// CHECK-O2-LABEL: @test_vfredmin_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredmin.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p515 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmin_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfredmin_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p515, result, gvl);
}


unsigned long* p516;
// CHECK-O2-LABEL: @test_vmand_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmand.v1i1.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p516 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmand_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmand_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p516, result);
}


unsigned int* p517;
// CHECK-O2-LABEL: @test_vmand_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmand.v2i1.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p517 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmand_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmand_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p517, result);
}


unsigned short* p518;
// CHECK-O2-LABEL: @test_vmand_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmand.v4i1.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p518 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmand_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmand_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p518, result);
}


unsigned char* p519;
// CHECK-O2-LABEL: @test_vmand_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmand.v8i1.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p519 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmand_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmand_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p519, result);
}


unsigned long* p520;
// CHECK-O2-LABEL: @test_vmnand_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmnand.v1i1.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p520 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmnand_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmnand_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p520, result);
}


unsigned int* p521;
// CHECK-O2-LABEL: @test_vmnand_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmnand.v2i1.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p521 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmnand_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmnand_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p521, result);
}


unsigned short* p522;
// CHECK-O2-LABEL: @test_vmnand_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmnand.v4i1.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p522 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmnand_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmnand_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p522, result);
}


unsigned char* p523;
// CHECK-O2-LABEL: @test_vmnand_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmnand.v8i1.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p523 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmnand_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmnand_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p523, result);
}


unsigned long* p524;
// CHECK-O2-LABEL: @test_vmandnot_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmandnot.v1i1.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p524 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmandnot_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmandnot_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p524, result);
}


unsigned int* p525;
// CHECK-O2-LABEL: @test_vmandnot_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmandnot.v2i1.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p525 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmandnot_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmandnot_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p525, result);
}


unsigned short* p526;
// CHECK-O2-LABEL: @test_vmandnot_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmandnot.v4i1.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p526 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmandnot_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmandnot_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p526, result);
}


unsigned char* p527;
// CHECK-O2-LABEL: @test_vmandnot_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmandnot.v8i1.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p527 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmandnot_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmandnot_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p527, result);
}


unsigned long* p528;
// CHECK-O2-LABEL: @test_vmxor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmxor.v1i1.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p528 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmxor_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmxor_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p528, result);
}


unsigned int* p529;
// CHECK-O2-LABEL: @test_vmxor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmxor.v2i1.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p529 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmxor_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmxor_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p529, result);
}


unsigned short* p530;
// CHECK-O2-LABEL: @test_vmxor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmxor.v4i1.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p530 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmxor_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmxor_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p530, result);
}


unsigned char* p531;
// CHECK-O2-LABEL: @test_vmxor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmxor.v8i1.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p531 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmxor_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmxor_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p531, result);
}


unsigned long* p532;
// CHECK-O2-LABEL: @test_vmor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmor.v1i1.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p532 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmor_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmor_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p532, result);
}


unsigned int* p533;
// CHECK-O2-LABEL: @test_vmor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmor.v2i1.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p533 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmor_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmor_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p533, result);
}


unsigned short* p534;
// CHECK-O2-LABEL: @test_vmor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmor.v4i1.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p534 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmor_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmor_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p534, result);
}


unsigned char* p535;
// CHECK-O2-LABEL: @test_vmor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmor.v8i1.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p535 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmor_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmor_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p535, result);
}


unsigned long* p536;
// CHECK-O2-LABEL: @test_vmnor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmnor.v1i1.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p536 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmnor_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmnor_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p536, result);
}


unsigned int* p537;
// CHECK-O2-LABEL: @test_vmnor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmnor.v2i1.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p537 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmnor_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmnor_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p537, result);
}


unsigned short* p538;
// CHECK-O2-LABEL: @test_vmnor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmnor.v4i1.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p538 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmnor_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmnor_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p538, result);
}


unsigned char* p539;
// CHECK-O2-LABEL: @test_vmnor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmnor.v8i1.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p539 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmnor_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmnor_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p539, result);
}


unsigned long* p540;
// CHECK-O2-LABEL: @test_vmornot_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmornot.v1i1.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p540 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmornot_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmornot_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p540, result);
}


unsigned int* p541;
// CHECK-O2-LABEL: @test_vmornot_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmornot.v2i1.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p541 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmornot_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmornot_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p541, result);
}


unsigned short* p542;
// CHECK-O2-LABEL: @test_vmornot_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmornot.v4i1.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p542 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmornot_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmornot_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p542, result);
}


unsigned char* p543;
// CHECK-O2-LABEL: @test_vmornot_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmornot.v8i1.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p543 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmornot_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmornot_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p543, result);
}


unsigned long* p544;
// CHECK-O2-LABEL: @test_vmxnor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmxnor.v1i1.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p544 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmxnor_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmxnor_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p544, result);
}


unsigned int* p545;
// CHECK-O2-LABEL: @test_vmxnor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmxnor.v2i1.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p545 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmxnor_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmxnor_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p545, result);
}


unsigned short* p546;
// CHECK-O2-LABEL: @test_vmxnor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmxnor.v4i1.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p546 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmxnor_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmxnor_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p546, result);
}


unsigned char* p547;
// CHECK-O2-LABEL: @test_vmxnor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmxnor.v8i1.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p547 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmxnor_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmxnor_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p547, result);
}


signed long int* p548;
// CHECK-O2-LABEL: @test_vmpopc_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.v1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p548, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_1xi1(unsigned long gvl)
{
  signed long int result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmpopc_1xi1(lhs, gvl);
  *p548 = result;
}


signed long int* p549;
// CHECK-O2-LABEL: @test_vmpopc_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.v2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p549, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_2xi1(unsigned long gvl)
{
  signed long int result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmpopc_2xi1(lhs, gvl);
  *p549 = result;
}


signed long int* p550;
// CHECK-O2-LABEL: @test_vmpopc_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.v4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p550, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_4xi1(unsigned long gvl)
{
  signed long int result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmpopc_4xi1(lhs, gvl);
  *p550 = result;
}


signed long int* p551;
// CHECK-O2-LABEL: @test_vmpopc_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.v8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p551, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_8xi1(unsigned long gvl)
{
  signed long int result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmpopc_8xi1(lhs, gvl);
  *p551 = result;
}


signed long int* p552;
// CHECK-O2-LABEL: @test_vmpopc_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.mask.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p552, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_1xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmpopc_1xi1_mask(lhs, mask, gvl);
  *p552 = result;
}


signed long int* p553;
// CHECK-O2-LABEL: @test_vmpopc_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.mask.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p553, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_2xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmpopc_2xi1_mask(lhs, mask, gvl);
  *p553 = result;
}


signed long int* p554;
// CHECK-O2-LABEL: @test_vmpopc_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.mask.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p554, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_4xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmpopc_4xi1_mask(lhs, mask, gvl);
  *p554 = result;
}


signed long int* p555;
// CHECK-O2-LABEL: @test_vmpopc_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.mask.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p555, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_8xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmpopc_8xi1_mask(lhs, mask, gvl);
  *p555 = result;
}


signed long int* p556;
// CHECK-O2-LABEL: @test_vmfirst_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.v1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p556, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_1xi1(unsigned long gvl)
{
  signed long int result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmfirst_1xi1(lhs, gvl);
  *p556 = result;
}


signed long int* p557;
// CHECK-O2-LABEL: @test_vmfirst_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.v2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p557, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_2xi1(unsigned long gvl)
{
  signed long int result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmfirst_2xi1(lhs, gvl);
  *p557 = result;
}


signed long int* p558;
// CHECK-O2-LABEL: @test_vmfirst_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.v4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p558, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_4xi1(unsigned long gvl)
{
  signed long int result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmfirst_4xi1(lhs, gvl);
  *p558 = result;
}


signed long int* p559;
// CHECK-O2-LABEL: @test_vmfirst_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.v8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p559, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_8xi1(unsigned long gvl)
{
  signed long int result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmfirst_8xi1(lhs, gvl);
  *p559 = result;
}


signed long int* p560;
// CHECK-O2-LABEL: @test_vmfirst_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.mask.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p560, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_1xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmfirst_1xi1_mask(lhs, mask, gvl);
  *p560 = result;
}


signed long int* p561;
// CHECK-O2-LABEL: @test_vmfirst_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.mask.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p561, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_2xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmfirst_2xi1_mask(lhs, mask, gvl);
  *p561 = result;
}


signed long int* p562;
// CHECK-O2-LABEL: @test_vmfirst_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.mask.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p562, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_4xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmfirst_4xi1_mask(lhs, mask, gvl);
  *p562 = result;
}


signed long int* p563;
// CHECK-O2-LABEL: @test_vmfirst_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.mask.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p563, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_8xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmfirst_8xi1_mask(lhs, mask, gvl);
  *p563 = result;
}


unsigned long* p564;
// CHECK-O2-LABEL: @test_vmsbf_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsbf.v1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p564 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmsbf_1xi1(lhs, gvl);
  __builtin_epi_vstore_1xi1(p564, result);
}


unsigned int* p565;
// CHECK-O2-LABEL: @test_vmsbf_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsbf.v2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p565 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmsbf_2xi1(lhs, gvl);
  __builtin_epi_vstore_2xi1(p565, result);
}


unsigned short* p566;
// CHECK-O2-LABEL: @test_vmsbf_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsbf.v4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p566 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmsbf_4xi1(lhs, gvl);
  __builtin_epi_vstore_4xi1(p566, result);
}


unsigned char* p567;
// CHECK-O2-LABEL: @test_vmsbf_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsbf.v8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p567 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmsbf_8xi1(lhs, gvl);
  __builtin_epi_vstore_8xi1(p567, result);
}


unsigned long* p568;
// CHECK-O2-LABEL: @test_vmsbf_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsbf.mask.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p568 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_1xi1_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsbf_1xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p568, result);
}


unsigned int* p569;
// CHECK-O2-LABEL: @test_vmsbf_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsbf.mask.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p569 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_2xi1_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsbf_2xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p569, result);
}


unsigned short* p570;
// CHECK-O2-LABEL: @test_vmsbf_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsbf.mask.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p570 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_4xi1_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsbf_4xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p570, result);
}


unsigned char* p571;
// CHECK-O2-LABEL: @test_vmsbf_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsbf.mask.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p571 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_8xi1_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsbf_8xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p571, result);
}


unsigned long* p572;
// CHECK-O2-LABEL: @test_vmsif_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsif.v1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p572 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmsif_1xi1(lhs, gvl);
  __builtin_epi_vstore_1xi1(p572, result);
}


unsigned int* p573;
// CHECK-O2-LABEL: @test_vmsif_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsif.v2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p573 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmsif_2xi1(lhs, gvl);
  __builtin_epi_vstore_2xi1(p573, result);
}


unsigned short* p574;
// CHECK-O2-LABEL: @test_vmsif_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsif.v4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p574 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmsif_4xi1(lhs, gvl);
  __builtin_epi_vstore_4xi1(p574, result);
}


unsigned char* p575;
// CHECK-O2-LABEL: @test_vmsif_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsif.v8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p575 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmsif_8xi1(lhs, gvl);
  __builtin_epi_vstore_8xi1(p575, result);
}


unsigned long* p576;
// CHECK-O2-LABEL: @test_vmsif_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsif.mask.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p576 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_1xi1_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsif_1xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p576, result);
}


unsigned int* p577;
// CHECK-O2-LABEL: @test_vmsif_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsif.mask.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p577 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_2xi1_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsif_2xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p577, result);
}


unsigned short* p578;
// CHECK-O2-LABEL: @test_vmsif_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsif.mask.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p578 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_4xi1_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsif_4xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p578, result);
}


unsigned char* p579;
// CHECK-O2-LABEL: @test_vmsif_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsif.mask.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p579 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_8xi1_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsif_8xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p579, result);
}


unsigned long* p580;
// CHECK-O2-LABEL: @test_vmsof_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsof.v1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p580 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmsof_1xi1(lhs, gvl);
  __builtin_epi_vstore_1xi1(p580, result);
}


unsigned int* p581;
// CHECK-O2-LABEL: @test_vmsof_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsof.v2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p581 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmsof_2xi1(lhs, gvl);
  __builtin_epi_vstore_2xi1(p581, result);
}


unsigned short* p582;
// CHECK-O2-LABEL: @test_vmsof_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsof.v4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p582 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmsof_4xi1(lhs, gvl);
  __builtin_epi_vstore_4xi1(p582, result);
}


unsigned char* p583;
// CHECK-O2-LABEL: @test_vmsof_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsof.v8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p583 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmsof_8xi1(lhs, gvl);
  __builtin_epi_vstore_8xi1(p583, result);
}


unsigned long* p584;
// CHECK-O2-LABEL: @test_vmsof_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsof.mask.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p584 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_1xi1_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsof_1xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p584, result);
}


unsigned int* p585;
// CHECK-O2-LABEL: @test_vmsof_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsof.mask.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p585 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_2xi1_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsof_2xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p585, result);
}


unsigned short* p586;
// CHECK-O2-LABEL: @test_vmsof_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsof.mask.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p586 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_4xi1_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsof_4xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p586, result);
}


unsigned char* p587;
// CHECK-O2-LABEL: @test_vmsof_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsof.mask.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p587 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_8xi1_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsof_8xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p587, result);
}


signed char* p588;
// CHECK-O2-LABEL: @test_vmiota_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmiota.v8i8.v8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p588 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmiota_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmiota_8xi8(lhs, gvl);
  __builtin_epi_vstore_8xi8(p588, result, gvl);
}


short* p589;
// CHECK-O2-LABEL: @test_vmiota_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmiota.v4i16.v4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p589 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmiota_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmiota_4xi16(lhs, gvl);
  __builtin_epi_vstore_4xi16(p589, result, gvl);
}


int* p590;
// CHECK-O2-LABEL: @test_vmiota_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmiota.v2i32.v2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p590 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmiota_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmiota_2xi32(lhs, gvl);
  __builtin_epi_vstore_2xi32(p590, result, gvl);
}


long* p591;
// CHECK-O2-LABEL: @test_vmiota_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmiota.v1i64.v1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p591 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmiota_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmiota_1xi64(lhs, gvl);
  __builtin_epi_vstore_1xi64(p591, result, gvl);
}


signed char* p592;
// CHECK-O2-LABEL: @test_vmiota_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmiota.mask.v8i8.v8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p592 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmiota_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmiota_8xi8_mask(lhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p592, result, gvl);
}


short* p593;
// CHECK-O2-LABEL: @test_vmiota_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmiota.mask.v4i16.v4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p593 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmiota_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmiota_4xi16_mask(lhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p593, result, gvl);
}


int* p594;
// CHECK-O2-LABEL: @test_vmiota_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmiota.mask.v2i32.v2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p594 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmiota_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmiota_2xi32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p594, result, gvl);
}


long* p595;
// CHECK-O2-LABEL: @test_vmiota_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmiota.mask.v1i64.v1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p595 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmiota_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmiota_1xi64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p595, result, gvl);
}


// CHECK-O2-LABEL: @test_vextract_8xi8_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i8 @llvm.epi.vext.x.v.i8.v8i8(<vscale x 8 x i8> undef, i64 [[IDX:%.*]])
// CHECK-O2-NEXT:    ret i8 [[TMP0]]
//
signed char test_vextract_8xi8_(unsigned long idx)
{
  signed char result;
  __epi_8xi8 lhs;
  result = __builtin_epi_vextract_8xi8(lhs, idx);
  return result;
}


// CHECK-O2-LABEL: @test_vextract_4xi16_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i16 @llvm.epi.vext.x.v.i16.v4i16(<vscale x 4 x i16> undef, i64 [[IDX:%.*]])
// CHECK-O2-NEXT:    ret i16 [[TMP0]]
//
signed short int test_vextract_4xi16_(unsigned long idx)
{
  signed short int result;
  __epi_4xi16 lhs;
  result = __builtin_epi_vextract_4xi16(lhs, idx);
  return result;
}


// CHECK-O2-LABEL: @test_vextract_2xi32_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.epi.vext.x.v.i32.v2i32(<vscale x 2 x i32> undef, i64 [[IDX:%.*]])
// CHECK-O2-NEXT:    ret i32 [[TMP0]]
//
signed int test_vextract_2xi32_(unsigned long idx)
{
  signed int result;
  __epi_2xi32 lhs;
  result = __builtin_epi_vextract_2xi32(lhs, idx);
  return result;
}


// CHECK-O2-LABEL: @test_vextract_1xi64_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vext.x.v.i64.v1i64(<vscale x 1 x i64> undef, i64 [[IDX:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vextract_1xi64_(unsigned long idx)
{
  signed long int result;
  __epi_1xi64 lhs;
  result = __builtin_epi_vextract_1xi64(lhs, idx);
  return result;
}


signed char* p596;
// CHECK-O2-LABEL: @test_vsetfirst_8xi8_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[VALUE:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmv.s.x.v8i8.i8(i8 [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p596 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_8xi8_(unsigned long int value, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vsetfirst_8xi8(value, gvl);
  __builtin_epi_vstore_8xi8(p596, result, gvl);
}


short* p597;
// CHECK-O2-LABEL: @test_vsetfirst_4xi16_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[VALUE:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmv.s.x.v4i16.i16(i16 [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p597 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_4xi16_(unsigned long int value, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vsetfirst_4xi16(value, gvl);
  __builtin_epi_vstore_4xi16(p597, result, gvl);
}


int* p598;
// CHECK-O2-LABEL: @test_vsetfirst_2xi32_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[VALUE:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmv.s.x.v2i32.i32(i32 [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p598 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_2xi32_(unsigned long int value, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vsetfirst_2xi32(value, gvl);
  __builtin_epi_vstore_2xi32(p598, result, gvl);
}


long* p599;
// CHECK-O2-LABEL: @test_vsetfirst_1xi64_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmv.s.x.v1i64.i64(i64 [[VALUE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p599 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_1xi64_(unsigned long int value, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vsetfirst_1xi64(value, gvl);
  __builtin_epi_vstore_1xi64(p599, result, gvl);
}


float* p600;
// CHECK-O2-LABEL: @test_vsetfirst_2xf32_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[VALUE:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmv.s.f.v2f32.f32(float [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p600 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_2xf32_(unsigned long int value, unsigned long gvl)
{
  __epi_2xf32 result;
  result = __builtin_epi_vsetfirst_2xf32(value, gvl);
  __builtin_epi_vstore_2xf32(p600, result, gvl);
}


double* p601;
// CHECK-O2-LABEL: @test_vsetfirst_1xf64_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[VALUE:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmv.s.f.v1f64.f64(double [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p601 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_1xf64_(unsigned long int value, unsigned long gvl)
{
  __epi_1xf64 result;
  result = __builtin_epi_vsetfirst_1xf64(value, gvl);
  __builtin_epi_vstore_1xf64(p601, result, gvl);
}


// CHECK-O2-LABEL: @test_vgetfirst_2xf32_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call float @llvm.epi.vfmv.f.s.f32.v2f32(<vscale x 2 x float> undef)
// CHECK-O2-NEXT:    ret float [[TMP0]]
//
float test_vgetfirst_2xf32_(void)
{
  float result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vgetfirst_2xf32(lhs);
  return result;
}


// CHECK-O2-LABEL: @test_vgetfirst_1xf64_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call double @llvm.epi.vfmv.f.s.f64.v1f64(<vscale x 1 x double> undef)
// CHECK-O2-NEXT:    ret double [[TMP0]]
//
double test_vgetfirst_1xf64_(void)
{
  double result;
  __epi_1xf64 lhs;
  result = __builtin_epi_vgetfirst_1xf64(lhs);
  return result;
}


signed char* p602;
// CHECK-O2-LABEL: @test_vslideup_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslideup.v8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p602 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p602, result, gvl);
}


short* p603;
// CHECK-O2-LABEL: @test_vslideup_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslideup.v4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p603 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p603, result, gvl);
}


int* p604;
// CHECK-O2-LABEL: @test_vslideup_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslideup.v2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p604 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p604, result, gvl);
}


long* p605;
// CHECK-O2-LABEL: @test_vslideup_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslideup.v1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p605 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p605, result, gvl);
}


float* p606;
// CHECK-O2-LABEL: @test_vslideup_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslideup.v2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p606 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p606, result, gvl);
}


double* p607;
// CHECK-O2-LABEL: @test_vslideup_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslideup.v1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p607 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p607, result, gvl);
}


signed char* p608;
// CHECK-O2-LABEL: @test_vslideup_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslideup.mask.v8i8.i64.v8i1(<vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p608 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vslideup_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p608, result, gvl);
}


short* p609;
// CHECK-O2-LABEL: @test_vslideup_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslideup.mask.v4i16.i64.v4i1(<vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p609 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vslideup_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p609, result, gvl);
}


int* p610;
// CHECK-O2-LABEL: @test_vslideup_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslideup.mask.v2i32.i64.v2i1(<vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p610 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslideup_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p610, result, gvl);
}


long* p611;
// CHECK-O2-LABEL: @test_vslideup_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslideup.mask.v1i64.i64.v1i1(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p611 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslideup_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p611, result, gvl);
}


float* p612;
// CHECK-O2-LABEL: @test_vslideup_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslideup.mask.v2f32.i64.v2i1(<vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p612 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslideup_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p612, result, gvl);
}


double* p613;
// CHECK-O2-LABEL: @test_vslideup_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslideup.mask.v1f64.i64.v1i1(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p613 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslideup_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p613, result, gvl);
}


signed char* p614;
// CHECK-O2-LABEL: @test_vslidedown_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslidedown.v8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p614 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p614, result, gvl);
}


short* p615;
// CHECK-O2-LABEL: @test_vslidedown_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslidedown.v4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p615 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p615, result, gvl);
}


int* p616;
// CHECK-O2-LABEL: @test_vslidedown_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslidedown.v2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p616 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p616, result, gvl);
}


long* p617;
// CHECK-O2-LABEL: @test_vslidedown_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslidedown.v1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p617 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p617, result, gvl);
}


float* p618;
// CHECK-O2-LABEL: @test_vslidedown_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslidedown.v2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p618 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p618, result, gvl);
}


double* p619;
// CHECK-O2-LABEL: @test_vslidedown_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslidedown.v1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p619 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p619, result, gvl);
}


signed char* p620;
// CHECK-O2-LABEL: @test_vslidedown_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslidedown.mask.v8i8.i64.v8i1(<vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p620 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vslidedown_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p620, result, gvl);
}


short* p621;
// CHECK-O2-LABEL: @test_vslidedown_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslidedown.mask.v4i16.i64.v4i1(<vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p621 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vslidedown_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p621, result, gvl);
}


int* p622;
// CHECK-O2-LABEL: @test_vslidedown_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslidedown.mask.v2i32.i64.v2i1(<vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p622 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslidedown_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p622, result, gvl);
}


long* p623;
// CHECK-O2-LABEL: @test_vslidedown_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslidedown.mask.v1i64.i64.v1i1(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p623 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslidedown_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p623, result, gvl);
}


float* p624;
// CHECK-O2-LABEL: @test_vslidedown_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslidedown.mask.v2f32.i64.v2i1(<vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p624 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslidedown_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p624, result, gvl);
}


double* p625;
// CHECK-O2-LABEL: @test_vslidedown_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslidedown.mask.v1f64.i64.v1i1(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p625 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslidedown_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p625, result, gvl);
}


signed char* p626;
// CHECK-O2-LABEL: @test_vslide1up_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslide1up.v8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p626 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p626, result, gvl);
}


short* p627;
// CHECK-O2-LABEL: @test_vslide1up_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslide1up.v4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p627 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p627, result, gvl);
}


int* p628;
// CHECK-O2-LABEL: @test_vslide1up_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslide1up.v2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p628 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p628, result, gvl);
}


long* p629;
// CHECK-O2-LABEL: @test_vslide1up_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslide1up.v1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p629 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p629, result, gvl);
}


float* p630;
// CHECK-O2-LABEL: @test_vslide1up_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslide1up.v2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p630 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p630, result, gvl);
}


double* p631;
// CHECK-O2-LABEL: @test_vslide1up_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslide1up.v1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p631 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p631, result, gvl);
}


signed char* p632;
// CHECK-O2-LABEL: @test_vslide1up_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslide1up.mask.v8i8.i64.v8i1(<vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p632 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vslide1up_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p632, result, gvl);
}


short* p633;
// CHECK-O2-LABEL: @test_vslide1up_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslide1up.mask.v4i16.i64.v4i1(<vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p633 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vslide1up_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p633, result, gvl);
}


int* p634;
// CHECK-O2-LABEL: @test_vslide1up_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslide1up.mask.v2i32.i64.v2i1(<vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p634 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslide1up_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p634, result, gvl);
}


long* p635;
// CHECK-O2-LABEL: @test_vslide1up_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslide1up.mask.v1i64.i64.v1i1(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p635 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslide1up_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p635, result, gvl);
}


float* p636;
// CHECK-O2-LABEL: @test_vslide1up_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslide1up.mask.v2f32.i64.v2i1(<vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p636 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslide1up_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p636, result, gvl);
}


double* p637;
// CHECK-O2-LABEL: @test_vslide1up_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslide1up.mask.v1f64.i64.v1i1(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p637 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslide1up_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p637, result, gvl);
}


signed char* p638;
// CHECK-O2-LABEL: @test_vslide1down_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslide1down.v8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p638 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p638, result, gvl);
}


short* p639;
// CHECK-O2-LABEL: @test_vslide1down_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslide1down.v4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p639 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p639, result, gvl);
}


int* p640;
// CHECK-O2-LABEL: @test_vslide1down_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslide1down.v2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p640 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p640, result, gvl);
}


long* p641;
// CHECK-O2-LABEL: @test_vslide1down_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslide1down.v1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p641 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p641, result, gvl);
}


float* p642;
// CHECK-O2-LABEL: @test_vslide1down_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslide1down.v2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p642 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p642, result, gvl);
}


double* p643;
// CHECK-O2-LABEL: @test_vslide1down_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslide1down.v1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p643 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p643, result, gvl);
}


signed char* p644;
// CHECK-O2-LABEL: @test_vslide1down_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslide1down.mask.v8i8.i64.v8i1(<vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p644 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vslide1down_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p644, result, gvl);
}


short* p645;
// CHECK-O2-LABEL: @test_vslide1down_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslide1down.mask.v4i16.i64.v4i1(<vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p645 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vslide1down_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p645, result, gvl);
}


int* p646;
// CHECK-O2-LABEL: @test_vslide1down_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslide1down.mask.v2i32.i64.v2i1(<vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p646 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslide1down_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p646, result, gvl);
}


long* p647;
// CHECK-O2-LABEL: @test_vslide1down_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslide1down.mask.v1i64.i64.v1i1(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p647 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslide1down_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p647, result, gvl);
}


float* p648;
// CHECK-O2-LABEL: @test_vslide1down_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslide1down.mask.v2f32.i64.v2i1(<vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p648 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslide1down_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p648, result, gvl);
}


double* p649;
// CHECK-O2-LABEL: @test_vslide1down_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslide1down.mask.v1f64.i64.v1i1(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p649 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslide1down_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p649, result, gvl);
}


signed char* p650;
// CHECK-O2-LABEL: @test_vrgather_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrgather.v8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p650 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p650, result, gvl);
}


short* p651;
// CHECK-O2-LABEL: @test_vrgather_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrgather.v4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p651 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p651, result, gvl);
}


int* p652;
// CHECK-O2-LABEL: @test_vrgather_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrgather.v2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p652 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p652, result, gvl);
}


long* p653;
// CHECK-O2-LABEL: @test_vrgather_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrgather.v1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p653 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p653, result, gvl);
}


float* p654;
// CHECK-O2-LABEL: @test_vrgather_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vrgather.v2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p654 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p654, result, gvl);
}


double* p655;
// CHECK-O2-LABEL: @test_vrgather_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vrgather.v1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p655 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p655, result, gvl);
}


signed char* p656;
// CHECK-O2-LABEL: @test_vrgather_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrgather.mask.v8i8.i64.v8i1(<vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p656 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  signed long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vrgather_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p656, result, gvl);
}


short* p657;
// CHECK-O2-LABEL: @test_vrgather_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrgather.mask.v4i16.i64.v4i1(<vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p657 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  signed long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vrgather_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p657, result, gvl);
}


int* p658;
// CHECK-O2-LABEL: @test_vrgather_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrgather.mask.v2i32.i64.v2i1(<vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p658 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  signed long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vrgather_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p658, result, gvl);
}


long* p659;
// CHECK-O2-LABEL: @test_vrgather_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrgather.mask.v1i64.i64.v1i1(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p659 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  signed long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vrgather_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p659, result, gvl);
}


float* p660;
// CHECK-O2-LABEL: @test_vrgather_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vrgather.mask.v2f32.i64.v2i1(<vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p660 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  signed long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vrgather_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p660, result, gvl);
}


double* p661;
// CHECK-O2-LABEL: @test_vrgather_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vrgather.mask.v1f64.i64.v1i1(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p661 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  signed long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vrgather_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p661, result, gvl);
}


signed char* p662;
// CHECK-O2-LABEL: @test_vcompress_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vcompress.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p662 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vcompress_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p662, result, gvl);
}


short* p663;
// CHECK-O2-LABEL: @test_vcompress_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vcompress.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p663 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vcompress_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p663, result, gvl);
}


int* p664;
// CHECK-O2-LABEL: @test_vcompress_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vcompress.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p664 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vcompress_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p664, result, gvl);
}


long* p665;
// CHECK-O2-LABEL: @test_vcompress_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vcompress.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p665 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vcompress_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p665, result, gvl);
}


float* p666;
// CHECK-O2-LABEL: @test_vcompress_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vcompress.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p666 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vcompress_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p666, result, gvl);
}


double* p667;
// CHECK-O2-LABEL: @test_vcompress_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vcompress.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p667 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vcompress_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p667, result, gvl);
}


signed char* p668;
// CHECK-O2-LABEL: @test_vdot_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdot.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p668 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vdot_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p668, result, gvl);
}


short* p669;
// CHECK-O2-LABEL: @test_vdot_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdot.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p669 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vdot_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p669, result, gvl);
}


int* p670;
// CHECK-O2-LABEL: @test_vdot_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdot.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p670 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vdot_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p670, result, gvl);
}


long* p671;
// CHECK-O2-LABEL: @test_vdot_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdot.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p671 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vdot_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p671, result, gvl);
}


signed char* p672;
// CHECK-O2-LABEL: @test_vdot_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdot.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p672 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vdot_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p672, result, gvl);
}


short* p673;
// CHECK-O2-LABEL: @test_vdot_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdot.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p673 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vdot_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p673, result, gvl);
}


int* p674;
// CHECK-O2-LABEL: @test_vdot_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdot.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p674 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vdot_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p674, result, gvl);
}


long* p675;
// CHECK-O2-LABEL: @test_vdot_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdot.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p675 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vdot_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p675, result, gvl);
}


signed char* p676;
// CHECK-O2-LABEL: @test_vdotu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdotu.v8i8.v8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p676 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vdotu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p676, result, gvl);
}


short* p677;
// CHECK-O2-LABEL: @test_vdotu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdotu.v4i16.v4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p677 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vdotu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p677, result, gvl);
}


int* p678;
// CHECK-O2-LABEL: @test_vdotu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdotu.v2i32.v2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p678 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vdotu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p678, result, gvl);
}


long* p679;
// CHECK-O2-LABEL: @test_vdotu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdotu.v1i64.v1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p679 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vdotu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p679, result, gvl);
}


signed char* p680;
// CHECK-O2-LABEL: @test_vdotu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdotu.mask.v8i8.v8i8.v8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p680 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vdotu_8xi8_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p680, result, gvl);
}


short* p681;
// CHECK-O2-LABEL: @test_vdotu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdotu.mask.v4i16.v4i16.v4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p681 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vdotu_4xi16_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p681, result, gvl);
}


int* p682;
// CHECK-O2-LABEL: @test_vdotu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdotu.mask.v2i32.v2i32.v2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p682 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vdotu_2xi32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p682, result, gvl);
}


long* p683;
// CHECK-O2-LABEL: @test_vdotu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdotu.mask.v1i64.v1i64.v1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p683 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vdotu_1xi64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p683, result, gvl);
}


float* p684;
// CHECK-O2-LABEL: @test_vfdot_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfdot.v2f32.v2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p684 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdot_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfdot_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p684, result, gvl);
}


double* p685;
// CHECK-O2-LABEL: @test_vfdot_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfdot.v1f64.v1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p685 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdot_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfdot_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p685, result, gvl);
}


float* p686;
// CHECK-O2-LABEL: @test_vfdot_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfdot.mask.v2f32.v2f32.v2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p686 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdot_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfdot_2xf32_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p686, result, gvl);
}


double* p687;
// CHECK-O2-LABEL: @test_vfdot_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfdot.mask.v1f64.v1f64.v1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p687 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdot_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfdot_1xf64_mask(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p687, result, gvl);
}


signed char* p688;
// CHECK-O2-LABEL: @test_vbroadcast_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vbroadcast.v8i8.i8(i8 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p688 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  signed char lhs;
  result = __builtin_epi_vbroadcast_8xi8(lhs, gvl);
  __builtin_epi_vstore_8xi8(p688, result, gvl);
}


short* p689;
// CHECK-O2-LABEL: @test_vbroadcast_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vbroadcast.v4i16.i16(i16 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p689 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  signed short int lhs;
  result = __builtin_epi_vbroadcast_4xi16(lhs, gvl);
  __builtin_epi_vstore_4xi16(p689, result, gvl);
}


int* p690;
// CHECK-O2-LABEL: @test_vbroadcast_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vbroadcast.v2i32.i32(i32 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p690 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  signed int lhs;
  result = __builtin_epi_vbroadcast_2xi32(lhs, gvl);
  __builtin_epi_vstore_2xi32(p690, result, gvl);
}


long* p691;
// CHECK-O2-LABEL: @test_vbroadcast_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vbroadcast.v1i64.i64(i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p691 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  signed long int lhs;
  result = __builtin_epi_vbroadcast_1xi64(lhs, gvl);
  __builtin_epi_vstore_1xi64(p691, result, gvl);
}


signed char* p692;
// CHECK-O2-LABEL: @test_vbroadcast_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vbroadcast.mask.v8i8.i8.v8i1(i8 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p692 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  signed char lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vbroadcast_8xi8_mask(lhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p692, result, gvl);
}


short* p693;
// CHECK-O2-LABEL: @test_vbroadcast_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vbroadcast.mask.v4i16.i16.v4i1(i16 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p693 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  signed short int lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vbroadcast_4xi16_mask(lhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p693, result, gvl);
}


int* p694;
// CHECK-O2-LABEL: @test_vbroadcast_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vbroadcast.mask.v2i32.i32.v2i1(i32 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p694 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  signed int lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vbroadcast_2xi32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p694, result, gvl);
}


long* p695;
// CHECK-O2-LABEL: @test_vbroadcast_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vbroadcast.mask.v1i64.i64.v1i1(i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p695 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  signed long int lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vbroadcast_1xi64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p695, result, gvl);
}


float* p696;
// CHECK-O2-LABEL: @test_vbroadcast_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vbroadcast.v2f32.f32(float undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p696 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  float lhs;
  result = __builtin_epi_vbroadcast_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xf32(p696, result, gvl);
}


double* p697;
// CHECK-O2-LABEL: @test_vbroadcast_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.v1f64.f64(double undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p697 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  double lhs;
  result = __builtin_epi_vbroadcast_1xf64(lhs, gvl);
  __builtin_epi_vstore_1xf64(p697, result, gvl);
}


float* p698;
// CHECK-O2-LABEL: @test_vbroadcast_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vbroadcast.mask.v2f32.f32.v2i1(float undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p698 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  float lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vbroadcast_2xf32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p698, result, gvl);
}


double* p699;
// CHECK-O2-LABEL: @test_vbroadcast_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.mask.v1f64.f64.v1i1(double undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p699 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.v1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  double lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vbroadcast_1xf64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p699, result, gvl);
}

