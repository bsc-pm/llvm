; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+epi < %s | FileCheck %s

; Using optnone and noinline to avoid redundant memory operations to be
; removed.
define void @foo(i32* %p, i64 %gvl) nounwind optnone noinline
; CHECK-LABEL: foo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    sd ra, 8(sp)
; CHECK-NEXT:    sd s0, 0(sp)
; CHECK-NEXT:    addi s0, sp, 16
; CHECK-NEXT:    slli a1, a1, 2
; CHECK-NEXT:    addi a1, a1, 15
; CHECK-NEXT:    andi a1, a1, -16
; CHECK-NEXT:    sub a1, sp, a1
; CHECK-NEXT:    mv sp, a1
; CHECK-NEXT:    vsetvli a2, zero, e8, m1
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    vse.v v0, (a1)
; CHECK-NEXT:    vle.v v0, (a1)
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    addi sp, s0, -16
; CHECK-NEXT:    ld s0, 0(sp)
; CHECK-NEXT:    ld ra, 8(sp)
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
{
  %x0 = alloca i32, i64 %gvl
  %x1 = bitcast i32* %x0 to <vscale x 1 x i1>*

  %x2 = bitcast i32* %p to <vscale x 1 x i1>*

  %x3 = load <vscale x 1 x i1>, <vscale x 1 x i1>* %x2, align 1

  store <vscale x 1 x i1> %x3, <vscale x 1 x i1>* %x1, align 1
  %x4 = load <vscale x 1 x i1>, <vscale x 1 x i1>* %x1, align 1

  store <vscale x 1 x i1> %x4, <vscale x 1 x i1>* %x2, align 1
  ret void
}
