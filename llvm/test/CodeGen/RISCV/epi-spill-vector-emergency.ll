; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+epi -verify-machineinstrs -O0 < %s \
; RUN:    | FileCheck --check-prefix=SPILL-O0 %s
@scratch = global i64 0, align 16

; This test checks whether we have enough GPR emergency spill slots when we
; have to spill a vector and there are no free GPRs available. Currently 3 GPRs
; are required for a vector spill: address handle, old VL and old VType.
; In the output assembler we can see how a2, a3 and a4 are spilled before the
; vector spill ocurs.
define void @foo(i64 %avl) nounwind {
; SPILL-O0-LABEL: foo:
; SPILL-O0:       # %bb.0:
; SPILL-O0-NEXT:    addi sp, sp, -208
; SPILL-O0-NEXT:    sd ra, 200(sp)
; SPILL-O0-NEXT:    sd s0, 192(sp)
; SPILL-O0-NEXT:    sd s1, 184(sp)
; SPILL-O0-NEXT:    sd s2, 176(sp)
; SPILL-O0-NEXT:    sd s3, 168(sp)
; SPILL-O0-NEXT:    sd s4, 160(sp)
; SPILL-O0-NEXT:    sd s5, 152(sp)
; SPILL-O0-NEXT:    sd s6, 144(sp)
; SPILL-O0-NEXT:    sd s7, 136(sp)
; SPILL-O0-NEXT:    sd s8, 128(sp)
; SPILL-O0-NEXT:    sd s9, 120(sp)
; SPILL-O0-NEXT:    sd s10, 112(sp)
; SPILL-O0-NEXT:    sd s11, 104(sp)
; SPILL-O0-NEXT:    addi s0, sp, 208
; SPILL-O0-NEXT:    rdvtype a3
; SPILL-O0-NEXT:    rdvl a2
; SPILL-O0-NEXT:    vsetvli a1, zero, e64, m1
; SPILL-O0-NEXT:    vsetvl zero, a2, a3
; SPILL-O0-NEXT:    slli a1, a1, 3
; SPILL-O0-NEXT:    sub sp, sp, a1
; SPILL-O0-NEXT:    andi sp, sp, -16
; SPILL-O0-NEXT:    sd sp, -200(s0)
; SPILL-O0-NEXT:    lui a1, %hi(scratch)
; SPILL-O0-NEXT:    addi a1, a1, %lo(scratch)
; SPILL-O0-NEXT:    addi a2, a1, 216
; SPILL-O0-NEXT:    addi a3, a1, 224
; SPILL-O0-NEXT:    addi a4, a1, 232
; SPILL-O0-NEXT:    addi a5, a1, 240
; SPILL-O0-NEXT:    addi a6, a1, 248
; SPILL-O0-NEXT:    addi a7, a1, 32
; SPILL-O0-NEXT:    addi t0, a1, 40
; SPILL-O0-NEXT:    addi t1, a1, 48
; SPILL-O0-NEXT:    addi t2, a1, 56
; SPILL-O0-NEXT:    addi t3, a1, 64
; SPILL-O0-NEXT:    addi t4, a1, 72
; SPILL-O0-NEXT:    addi t5, a1, 80
; SPILL-O0-NEXT:    addi t6, a1, 88
; SPILL-O0-NEXT:    addi s1, a1, 96
; SPILL-O0-NEXT:    addi s2, a1, 104
; SPILL-O0-NEXT:    addi s3, a1, 112
; SPILL-O0-NEXT:    addi s4, a1, 120
; SPILL-O0-NEXT:    addi s5, a1, 128
; SPILL-O0-NEXT:    addi s6, a1, 136
; SPILL-O0-NEXT:    addi s7, a1, 144
; SPILL-O0-NEXT:    addi s8, a1, 152
; SPILL-O0-NEXT:    addi s9, a1, 160
; SPILL-O0-NEXT:    addi s10, a1, 168
; SPILL-O0-NEXT:    addi s11, a1, 176
; SPILL-O0-NEXT:    sd a0, -136(s0)
; SPILL-O0-NEXT:    addi a0, a1, 184
; SPILL-O0-NEXT:    sd a0, -144(s0)
; SPILL-O0-NEXT:    addi a0, a1, 192
; SPILL-O0-NEXT:    sd a0, -152(s0)
; SPILL-O0-NEXT:    addi a0, a1, 200
; SPILL-O0-NEXT:    sd a0, -160(s0)
; SPILL-O0-NEXT:    addi a0, a1, 208
; SPILL-O0-NEXT:    sd a0, -168(s0)
; SPILL-O0-NEXT:    addi a0, a1, 256
; SPILL-O0-NEXT:    sd a0, -176(s0)
; SPILL-O0-NEXT:    ld a0, -136(s0)
; SPILL-O0-NEXT:    vsetvli a0, a0, e64, m1
; SPILL-O0-NEXT:    ld a0, -176(s0)
; SPILL-O0-NEXT:    vle.v v0, (a0)
; SPILL-O0-NEXT:    vle.v v1, (a6)
; SPILL-O0-NEXT:    vle.v v2, (a5)
; SPILL-O0-NEXT:    vle.v v3, (a4)
; SPILL-O0-NEXT:    vle.v v4, (a3)
; SPILL-O0-NEXT:    vle.v v5, (a2)
; SPILL-O0-NEXT:    ld a0, -168(s0)
; SPILL-O0-NEXT:    vle.v v6, (a0)
; SPILL-O0-NEXT:    ld a0, -160(s0)
; SPILL-O0-NEXT:    vle.v v7, (a0)
; SPILL-O0-NEXT:    ld a0, -152(s0)
; SPILL-O0-NEXT:    vle.v v16, (a0)
; SPILL-O0-NEXT:    ld a0, -144(s0)
; SPILL-O0-NEXT:    vle.v v17, (a0)
; SPILL-O0-NEXT:    vle.v v18, (s11)
; SPILL-O0-NEXT:    vle.v v19, (s10)
; SPILL-O0-NEXT:    vle.v v20, (s9)
; SPILL-O0-NEXT:    vle.v v21, (s8)
; SPILL-O0-NEXT:    vle.v v22, (s7)
; SPILL-O0-NEXT:    vle.v v23, (s6)
; SPILL-O0-NEXT:    vle.v v8, (s5)
; SPILL-O0-NEXT:    vle.v v9, (s4)
; SPILL-O0-NEXT:    vle.v v10, (s3)
; SPILL-O0-NEXT:    vle.v v11, (s2)
; SPILL-O0-NEXT:    vle.v v12, (s1)
; SPILL-O0-NEXT:    vle.v v13, (t6)
; SPILL-O0-NEXT:    vle.v v14, (t5)
; SPILL-O0-NEXT:    vle.v v15, (t4)
; SPILL-O0-NEXT:    vle.v v24, (t3)
; SPILL-O0-NEXT:    vle.v v25, (t2)
; SPILL-O0-NEXT:    vle.v v26, (t1)
; SPILL-O0-NEXT:    vle.v v27, (t0)
; SPILL-O0-NEXT:    vle.v v28, (a7)
; SPILL-O0-NEXT:    addi a0, a1, 24
; SPILL-O0-NEXT:    vle.v v29, (a0)
; SPILL-O0-NEXT:    sd a0, -184(s0)
; SPILL-O0-NEXT:    addi a0, a1, 16
; SPILL-O0-NEXT:    vle.v v30, (a0)
; SPILL-O0-NEXT:    sd a0, -192(s0)
; SPILL-O0-NEXT:    addi a0, a1, 8
; SPILL-O0-NEXT:    vle.v v31, (a0)
; SPILL-O0-NEXT:    sd a2, -112(s0)
; SPILL-O0-NEXT:    sd a3, -120(s0)
; SPILL-O0-NEXT:    sd a4, -128(s0)
; SPILL-O0-NEXT:    ld a4, -200(s0)
; SPILL-O0-NEXT:    rdvtype a3
; SPILL-O0-NEXT:    rdvl a2
; SPILL-O0-NEXT:    vsetvli zero, zero, e64, m1
; SPILL-O0-NEXT:    vse.v v0, (a4)
; SPILL-O0-NEXT:    ld a4, -128(s0)
; SPILL-O0-NEXT:    vsetvl zero, a2, a3
; SPILL-O0-NEXT:    ld a3, -120(s0)
; SPILL-O0-NEXT:    ld a2, -112(s0)
; SPILL-O0-NEXT:    vle.v v0, (a1)
; SPILL-O0-NEXT:    vse.v v0, (a1)
; SPILL-O0-NEXT:    vse.v v31, (a0)
; SPILL-O0-NEXT:    ld a0, -192(s0)
; SPILL-O0-NEXT:    vse.v v30, (a0)
; SPILL-O0-NEXT:    ld a0, -184(s0)
; SPILL-O0-NEXT:    vse.v v29, (a0)
; SPILL-O0-NEXT:    vse.v v28, (a7)
; SPILL-O0-NEXT:    vse.v v27, (t0)
; SPILL-O0-NEXT:    vse.v v26, (t1)
; SPILL-O0-NEXT:    vse.v v25, (t2)
; SPILL-O0-NEXT:    vse.v v24, (t3)
; SPILL-O0-NEXT:    vse.v v15, (t4)
; SPILL-O0-NEXT:    vse.v v14, (t5)
; SPILL-O0-NEXT:    vse.v v13, (t6)
; SPILL-O0-NEXT:    vse.v v12, (s1)
; SPILL-O0-NEXT:    vse.v v11, (s2)
; SPILL-O0-NEXT:    vse.v v10, (s3)
; SPILL-O0-NEXT:    vse.v v9, (s4)
; SPILL-O0-NEXT:    vse.v v8, (s5)
; SPILL-O0-NEXT:    vse.v v23, (s6)
; SPILL-O0-NEXT:    vse.v v22, (s7)
; SPILL-O0-NEXT:    vse.v v21, (s8)
; SPILL-O0-NEXT:    vse.v v20, (s9)
; SPILL-O0-NEXT:    vse.v v19, (s10)
; SPILL-O0-NEXT:    vse.v v18, (s11)
; SPILL-O0-NEXT:    ld a0, -144(s0)
; SPILL-O0-NEXT:    vse.v v17, (a0)
; SPILL-O0-NEXT:    ld a0, -152(s0)
; SPILL-O0-NEXT:    vse.v v16, (a0)
; SPILL-O0-NEXT:    ld a0, -160(s0)
; SPILL-O0-NEXT:    vse.v v7, (a0)
; SPILL-O0-NEXT:    ld a0, -168(s0)
; SPILL-O0-NEXT:    vse.v v6, (a0)
; SPILL-O0-NEXT:    vse.v v5, (a2)
; SPILL-O0-NEXT:    vse.v v4, (a3)
; SPILL-O0-NEXT:    vse.v v3, (a4)
; SPILL-O0-NEXT:    vse.v v2, (a5)
; SPILL-O0-NEXT:    vse.v v1, (a6)
; SPILL-O0-NEXT:    ld a2, -200(s0)
; SPILL-O0-NEXT:    rdvtype a1
; SPILL-O0-NEXT:    rdvl a0
; SPILL-O0-NEXT:    vsetvli zero, zero, e64, m1
; SPILL-O0-NEXT:    vle.v v0, (a2)
; SPILL-O0-NEXT:    vsetvl zero, a0, a1
; SPILL-O0-NEXT:    ld a0, -176(s0)
; SPILL-O0-NEXT:    vse.v v0, (a0)
; SPILL-O0-NEXT:    addi sp, s0, -208
; SPILL-O0-NEXT:    ld s11, 104(sp)
; SPILL-O0-NEXT:    ld s10, 112(sp)
; SPILL-O0-NEXT:    ld s9, 120(sp)
; SPILL-O0-NEXT:    ld s8, 128(sp)
; SPILL-O0-NEXT:    ld s7, 136(sp)
; SPILL-O0-NEXT:    ld s6, 144(sp)
; SPILL-O0-NEXT:    ld s5, 152(sp)
; SPILL-O0-NEXT:    ld s4, 160(sp)
; SPILL-O0-NEXT:    ld s3, 168(sp)
; SPILL-O0-NEXT:    ld s2, 176(sp)
; SPILL-O0-NEXT:    ld s1, 184(sp)
; SPILL-O0-NEXT:    ld s0, 192(sp)
; SPILL-O0-NEXT:    ld ra, 200(sp)
; SPILL-O0-NEXT:    addi sp, sp, 208
; SPILL-O0-NEXT:    ret
  %p1 = getelementptr inbounds i64, i64* @scratch, i64 1
  %p2 = getelementptr inbounds i64, i64* @scratch, i64 2
  %p3 = getelementptr inbounds i64, i64* @scratch, i64 3
  %p4 = getelementptr inbounds i64, i64* @scratch, i64 4
  %p5 = getelementptr inbounds i64, i64* @scratch, i64 5
  %p6 = getelementptr inbounds i64, i64* @scratch, i64 6
  %p7 = getelementptr inbounds i64, i64* @scratch, i64 7
  %p8 = getelementptr inbounds i64, i64* @scratch, i64 8
  %p9 = getelementptr inbounds i64, i64* @scratch, i64 9
  %p10 = getelementptr inbounds i64, i64* @scratch, i64 10
  %p11 = getelementptr inbounds i64, i64* @scratch, i64 11
  %p12 = getelementptr inbounds i64, i64* @scratch, i64 12
  %p13 = getelementptr inbounds i64, i64* @scratch, i64 13
  %p14 = getelementptr inbounds i64, i64* @scratch, i64 14
  %p15 = getelementptr inbounds i64, i64* @scratch, i64 15
  %p16 = getelementptr inbounds i64, i64* @scratch, i64 16
  %p17 = getelementptr inbounds i64, i64* @scratch, i64 17
  %p18 = getelementptr inbounds i64, i64* @scratch, i64 18
  %p19 = getelementptr inbounds i64, i64* @scratch, i64 19
  %p20 = getelementptr inbounds i64, i64* @scratch, i64 20
  %p21 = getelementptr inbounds i64, i64* @scratch, i64 21
  %p22 = getelementptr inbounds i64, i64* @scratch, i64 22
  %p23 = getelementptr inbounds i64, i64* @scratch, i64 23
  %p24 = getelementptr inbounds i64, i64* @scratch, i64 24
  %p25 = getelementptr inbounds i64, i64* @scratch, i64 25
  %p26 = getelementptr inbounds i64, i64* @scratch, i64 26
  %p27 = getelementptr inbounds i64, i64* @scratch, i64 27
  %p28 = getelementptr inbounds i64, i64* @scratch, i64 28
  %p29 = getelementptr inbounds i64, i64* @scratch, i64 29
  %p30 = getelementptr inbounds i64, i64* @scratch, i64 30
  %p31 = getelementptr inbounds i64, i64* @scratch, i64 31
  %pspill = getelementptr inbounds i64, i64* @scratch, i64 32

  %bc0 = bitcast i64* @scratch to <vscale x 1 x double>*
  %bc1 = bitcast i64* %p1 to <vscale x 1 x double>*
  %bc2 = bitcast i64* %p2 to <vscale x 1 x double>*
  %bc3 = bitcast i64* %p3 to <vscale x 1 x double>*
  %bc4 = bitcast i64* %p4 to <vscale x 1 x double>*
  %bc5 = bitcast i64* %p5 to <vscale x 1 x double>*
  %bc6 = bitcast i64* %p6 to <vscale x 1 x double>*
  %bc7 = bitcast i64* %p7 to <vscale x 1 x double>*
  %bc8 = bitcast i64* %p8 to <vscale x 1 x double>*
  %bc9 = bitcast i64* %p9 to <vscale x 1 x double>*
  %bc10 = bitcast i64* %p10 to <vscale x 1 x double>*
  %bc11 = bitcast i64* %p11 to <vscale x 1 x double>*
  %bc12 = bitcast i64* %p12 to <vscale x 1 x double>*
  %bc13 = bitcast i64* %p13 to <vscale x 1 x double>*
  %bc14 = bitcast i64* %p14 to <vscale x 1 x double>*
  %bc15 = bitcast i64* %p15 to <vscale x 1 x double>*
  %bc16 = bitcast i64* %p16 to <vscale x 1 x double>*
  %bc17 = bitcast i64* %p17 to <vscale x 1 x double>*
  %bc18 = bitcast i64* %p18 to <vscale x 1 x double>*
  %bc19 = bitcast i64* %p19 to <vscale x 1 x double>*
  %bc20 = bitcast i64* %p20 to <vscale x 1 x double>*
  %bc21 = bitcast i64* %p21 to <vscale x 1 x double>*
  %bc22 = bitcast i64* %p22 to <vscale x 1 x double>*
  %bc23 = bitcast i64* %p23 to <vscale x 1 x double>*
  %bc24 = bitcast i64* %p24 to <vscale x 1 x double>*
  %bc25 = bitcast i64* %p25 to <vscale x 1 x double>*
  %bc26 = bitcast i64* %p26 to <vscale x 1 x double>*
  %bc27 = bitcast i64* %p27 to <vscale x 1 x double>*
  %bc28 = bitcast i64* %p28 to <vscale x 1 x double>*
  %bc29 = bitcast i64* %p29 to <vscale x 1 x double>*
  %bc30 = bitcast i64* %p30 to <vscale x 1 x double>*
  %bc31 = bitcast i64* %p31 to <vscale x 1 x double>*
  %bcspill = bitcast i64* %pspill to <vscale x 1 x double>*

  %v0 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc0, i64 %avl)
  %v1 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc1, i64 %avl)
  %v2 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc2, i64 %avl)
  %v3 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc3, i64 %avl)
  %v4 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc4, i64 %avl)
  %v5 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc5, i64 %avl)
  %v6 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc6, i64 %avl)
  %v7 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc7, i64 %avl)
  %v8 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc8, i64 %avl)
  %v9 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc9, i64 %avl)
  %v10 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc10, i64 %avl)
  %v11 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc11, i64 %avl)
  %v12 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc12, i64 %avl)
  %v13 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc13, i64 %avl)
  %v14 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc14, i64 %avl)
  %v15 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc15, i64 %avl)
  %v16 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc16, i64 %avl)
  %v17 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc17, i64 %avl)
  %v18 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc18, i64 %avl)
  %v19 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc19, i64 %avl)
  %v20 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc20, i64 %avl)
  %v21 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc21, i64 %avl)
  %v22 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc22, i64 %avl)
  %v23 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc23, i64 %avl)
  %v24 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc24, i64 %avl)
  %v25 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc25, i64 %avl)
  %v26 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc26, i64 %avl)
  %v27 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc27, i64 %avl)
  %v28 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc28, i64 %avl)
  %v29 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc29, i64 %avl)
  %v30 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc30, i64 %avl)
  %v31 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc31, i64 %avl)
  %vspill = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bcspill, i64 %avl)

  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v0, <vscale x 1 x double>* %bc0, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v1, <vscale x 1 x double>* %bc1, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v2, <vscale x 1 x double>* %bc2, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v3, <vscale x 1 x double>* %bc3, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v4, <vscale x 1 x double>* %bc4, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v5, <vscale x 1 x double>* %bc5, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v6, <vscale x 1 x double>* %bc6, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v7, <vscale x 1 x double>* %bc7, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v8, <vscale x 1 x double>* %bc8, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v9, <vscale x 1 x double>* %bc9, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v10, <vscale x 1 x double>* %bc10, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v11, <vscale x 1 x double>* %bc11, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v12, <vscale x 1 x double>* %bc12, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v13, <vscale x 1 x double>* %bc13, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v14, <vscale x 1 x double>* %bc14, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v15, <vscale x 1 x double>* %bc15, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v16, <vscale x 1 x double>* %bc16, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v17, <vscale x 1 x double>* %bc17, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v18, <vscale x 1 x double>* %bc18, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v19, <vscale x 1 x double>* %bc19, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v20, <vscale x 1 x double>* %bc20, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v21, <vscale x 1 x double>* %bc21, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v22, <vscale x 1 x double>* %bc22, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v23, <vscale x 1 x double>* %bc23, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v24, <vscale x 1 x double>* %bc24, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v25, <vscale x 1 x double>* %bc25, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v26, <vscale x 1 x double>* %bc26, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v27, <vscale x 1 x double>* %bc27, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v28, <vscale x 1 x double>* %bc28, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v29, <vscale x 1 x double>* %bc29, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v30, <vscale x 1 x double>* %bc30, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v31, <vscale x 1 x double>* %bc31, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %vspill, <vscale x 1 x double>* %bcspill, i64 %avl)

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>*, i64)
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>*, i64)
