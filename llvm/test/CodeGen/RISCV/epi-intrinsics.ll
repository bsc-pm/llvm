; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+epi -verify-machineinstrs < %s \
; RUN:    | FileCheck %s

define void @test_setvl()
; CHECK-LABEL: test_setvl:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a0, a0, e8, m1
; CHECK-NEXT:    vsetvli a0, a0, e16, m1
; CHECK-NEXT:    vsetvli a0, a0, e32, m1
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vsetvli a0, a0, e128, m1
; CHECK-NEXT:    vsetvli a0, a0, e8, m2
; CHECK-NEXT:    vsetvli a0, a0, e8, m4
; CHECK-NEXT:    vsetvli a0, a0, e8, m8
; CHECK-NEXT:    ret
{
entry:
  %a1 = call i64 @llvm.epi.setvl(i64 undef, i64 0, i64 0)
  %a2 = call i64 @llvm.epi.setvl(i64 undef, i64 1, i64 0)
  %a3 = call i64 @llvm.epi.setvl(i64 undef, i64 2, i64 0)
  %a4 = call i64 @llvm.epi.setvl(i64 undef, i64 3, i64 0)
  %a5 = call i64 @llvm.epi.setvl(i64 undef, i64 4, i64 0)
  %a6 = call i64 @llvm.epi.setvl(i64 undef, i64 0, i64 1)
  %a7 = call i64 @llvm.epi.setvl(i64 undef, i64 0, i64 2)
  %a8 = call i64 @llvm.epi.setvl(i64 undef, i64 0, i64 3)
  ret void
}

declare i64 @llvm.epi.setvl(i64, i64, i64)

define i64 @test_vl()
; CHECK-LABEL: test_vl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    ret
{
  %a = call i64 @llvm.epi.readvl()
  ret i64 %a
}

declare i64 @llvm.epi.readvl()


define void @test_load_stores()
; CHECK-LABEL: test_load_stores:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e8, m1
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e8, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e16, m1
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e16, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e32, m1
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e32, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e64, m1
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e32, m1
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e32, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e64, m1
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    vsetvli zero, a0, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ret
{
  %a1 = call <vscale x 8 x i8> @llvm.epi.vload.nxv1i8(<vscale x 8 x i8>* undef, i64 undef)
  call void @llvm.epi.vstore.nxv1i8(<vscale x 8 x i8> %a1, <vscale x 8 x i8>* undef, i64 undef)

  %a2 = call <vscale x 4 x i16> @llvm.epi.vload.nxv1i16(<vscale x 4 x i16>* undef, i64 undef)
  call void @llvm.epi.vstore.nxv1i16(<vscale x 4 x i16> %a2, <vscale x 4 x i16>* undef, i64 undef)

  %a3 = call <vscale x 2 x i32> @llvm.epi.vload.nxv1i32(<vscale x 2 x i32>* undef, i64 undef)
  call void @llvm.epi.vstore.nxv1i32(<vscale x 2 x i32> %a3, <vscale x 2 x i32>* undef, i64 undef)

  %a4 = call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* undef, i64 undef)
  call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> %a4, <vscale x 1 x i64>* undef, i64 undef)

  %a5 = call <vscale x 2 x float> @llvm.epi.vload.nxv1f32(<vscale x 2 x float>* undef, i64 undef)
  call void @llvm.epi.vstore.nxv1f32(<vscale x 2 x float> %a5, <vscale x 2 x float>* undef, i64 undef)

  %a6 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* undef, i64 undef)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %a6, <vscale x 1 x double>* undef, i64 undef)

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vload.nxv1i8(<vscale x 8 x i8>*, i64)
declare void @llvm.epi.vstore.nxv1i8(<vscale x 8 x i8>, <vscale x 8 x i8>*, i64)

declare <vscale x 4 x i16> @llvm.epi.vload.nxv1i16(<vscale x 4 x i16>*, i64)
declare void @llvm.epi.vstore.nxv1i16(<vscale x 4 x i16>, <vscale x 4 x i16>*, i64)

declare <vscale x 2 x i32> @llvm.epi.vload.nxv1i32(<vscale x 2 x i32>*, i64)
declare void @llvm.epi.vstore.nxv1i32(<vscale x 2 x i32>, <vscale x 2 x i32>*, i64)

declare <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>*, i64)
declare void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>*, i64)

declare <vscale x 2 x float> @llvm.epi.vload.nxv1f32(<vscale x 2 x float>*, i64)
declare void @llvm.epi.vstore.nxv1f32(<vscale x 2 x float>, <vscale x 2 x float>*, i64)

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>*, i64)
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>*, i64)

