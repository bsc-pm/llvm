; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+v -verify-machineinstrs < %s | FileCheck %s

; NOTE: This test checks the immediate ranges for instructions that treat the
; immediate operand as an unsigned (uimm5: [0, 31]).

declare <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64(<vscale x 1 x i64>, i64, i64)
declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64(<vscale x 1 x i64>, i64, i64)
declare <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64(<vscale x 1 x i64>, i64, i64)
declare <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64(<vscale x 1 x i64>, i64, i64)
declare <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64(<vscale x 1 x i64>, i64, i64)
declare <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64(<vscale x 1 x i64>, i64, i64)
declare <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64(<vscale x 1 x i64>, i64, i64)
declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64(<vscale x 1 x i64>, i64, i64)

declare <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i64(<vscale x 2 x i64>, i64, i64)
declare <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i64(<vscale x 2 x i64>, i64, i64)
declare <vscale x 2 x i32> @llvm.epi.vnclip.nxv2i64(<vscale x 2 x i64>, i64, i64)
declare <vscale x 2 x i32> @llvm.epi.vnclipu.nxv2i64(<vscale x 2 x i64>, i64, i64)

@scratch = global i8 0, align 16

define <vscale x 1 x i64> @foo(<vscale x 1 x i64> %v, <vscale x 2 x i64> %w, i64 %gvl) nounwind
; CHECK-LABEL: foo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a0, a0, e64,m1
; CHECK-NEXT:    vsll.vi v0, v16, 0
; CHECK-NEXT:    vsll.vi v0, v0, 31
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    vsll.vx v0, v0, a0
; CHECK-NEXT:    addi a1, zero, 32
; CHECK-NEXT:    vsll.vx v0, v0, a1
; CHECK-NEXT:    vsrl.vi v0, v0, 0
; CHECK-NEXT:    vsrl.vi v0, v0, 31
; CHECK-NEXT:    vsrl.vx v0, v0, a0
; CHECK-NEXT:    vsrl.vx v0, v0, a1
; CHECK-NEXT:    vsra.vi v0, v0, 0
; CHECK-NEXT:    vsra.vi v0, v0, 31
; CHECK-NEXT:    vsra.vx v0, v0, a0
; CHECK-NEXT:    vsra.vx v0, v0, a1
; CHECK-NEXT:    vssrl.vi v0, v0, 0
; CHECK-NEXT:    vssrl.vi v0, v0, 31
; CHECK-NEXT:    vssrl.vx v0, v0, a0
; CHECK-NEXT:    vssrl.vx v0, v0, a1
; CHECK-NEXT:    vssra.vi v0, v0, 0
; CHECK-NEXT:    vssra.vi v0, v0, 31
; CHECK-NEXT:    vssra.vx v0, v0, a0
; CHECK-NEXT:    vssra.vx v0, v0, a1
; CHECK-NEXT:    vslideup.vi v1, v0, 0
; CHECK-NEXT:    vslideup.vi v0, v1, 31
; CHECK-NEXT:    vslideup.vx v1, v0, a0
; CHECK-NEXT:    vslideup.vx v0, v1, a1
; CHECK-NEXT:    vslidedown.vi v0, v0, 0
; CHECK-NEXT:    vslidedown.vi v0, v0, 31
; CHECK-NEXT:    vslidedown.vx v0, v0, a0
; CHECK-NEXT:    vslidedown.vx v0, v0, a1
; CHECK-NEXT:    vrgather.vi v1, v0, 0
; CHECK-NEXT:    vrgather.vi v0, v1, 31
; CHECK-NEXT:    vrgather.vx v1, v0, a0
; CHECK-NEXT:    vrgather.vx v16, v1, a1
; CHECK-NEXT:    ret
{
  %1 = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64(<vscale x 1 x i64> %v, i64 0, i64 %gvl)
  %2 = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64(<vscale x 1 x i64> %1, i64 31, i64 %gvl)
  %3 = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64(<vscale x 1 x i64> %2, i64 -1, i64 %gvl)
  %4 = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64(<vscale x 1 x i64> %3, i64 32, i64 %gvl)

  %5 = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64(<vscale x 1 x i64> %4, i64 0, i64 %gvl)
  %6 = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64(<vscale x 1 x i64> %5, i64 31, i64 %gvl)
  %7 = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64(<vscale x 1 x i64> %6, i64 -1, i64 %gvl)
  %8 = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64(<vscale x 1 x i64> %7, i64 32, i64 %gvl)

  %9 = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64(<vscale x 1 x i64> %8, i64 0, i64 %gvl)
  %10 = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64(<vscale x 1 x i64> %9, i64 31, i64 %gvl)
  %11 = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64(<vscale x 1 x i64> %10, i64 -1, i64 %gvl)
  %12 = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64(<vscale x 1 x i64> %11, i64 32, i64 %gvl)

  %13 = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64(<vscale x 1 x i64> %12, i64 0, i64 %gvl)
  %14 = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64(<vscale x 1 x i64> %13, i64 31, i64 %gvl)
  %15 = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64(<vscale x 1 x i64> %14, i64 -1, i64 %gvl)
  %16 = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64(<vscale x 1 x i64> %15, i64 32, i64 %gvl)

  %17 = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64(<vscale x 1 x i64> %16, i64 0, i64 %gvl)
  %18 = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64(<vscale x 1 x i64> %17, i64 31, i64 %gvl)
  %19 = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64(<vscale x 1 x i64> %18, i64 -1, i64 %gvl)
  %20 = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64(<vscale x 1 x i64> %19, i64 32, i64 %gvl)

  %21 = call <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64(<vscale x 1 x i64> %20, i64 0, i64 %gvl)
  %22 = call <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64(<vscale x 1 x i64> %21, i64 31, i64 %gvl)
  %23 = call <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64(<vscale x 1 x i64> %22, i64 -1, i64 %gvl)
  %24 = call <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64(<vscale x 1 x i64> %23, i64 32, i64 %gvl)

  %25 = call <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64(<vscale x 1 x i64> %24, i64 0, i64 %gvl)
  %26 = call <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64(<vscale x 1 x i64> %25, i64 31, i64 %gvl)
  %27 = call <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64(<vscale x 1 x i64> %26, i64 -1, i64 %gvl)
  %28 = call <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64(<vscale x 1 x i64> %27, i64 32, i64 %gvl)

  %29 = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64(<vscale x 1 x i64> %28, i64 0, i64 %gvl)
  %30 = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64(<vscale x 1 x i64> %29, i64 31, i64 %gvl)
  %31 = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64(<vscale x 1 x i64> %30, i64 -1, i64 %gvl)
  %32 = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64(<vscale x 1 x i64> %31, i64 32, i64 %gvl)


  ; FIXME enable when supported
  ;%33 = call <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i64(<vscale x 2 x i64> %w, i64 0, i64 %gvl)
  ;%34 = call <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i64(<vscale x 2 x i64> %w, i64 31, i64 %gvl)
  ;%35 = call <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i64(<vscale x 2 x i64> %w, i64 -1, i64 %gvl)
  ;%36 = call <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i64(<vscale x 2 x i64> %w, i64 32, i64 %gvl)

  ;%37 = call <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i64(<vscale x 2 x i64> %w, i64 0, i64 %gvl)
  ;%38 = call <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i64(<vscale x 2 x i64> %w, i64 31, i64 %gvl)
  ;%39 = call <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i64(<vscale x 2 x i64> %w, i64 -1, i64 %gvl)
  ;%40 = call <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i64(<vscale x 2 x i64> %w, i64 32, i64 %gvl)

  ;%41 = call <vscale x 2 x i32> @llvm.epi.vnclip.nxv2i64(<vscale x 2 x i64> %w, i64 0, i64 %gvl)
  ;%42 = call <vscale x 2 x i32> @llvm.epi.vnclip.nxv2i64(<vscale x 2 x i64> %w, i64 31, i64 %gvl)
  ;%43 = call <vscale x 2 x i32> @llvm.epi.vnclip.nxv2i64(<vscale x 2 x i64> %w, i64 -1, i64 %gvl)
  ;%44 = call <vscale x 2 x i32> @llvm.epi.vnclip.nxv2i64(<vscale x 2 x i64> %w, i64 32, i64 %gvl)

  ;%45 = call <vscale x 2 x i32> @llvm.epi.vnclipu.nxv2i64(<vscale x 2 x i64> %w, i64 0, i64 %gvl)
  ;%46 = call <vscale x 2 x i32> @llvm.epi.vnclipu.nxv2i64(<vscale x 2 x i64> %w, i64 31, i64 %gvl)
  ;%47 = call <vscale x 2 x i32> @llvm.epi.vnclipu.nxv2i64(<vscale x 2 x i64> %w, i64 -1, i64 %gvl)
  ;%48 = call <vscale x 2 x i32> @llvm.epi.vnclipu.nxv2i64(<vscale x 2 x i64> %w, i64 32, i64 %gvl)

  ;%49 = add <vscale x 2 x i32> %33, %34
  ;%50 = add <vscale x 2 x i32> %49, %35
  ;%51 = add <vscale x 2 x i32> %50, %36
  ;%52 = add <vscale x 2 x i32> %51, %37
  ;%53 = add <vscale x 2 x i32> %52, %38
  ;%54 = add <vscale x 2 x i32> %53, %39
  ;%55 = add <vscale x 2 x i32> %54, %40
  ;%56 = add <vscale x 2 x i32> %55, %41
  ;%57 = add <vscale x 2 x i32> %56, %42
  ;%58 = add <vscale x 2 x i32> %57, %43
  ;%59 = add <vscale x 2 x i32> %58, %44
  ;%60 = add <vscale x 2 x i32> %59, %45
  ;%61 = add <vscale x 2 x i32> %60, %46
  ;%62 = add <vscale x 2 x i32> %61, %47
  ;%63 = add <vscale x 2 x i32> %62, %48

  ;%p = bitcast i8* @scratch to <vscale x 2 x i32>*
  ;store <vscale x 2 x i32> %63, <vscale x 2 x i32>* %p

  ret <vscale x 1 x i64> %32
}
