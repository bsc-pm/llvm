; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+epi -verify-machineinstrs -O0 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+epi -verify-machineinstrs -O2 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

declare i64 @llvm.epi.vsetvl(
  i64, i64, i64);

declare i64 @llvm.epi.vsetvlmax(
  i64, i64);

declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
  <vscale x 1 x double>*,
  i64);

declare void @llvm.epi.vstore.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64);

define void @test_vsetvl_avl(<vscale x 1 x double>* %v, i64 signext %avl)
; CHECK-O0-LABEL: test_vsetvl_avl:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 16
; CHECK-O0-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O0-NEXT:    vsetvli a3, a2, e64, m1
; CHECK-O0-NEXT:    vle.v v0, (a0)
; CHECK-O0-NEXT:    vsetvli a3, a2, e64, m1
; CHECK-O0-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O0-NEXT:    lui a3, %hi(scratch)
; CHECK-O0-NEXT:    addi a3, a3, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a4, a2, e64, m1
; CHECK-O0-NEXT:    vse.v v0, (a3)
; CHECK-O0-NEXT:    sd a1, 8(sp)
; CHECK-O0-NEXT:    sd a0, 0(sp)
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_avl:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1
; CHECK-O2-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O2-NEXT:    vle.v v0, (a0)
; CHECK-O2-NEXT:    vsetvli a0, a1, e64, m1
; CHECK-O2-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O2-NEXT:    vse.v v0, (a0)
; CHECK-O2-NEXT:    ret
{
  %gvl = call i64 @llvm.epi.vsetvl(
    i64 %avl, i64 3, i64 0)

  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 %gvl)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 %gvl)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 %gvl)

    ret void
}

define void @test_vsetvl_zero(<vscale x 1 x double>* %v)
; CHECK-O0-LABEL: test_vsetvl_zero:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 16
; CHECK-O0-NEXT:    mv a1, zero
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1
; CHECK-O0-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O0-NEXT:    vle.v v0, (a0)
; CHECK-O0-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O0-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O0-NEXT:    lui a2, %hi(scratch)
; CHECK-O0-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, a1, e64, m1
; CHECK-O0-NEXT:    vse.v v0, (a2)
; CHECK-O0-NEXT:    sd a0, 8(sp)
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_zero:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    mv a1, zero
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1
; CHECK-O2-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O2-NEXT:    vle.v v0, (a0)
; CHECK-O2-NEXT:    vsetvli a0, a1, e64, m1
; CHECK-O2-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O2-NEXT:    vse.v v0, (a0)
; CHECK-O2-NEXT:    ret
{
  %gvl = call i64 @llvm.epi.vsetvl(
    i64 0, i64 3, i64 0)

  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 %gvl)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 %gvl)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 %gvl)

    ret void
}

define void @test_vsetvlmax(<vscale x 1 x double>* %v)
; CHECK-O0-LABEL: test_vsetvlmax:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 16
; CHECK-O0-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-O0-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O0-NEXT:    vle.v v0, (a0)
; CHECK-O0-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O0-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O0-NEXT:    lui a2, %hi(scratch)
; CHECK-O0-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, a1, e64, m1
; CHECK-O0-NEXT:    vse.v v0, (a2)
; CHECK-O0-NEXT:    sd a0, 8(sp)
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvlmax:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-O2-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O2-NEXT:    vle.v v0, (a0)
; CHECK-O2-NEXT:    vsetvli a0, a1, e64, m1
; CHECK-O2-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O2-NEXT:    vse.v v0, (a0)
; CHECK-O2-NEXT:    ret
{
  %vlmax = call i64 @llvm.epi.vsetvlmax(
    i64 3, i64 0)

  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 %vlmax)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 %vlmax)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 %vlmax)

    ret void
}

define void @test_gvl_zero(<vscale x 1 x double>* %v)
; CHECK-O0-LABEL: test_gvl_zero:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 16
; CHECK-O0-NEXT:    mv a1, zero
; CHECK-O0-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O0-NEXT:    vle.v v0, (a0)
; CHECK-O0-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O0-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O0-NEXT:    lui a2, %hi(scratch)
; CHECK-O0-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, a1, e64, m1
; CHECK-O0-NEXT:    vse.v v0, (a2)
; CHECK-O0-NEXT:    sd a0, 8(sp)
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_gvl_zero:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    mv a1, zero
; CHECK-O2-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O2-NEXT:    vle.v v0, (a0)
; CHECK-O2-NEXT:    vsetvli a0, a1, e64, m1
; CHECK-O2-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O2-NEXT:    vse.v v0, (a0)
; CHECK-O2-NEXT:    ret
{
  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 0)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 0)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 0)

    ret void
}

define void @test_implicit_vlmax(<vscale x 1 x double>* %v)
; CHECK-O0-LABEL: test_implicit_vlmax:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 16
; CHECK-O0-NEXT:    lui a1, %hi(scratch)
; CHECK-O0-NEXT:    addi a1, a1, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a2, zero, e64, m1
; CHECK-O0-NEXT:    vle.v v0, (a0)
; CHECK-O0-NEXT:    vsetvli a2, zero, e64, m1
; CHECK-O0-NEXT:    vse.v v0, (a1)
; CHECK-O0-NEXT:    sd a0, 8(sp)
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_implicit_vlmax:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-O2-NEXT:    vle.v v0, (a0)
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-O2-NEXT:    vse.v v0, (a0)
; CHECK-O2-NEXT:    ret
{
  %vec = load <vscale x 1 x double>, <vscale x 1 x double>* %v

  ; FIXME uncomment when 'fadd' pattern is implemented
  ;%add = fadd <vscale x 1 x double> %vec, %vec

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  store <vscale x 1 x double> %vec, <vscale x 1 x double>* %store_addr

  ret void
}
