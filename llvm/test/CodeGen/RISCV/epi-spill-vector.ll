; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+epi -verify-machineinstrs -O0 < %s \
; RUN:    | FileCheck --check-prefix=SPILL-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+epi -verify-machineinstrs -O2 < %s \
; RUN:    | FileCheck --check-prefix=SPILL-O2 %s

define void @builtins_f64(<vscale x 1 x double>* %vaddr) nounwind {
; SPILL-O0-LABEL: builtins_f64:
; SPILL-O0:       # %bb.0: # %entry
; SPILL-O0-NEXT:    addi sp, sp, -48
; SPILL-O0-NEXT:    sd ra, 40(sp)
; SPILL-O0-NEXT:    sd s0, 32(sp)
; SPILL-O0-NEXT:    addi s0, sp, 48
; SPILL-O0-NEXT:    vsetvli a1, zero, e64, m1
; SPILL-O0-NEXT:    slli a1, a1, 3
; SPILL-O0-NEXT:    sub sp, sp, a1
; SPILL-O0-NEXT:    andi sp, sp, -16
; SPILL-O0-NEXT:    sd sp, -24(s0)
; SPILL-O0-NEXT:    sub sp, sp, a1
; SPILL-O0-NEXT:    andi sp, sp, -16
; SPILL-O0-NEXT:    sd sp, -32(s0)
; SPILL-O0-NEXT:    vle.v v0, (a0)
; SPILL-O0:    ld a1, -32(s0)
; SPILL-O0-NEXT:    vse.v v1, (a1)
; SPILL-O0-NEXT:    ld a1, -24(s0)
; SPILL-O0-NEXT:    vle.v v1, (a1)
; SPILL-O0-NEXT:    vfadd.vv v0, v0, v1
; SPILL-O0-NEXT:    ld a1, -32(s0)
; SPILL-O0-NEXT:    vle.v v1, (a1)
; SPILL-O0:    vse.v v0, (a0)
; SPILL-O0-NEXT:    sd a0, -40(s0)
; SPILL-O0-NEXT:    addi sp, s0, -48
; SPILL-O0-NEXT:    ld s0, 32(sp)
; SPILL-O0-NEXT:    ld ra, 40(sp)
; SPILL-O0-NEXT:    addi sp, sp, 48
; SPILL-O0-NEXT:    ret
;
; SPILL-O2-LABEL: builtins_f64:
; SPILL-O2:       # %bb.0: # %entry
; SPILL-O2-NEXT:    addi sp, sp, -32
; SPILL-O2-NEXT:    sd ra, 24(sp)
; SPILL-O2-NEXT:    sd s0, 16(sp)
; SPILL-O2-NEXT:    addi s0, sp, 32
; SPILL-O2-NEXT:    vsetvli a1, zero, e64, m1
; SPILL-O2-NEXT:    slli a1, a1, 3
; SPILL-O2-NEXT:    sub sp, sp, a1
; SPILL-O2-NEXT:    andi sp, sp, -16
; SPILL-O2-NEXT:    sd sp, -24(s0)
; SPILL-O2-NEXT:    sub sp, sp, a1
; SPILL-O2-NEXT:    andi sp, sp, -16
; SPILL-O2-NEXT:    sd sp, -32(s0)
; SPILL-O2-NEXT:    vle.v v0, (a0)
; SPILL-O2-NEXT:    ld a1, -24(s0)
; SPILL-O2-NEXT:    vse.v v0, (a1)
; SPILL-O2-NEXT:    vfadd.vv v0, v0, v0
; SPILL-O2-NEXT:    ld a1, -32(s0)
; SPILL-O2:    ld a1, -24(s0)
; SPILL-O2-NEXT:    vle.v v1, (a1)
; SPILL-O2-NEXT:    vfadd.vv v0, v0, v1
; SPILL-O2-NEXT:    ld a1, -32(s0)
; SPILL-O2-NEXT:    vle.v v1, (a1)
; SPILL-O2:    vse.v v0, (a0)
; SPILL-O2-NEXT:    addi sp, s0, -32
; SPILL-O2-NEXT:    ld s0, 16(sp)
; SPILL-O2-NEXT:    ld ra, 24(sp)
; SPILL-O2-NEXT:    addi sp, sp, 32
; SPILL-O2-NEXT:    ret


entry:
  %v10 = load <vscale x 1 x double>, <vscale x 1 x double>* %vaddr, align 8

%v11 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v10, <vscale x 1 x double> %v10)
%v12 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v11, <vscale x 1 x double> %v11)
%v13 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v12, <vscale x 1 x double> %v12)
%v14 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v13, <vscale x 1 x double> %v13)
%v15 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v14, <vscale x 1 x double> %v14)
%v16 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v15, <vscale x 1 x double> %v15)
%v17 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v16, <vscale x 1 x double> %v16)
%v18 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v17, <vscale x 1 x double> %v17)
%v19 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v18, <vscale x 1 x double> %v18)
%v20 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v19, <vscale x 1 x double> %v19)
%v21 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v20, <vscale x 1 x double> %v20)
%v22 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v21, <vscale x 1 x double> %v21)
%v23 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v22, <vscale x 1 x double> %v22)
%v24 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v23, <vscale x 1 x double> %v23)
%v25 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v24, <vscale x 1 x double> %v24)
%v26 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v25, <vscale x 1 x double> %v25)
%v27 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v26, <vscale x 1 x double> %v26)
%v28 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v27, <vscale x 1 x double> %v27)
%v29 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v28, <vscale x 1 x double> %v28)
%v30 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v29, <vscale x 1 x double> %v29)
%v31 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v30, <vscale x 1 x double> %v30)
%v32 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v31, <vscale x 1 x double> %v31)
%v33 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v32, <vscale x 1 x double> %v32)
%v34 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v33, <vscale x 1 x double> %v33)
%v35 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v34, <vscale x 1 x double> %v34)
%v36 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v35, <vscale x 1 x double> %v35)
%v37 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v36, <vscale x 1 x double> %v36)
%v38 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v37, <vscale x 1 x double> %v37)
%v39 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v38, <vscale x 1 x double> %v38)
%v40 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v39, <vscale x 1 x double> %v39)
%v41 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v40, <vscale x 1 x double> %v40)
%v42 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v41, <vscale x 1 x double> %v41)
%v43 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v42, <vscale x 1 x double> %v42)
%v44 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v43, <vscale x 1 x double> %v43)
%v45 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v44, <vscale x 1 x double> %v44)
%v46 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v45, <vscale x 1 x double> %v45)
%v47 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v46, <vscale x 1 x double> %v46)
%v48 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v47, <vscale x 1 x double> %v47)
%v49 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v48, <vscale x 1 x double> %v48)
%v50 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v49, <vscale x 1 x double> %v49)
%v51 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v50, <vscale x 1 x double> %v50)
%v52 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v51, <vscale x 1 x double> %v51)
%v53 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v52, <vscale x 1 x double> %v52)
%v54 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v53, <vscale x 1 x double> %v53)
%v55 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v54, <vscale x 1 x double> %v54)
%v56 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v55, <vscale x 1 x double> %v55)
%v57 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v56, <vscale x 1 x double> %v56)
%v58 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v57, <vscale x 1 x double> %v57)
%v59 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v58, <vscale x 1 x double> %v58)
%v60 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v59, <vscale x 1 x double> %v59)
%v61 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v60, <vscale x 1 x double> %v60)
%v62 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v61, <vscale x 1 x double> %v61)
%v63 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v62, <vscale x 1 x double> %v62)
%v64 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v63, <vscale x 1 x double> %v63)
%v65 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v64, <vscale x 1 x double> %v64)
%v66 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v65, <vscale x 1 x double> %v65)
%v67 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v66, <vscale x 1 x double> %v66)
%v68 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v67, <vscale x 1 x double> %v67)
%v69 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v68, <vscale x 1 x double> %v68)
%v70 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v69, <vscale x 1 x double> %v69)
%v71 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v70, <vscale x 1 x double> %v10)
%v72 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v71, <vscale x 1 x double> %v11)
%v73 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v72, <vscale x 1 x double> %v12)
%v74 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v73, <vscale x 1 x double> %v13)
%v75 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v74, <vscale x 1 x double> %v14)
%v76 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v75, <vscale x 1 x double> %v15)
%v77 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v76, <vscale x 1 x double> %v16)
%v78 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v77, <vscale x 1 x double> %v17)
%v79 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v78, <vscale x 1 x double> %v18)
%v80 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v79, <vscale x 1 x double> %v19)
%v81 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v80, <vscale x 1 x double> %v20)
%v82 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v81, <vscale x 1 x double> %v21)
%v83 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v82, <vscale x 1 x double> %v22)
%v84 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v83, <vscale x 1 x double> %v23)
%v85 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v84, <vscale x 1 x double> %v24)
%v86 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v85, <vscale x 1 x double> %v25)
%v87 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v86, <vscale x 1 x double> %v26)
%v88 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v87, <vscale x 1 x double> %v27)
%v89 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v88, <vscale x 1 x double> %v28)
%v90 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v89, <vscale x 1 x double> %v29)
%v91 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v90, <vscale x 1 x double> %v30)
%v92 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v91, <vscale x 1 x double> %v31)
%v93 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v92, <vscale x 1 x double> %v32)
%v94 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v93, <vscale x 1 x double> %v33)
%v95 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v94, <vscale x 1 x double> %v34)
%v96 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v95, <vscale x 1 x double> %v35)
%v97 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v96, <vscale x 1 x double> %v36)
%v98 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v97, <vscale x 1 x double> %v37)
%v99 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v98, <vscale x 1 x double> %v38)
%v100 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v99, <vscale x 1 x double> %v39)
%v101 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v100, <vscale x 1 x double> %v40)
%v102 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double> %v101, <vscale x 1 x double> %v41)

  store <vscale x 1 x double> %v102, <vscale x 1 x double>* %vaddr, align 8
  ret void
}

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>)
