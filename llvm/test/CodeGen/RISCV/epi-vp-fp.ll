; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+v -verify-machineinstrs -O0 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+v -verify-machineinstrs -O2 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

define void @test_vp_fp(<vscale x 1 x double>* %a0, <vscale x 1 x double>* %a1, <vscale x 1 x double>* %a2, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fp:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    sd ra, 40(sp)
; CHECK-O0-NEXT:    sd s0, 32(sp)
; CHECK-O0-NEXT:    addi s0, sp, 48
; CHECK-O0-NEXT:    rdvlenb a4
; CHECK-O0-NEXT:    sub sp, sp, a4
; CHECK-O0-NEXT:    sd sp, -40(s0)
; CHECK-O0-NEXT:    # kill: def $x14 killed $x13
; CHECK-O0-NEXT:    lui a4, %hi(scratch)
; CHECK-O0-NEXT:    addi a4, a4, %lo(scratch)
; CHECK-O0-NEXT:    ld a5, -40(s0)
; CHECK-O0-NEXT:    vs1r.v v0, (a5)
; CHECK-O0-NEXT:    vsetvli a3, a3, e64,m1
; CHECK-O0-NEXT:    vle.v v1, (a0), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vle.v v2, (a1), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vle.v v3, (a2), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfadd.vv v4, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfsub.vv v5, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfmul.vv v6, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfdiv.vv v7, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfmacc.vv v1, v2, v3, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v4, (a4), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v5, (a4), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v6, (a4), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v7, (a4), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v1, (a4), v0.t
; CHECK-O0-NEXT:    addi sp, s0, -48
; CHECK-O0-NEXT:    ld s0, 32(sp)
; CHECK-O0-NEXT:    ld ra, 40(sp)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fp:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a3, a3, e64,m1
; CHECK-O2-NEXT:    vle.v v1, (a0), v0.t
; CHECK-O2-NEXT:    vle.v v2, (a1), v0.t
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vle.v v3, (a2), v0.t
; CHECK-O2-NEXT:    vfadd.vv v4, v1, v2, v0.t
; CHECK-O2-NEXT:    vfsub.vv v5, v1, v2, v0.t
; CHECK-O2-NEXT:    vfmul.vv v6, v1, v2, v0.t
; CHECK-O2-NEXT:    vfdiv.vv v7, v1, v2, v0.t
; CHECK-O2-NEXT:    vfmacc.vv v1, v2, v3, v0.t
; CHECK-O2-NEXT:    vse.v v4, (a0), v0.t
; CHECK-O2-NEXT:    vse.v v5, (a0), v0.t
; CHECK-O2-NEXT:    vse.v v6, (a0), v0.t
; CHECK-O2-NEXT:    vse.v v7, (a0), v0.t
; CHECK-O2-NEXT:    vse.v v1, (a0), v0.t
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  %i0 = call <vscale x 1 x double> @llvm.vp.load.nxv1f64(<vscale x 1 x double>* %a0, i32 8, <vscale x 1 x i1> %m, i32 %n)
  %i1 = call <vscale x 1 x double> @llvm.vp.load.nxv1f64(<vscale x 1 x double>* %a1, i32 8, <vscale x 1 x i1> %m, i32 %n)
  %i2 = call <vscale x 1 x double> @llvm.vp.load.nxv1f64(<vscale x 1 x double>* %a2, i32 8, <vscale x 1 x i1> %m, i32 %n)

  %r0 = call <vscale x 1 x double> @llvm.vp.fadd.nxv1f64(<vscale x 1 x double> %i0, <vscale x 1 x double> %i1, metadata !"", metadata !"", <vscale x 1 x i1> %m, i32 %n)
  %r1 = call <vscale x 1 x double> @llvm.vp.fsub.nxv1f64(<vscale x 1 x double> %i0, <vscale x 1 x double> %i1, metadata !"", metadata !"", <vscale x 1 x i1> %m, i32 %n)
  %r2 = call <vscale x 1 x double> @llvm.vp.fmul.nxv1f64(<vscale x 1 x double> %i0, <vscale x 1 x double> %i1, metadata !"", metadata !"", <vscale x 1 x i1> %m, i32 %n)
  %r3 = call <vscale x 1 x double> @llvm.vp.fdiv.nxv1f64(<vscale x 1 x double> %i0, <vscale x 1 x double> %i1, metadata !"", metadata !"", <vscale x 1 x i1> %m, i32 %n)
  ;%r4 = call <vscale x 1 x double> @llvm.vp.frem.nxv1f64(<vscale x 1 x double> %i0, <vscale x 1 x double> %i1, metadata !"", metadata !"", <vscale x 1 x i1> %m, i32 %n)
  %r5 = call <vscale x 1 x double> @llvm.vp.fma.nxv1f64(<vscale x 1 x double> %i0, <vscale x 1 x double> %i1, <vscale x 1 x double> %i2, metadata !"", metadata !"", <vscale x 1 x i1> %m, i32 %n)
  ;%r6 = call <vscale x 1 x double> @llvm.vp.fneg.nxv1f64(<vscale x 1 x double> %i0, metadata !"", <vscale x 1 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %r0, <vscale x 1 x double>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %r1, <vscale x 1 x double>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %r2, <vscale x 1 x double>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %r3, <vscale x 1 x double>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  ;call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %r4, <vscale x 1 x double>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %r5, <vscale x 1 x double>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  ;call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %r6, <vscale x 1 x double>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)

  ret void
}

define void @test_vp_fp_2(<vscale x 2 x float>* %a0, <vscale x 2 x float>* %a1, <vscale x 2 x float>* %a2, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fp_2:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    # kill: def $x14 killed $x13
; CHECK-O0-NEXT:    lui a4, %hi(scratch)
; CHECK-O0-NEXT:    addi a4, a4, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, a3, e32,m1
; CHECK-O0-NEXT:    vle.v v1, (a0)
; CHECK-O0-NEXT:    vle.v v2, (a1)
; CHECK-O0-NEXT:    vle.v v3, (a2)
; CHECK-O0-NEXT:    vfadd.vv v4, v1, v2
; CHECK-O0-NEXT:    vfsub.vv v5, v1, v2
; CHECK-O0-NEXT:    vfmul.vv v6, v1, v2
; CHECK-O0-NEXT:    vfdiv.vv v7, v1, v2
; CHECK-O0-NEXT:    vfmacc.vv v1, v2, v3
; CHECK-O0-NEXT:    vse.v v4, (a4)
; CHECK-O0-NEXT:    vse.v v5, (a4)
; CHECK-O0-NEXT:    vse.v v6, (a4)
; CHECK-O0-NEXT:    vse.v v7, (a4)
; CHECK-O0-NEXT:    vse.v v1, (a4)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fp_2:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a3, a3, e32,m1
; CHECK-O2-NEXT:    vle.v v1, (a0)
; CHECK-O2-NEXT:    vle.v v2, (a1)
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vle.v v3, (a2)
; CHECK-O2-NEXT:    vfadd.vv v4, v1, v2
; CHECK-O2-NEXT:    vfsub.vv v5, v1, v2
; CHECK-O2-NEXT:    vfmul.vv v6, v1, v2
; CHECK-O2-NEXT:    vfdiv.vv v7, v1, v2
; CHECK-O2-NEXT:    vfmacc.vv v1, v2, v3
; CHECK-O2-NEXT:    vse.v v4, (a0)
; CHECK-O2-NEXT:    vse.v v5, (a0)
; CHECK-O2-NEXT:    vse.v v6, (a0)
; CHECK-O2-NEXT:    vse.v v7, (a0)
; CHECK-O2-NEXT:    vse.v v1, (a0)
; CHECK-O2-NEXT:    ret
  %head = insertelement <vscale x 2 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 2 x i1> %head, <vscale x 2 x i1> undef, <vscale x 2 x i32> zeroinitializer

  %store_addr = bitcast i8* @scratch to <vscale x 2 x float>*

  %i0 = call <vscale x 2 x float> @llvm.vp.load.nxv2f32(<vscale x 2 x float>* %a0, i32 8, <vscale x 2 x i1> %allones, i32 %n)
  %i1 = call <vscale x 2 x float> @llvm.vp.load.nxv2f32(<vscale x 2 x float>* %a1, i32 8, <vscale x 2 x i1> %allones, i32 %n)
  %i2 = call <vscale x 2 x float> @llvm.vp.load.nxv2f32(<vscale x 2 x float>* %a2, i32 8, <vscale x 2 x i1> %allones, i32 %n)

  %r0 = call <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> %i0, <vscale x 2 x float> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %allones, i32 %n)
  %r1 = call <vscale x 2 x float> @llvm.vp.fsub.nxv2f32(<vscale x 2 x float> %i0, <vscale x 2 x float> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %allones, i32 %n)
  %r2 = call <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> %i0, <vscale x 2 x float> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %allones, i32 %n)
  %r3 = call <vscale x 2 x float> @llvm.vp.fdiv.nxv2f32(<vscale x 2 x float> %i0, <vscale x 2 x float> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %allones, i32 %n)
  ;%r4 = call <vscale x 2 x float> @llvm.vp.frem.nxv2f32(<vscale x 2 x float> %i0, <vscale x 2 x float> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %allones, i32 %n)
  %r5 = call <vscale x 2 x float> @llvm.vp.fma.nxv2f32(<vscale x 2 x float> %i0, <vscale x 2 x float> %i1, <vscale x 2 x float> %i2, metadata !"", metadata !"", <vscale x 2 x i1> %allones, i32 %n)
  ;%r6 = call <vscale x 2 x float> @llvm.vp.fneg.nxv2f32(<vscale x 2 x float> %i0, metadata !"", <vscale x 2 x i1> %allones, i32 %n)

  call void @llvm.vp.store.nxv2f32(<vscale x 2 x float> %r0, <vscale x 2 x float>* %store_addr, i32 8, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2f32(<vscale x 2 x float> %r1, <vscale x 2 x float>* %store_addr, i32 8, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2f32(<vscale x 2 x float> %r2, <vscale x 2 x float>* %store_addr, i32 8, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2f32(<vscale x 2 x float> %r3, <vscale x 2 x float>* %store_addr, i32 8, <vscale x 2 x i1> %allones, i32 %n)
  ;call void @llvm.vp.store.nxv2f32(<vscale x 2 x float> %r4, <vscale x 2 x float>* %store_addr, i32 8, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2f32(<vscale x 2 x float> %r5, <vscale x 2 x float>* %store_addr, i32 8, <vscale x 2 x i1> %allones, i32 %n)
  ;call void @llvm.vp.store.nxv2f32(<vscale x 2 x float> %r6, <vscale x 2 x float>* %store_addr, i32 8, <vscale x 2 x i1> %allones, i32 %n)

  ret void
}

define void @test_vp_fp_3(<vscale x 2 x double>* %a0, <vscale x 2 x double>* %a1, <vscale x 2 x double>* %a2, <vscale x 2 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fp_3:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    sd ra, 40(sp)
; CHECK-O0-NEXT:    sd s0, 32(sp)
; CHECK-O0-NEXT:    addi s0, sp, 48
; CHECK-O0-NEXT:    rdvlenb a4
; CHECK-O0-NEXT:    sub sp, sp, a4
; CHECK-O0-NEXT:    sd sp, -40(s0)
; CHECK-O0-NEXT:    # kill: def $x14 killed $x13
; CHECK-O0-NEXT:    lui a4, %hi(scratch)
; CHECK-O0-NEXT:    addi a4, a4, %lo(scratch)
; CHECK-O0-NEXT:    ld a5, -40(s0)
; CHECK-O0-NEXT:    vs1r.v v0, (a5)
; CHECK-O0-NEXT:    vsetvli a3, a3, e64,m2
; CHECK-O0-NEXT:    vle.v v2, (a0), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vle.v v4, (a1), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vle.v v6, (a2), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfadd.vv v16, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfsub.vv v18, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfmul.vv v20, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfdiv.vv v22, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vfmacc.vv v2, v4, v6, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v16, (a4), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v18, (a4), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v20, (a4), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v22, (a4), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse.v v2, (a4), v0.t
; CHECK-O0-NEXT:    addi sp, s0, -48
; CHECK-O0-NEXT:    ld s0, 32(sp)
; CHECK-O0-NEXT:    ld ra, 40(sp)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fp_3:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a3, a3, e64,m2
; CHECK-O2-NEXT:    vle.v v2, (a0), v0.t
; CHECK-O2-NEXT:    vle.v v4, (a1), v0.t
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vle.v v6, (a2), v0.t
; CHECK-O2-NEXT:    vfadd.vv v16, v2, v4, v0.t
; CHECK-O2-NEXT:    vfsub.vv v18, v2, v4, v0.t
; CHECK-O2-NEXT:    vfmul.vv v20, v2, v4, v0.t
; CHECK-O2-NEXT:    vfdiv.vv v22, v2, v4, v0.t
; CHECK-O2-NEXT:    vfmacc.vv v2, v4, v6, v0.t
; CHECK-O2-NEXT:    vse.v v16, (a0), v0.t
; CHECK-O2-NEXT:    vse.v v18, (a0), v0.t
; CHECK-O2-NEXT:    vse.v v20, (a0), v0.t
; CHECK-O2-NEXT:    vse.v v22, (a0), v0.t
; CHECK-O2-NEXT:    vse.v v2, (a0), v0.t
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x double>*

  %i0 = call <vscale x 2 x double> @llvm.vp.load.nxv2f64(<vscale x 2 x double>* %a0, i32 8, <vscale x 2 x i1> %m, i32 %n)
  %i1 = call <vscale x 2 x double> @llvm.vp.load.nxv2f64(<vscale x 2 x double>* %a1, i32 8, <vscale x 2 x i1> %m, i32 %n)
  %i2 = call <vscale x 2 x double> @llvm.vp.load.nxv2f64(<vscale x 2 x double>* %a2, i32 8, <vscale x 2 x i1> %m, i32 %n)

  %r0 = call <vscale x 2 x double> @llvm.vp.fadd.nxv2f64(<vscale x 2 x double> %i0, <vscale x 2 x double> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %m, i32 %n)
  %r1 = call <vscale x 2 x double> @llvm.vp.fsub.nxv2f64(<vscale x 2 x double> %i0, <vscale x 2 x double> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %m, i32 %n)
  %r2 = call <vscale x 2 x double> @llvm.vp.fmul.nxv2f64(<vscale x 2 x double> %i0, <vscale x 2 x double> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %m, i32 %n)
  %r3 = call <vscale x 2 x double> @llvm.vp.fdiv.nxv2f64(<vscale x 2 x double> %i0, <vscale x 2 x double> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %m, i32 %n)
  ;%r4 = call <vscale x 2 x double> @llvm.vp.frem.nxv2f64(<vscale x 2 x double> %i0, <vscale x 2 x double> %i1, metadata !"", metadata !"", <vscale x 2 x i1> %m, i32 %n)
  %r5 = call <vscale x 2 x double> @llvm.vp.fma.nxv2f64(<vscale x 2 x double> %i0, <vscale x 2 x double> %i1, <vscale x 2 x double> %i2, metadata !"", metadata !"", <vscale x 2 x i1> %m, i32 %n)
  ;%r6 = call <vscale x 2 x double> @llvm.vp.fneg.nxv2f64(<vscale x 2 x double> %i0, metadata !"", <vscale x 2 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv2f64(<vscale x 2 x double> %r0, <vscale x 2 x double>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2f64(<vscale x 2 x double> %r1, <vscale x 2 x double>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2f64(<vscale x 2 x double> %r2, <vscale x 2 x double>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2f64(<vscale x 2 x double> %r3, <vscale x 2 x double>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  ;call void @llvm.vp.store.nxv2f64(<vscale x 2 x double> %r4, <vscale x 2 x double>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2f64(<vscale x 2 x double> %r5, <vscale x 2 x double>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  ;call void @llvm.vp.store.nxv2f64(<vscale x 2 x double> %r6, <vscale x 2 x double>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)

  ret void
}

; load/store (double, m1)
declare <vscale x 1 x double> @llvm.vp.load.nxv1f64(<vscale x 1 x double>*, i32, <vscale x 1 x i1>, i32)
declare void @llvm.vp.store.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>*, i32, <vscale x 1 x i1>, i32)
; float arith (double, m1)
declare <vscale x 1 x double> @llvm.vp.fadd.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, metadata, metadata, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.vp.fsub.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, metadata, metadata, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.vp.fmul.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, metadata, metadata, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.vp.fdiv.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, metadata, metadata, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.vp.frem.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, metadata, metadata, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.vp.fma.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, metadata, metadata, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.vp.fneg.nxv1f64(<vscale x 1 x double>, metadata, <vscale x 1 x i1>, i32)

; load/store (float, m1)
declare <vscale x 2 x float> @llvm.vp.load.nxv2f32(<vscale x 2 x float>*, i32, <vscale x 2 x i1>, i32)
declare void @llvm.vp.store.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>*, i32, <vscale x 2 x i1>, i32)
; float arith (float, m1)
declare <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x float> @llvm.vp.fsub.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x float> @llvm.vp.fdiv.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x float> @llvm.vp.frem.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x float> @llvm.vp.fma.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x float> @llvm.vp.fneg.nxv2f32(<vscale x 2 x float>, metadata, <vscale x 2 x i1>, i32)

; load/store (double, m2)
declare <vscale x 2 x double> @llvm.vp.load.nxv2f64(<vscale x 2 x double>*, i32, <vscale x 2 x i1>, i32)
declare void @llvm.vp.store.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>*, i32, <vscale x 2 x i1>, i32)
; float arith (double, m2)
declare <vscale x 2 x double> @llvm.vp.fadd.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x double> @llvm.vp.fsub.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x double> @llvm.vp.fmul.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x double> @llvm.vp.fdiv.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x double> @llvm.vp.frem.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x double> @llvm.vp.fma.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, metadata, metadata, <vscale x 2 x i1>, i32)
declare <vscale x 2 x double> @llvm.vp.fneg.nxv2f64(<vscale x 2 x double>, metadata, <vscale x 2 x i1>, i32)
