; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple riscv64 -mattr=+m,+a,+f,+d,+epi | FileCheck %s

define void @store(<vscale x 1 x i64> %v1) nounwind
; CHECK-LABEL: store:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -512
; CHECK-NEXT:    vsetvli a0, zero, e64, m1
; CHECK-NEXT:    lui a0, 0
; CHECK-NEXT:    mv a0, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    vse.v v16, (a0)
; CHECK-NEXT:    addi sp, sp, 512
; CHECK-NEXT:    ret
{
  %buffer = alloca [64 x i64], align 8

  %v4 = bitcast [64 x i64]* %buffer to <vscale x 1 x i64>*
  store volatile <vscale x 1 x i64> %v1, <vscale x 1 x i64>* %v4, align 8

  ret void
}

define void @intrinsic(<vscale x 1 x i64> %v1, i64 %gvl) nounwind
; CHECK-LABEL: intrinsic:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -512
; CHECK-NEXT:    mv a1, sp
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vse.v v16, (a1)
; CHECK-NEXT:    addi sp, sp, 512
; CHECK-NEXT:    ret
{
  %buffer = alloca [64 x i64], align 8

  %v2 = bitcast [64 x i64]* %buffer to <vscale x 1 x i64>*
  call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> %v1, <vscale x 1 x i64>* %v2, i64 %gvl)

  ret void
}

declare void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>*, i64 %gvl)
