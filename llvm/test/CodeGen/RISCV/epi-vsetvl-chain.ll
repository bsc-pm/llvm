; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+epi -verify-machineinstrs -O0 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+epi -verify-machineinstrs -O2 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

declare i64 @llvm.epi.vsetvl(
  i64, i64, i64);

declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
  <vscale x 1 x double>*,
  i64);

declare void @llvm.epi.vstore.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64);

define void @test_vsetvl_chain(<vscale x 1 x double>* %v, i64 %avl)
; CHECK-O0-LABEL: test_vsetvl_chain:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 16
; CHECK-O0-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O0-NEXT:    lui a3, %hi(scratch)
; CHECK-O0-NEXT:    addi a3, a3, %lo(scratch)
; CHECK-O0-NEXT:    vle.v v0, (a0)
; CHECK-O0-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O0-NEXT:    vse.v v0, (a3)
; CHECK-O0-NEXT:    sd a1, 8(sp)
; CHECK-O0-NEXT:    sd a0, 0(sp)
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vle.v v0, (a0)
; CHECK-O2-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O2-NEXT:    vse.v v0, (a2)
; CHECK-O2-NEXT:    ret
{
  %gvl1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %gvl2 = call i64 @llvm.epi.vsetvl(i64 %gvl1, i64 3, i64 0)
  %gvl3 = call i64 @llvm.epi.vsetvl(i64 %gvl2, i64 3, i64 0)
  %gvl4 = call i64 @llvm.epi.vsetvl(i64 %gvl3, i64 3, i64 0)
  %gvl5 = call i64 @llvm.epi.vsetvl(i64 %gvl4, i64 3, i64 0)
  %gvl6 = call i64 @llvm.epi.vsetvl(i64 %gvl5, i64 3, i64 0)
  %gvl7 = call i64 @llvm.epi.vsetvl(i64 %gvl6, i64 3, i64 0)
  %gvl8 = call i64 @llvm.epi.vsetvl(i64 %gvl7, i64 3, i64 0)
  %gvl9 = call i64 @llvm.epi.vsetvl(i64 %gvl8, i64 3, i64 0)
  %gvl10 = call i64 @llvm.epi.vsetvl(i64 %gvl9, i64 3, i64 0)

  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 %gvl10)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 %gvl10)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 %gvl10)

  ret void
}

define void @test_vsetvl_chain_2(<vscale x 1 x double>* %v, i64 %avl)
; CHECK-O0-LABEL: test_vsetvl_chain_2:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 16
; CHECK-O0-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-O0-NEXT:    lui a3, %hi(scratch)
; CHECK-O0-NEXT:    addi a3, a3, %lo(scratch)
; CHECK-O0-NEXT:    vle.v v0, (a0)
; CHECK-O0-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O0-NEXT:    vse.v v0, (a3)
; CHECK-O0-NEXT:    sd a1, 8(sp)
; CHECK-O0-NEXT:    sd a0, 0(sp)
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_2:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vle.v v0, (a0)
; CHECK-O2-NEXT:    vfadd.vv v0, v0, v0
; CHECK-O2-NEXT:    vse.v v0, (a2)
; CHECK-O2-NEXT:    ret
{
  %gvl1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %gvl2 = call i64 @llvm.epi.vsetvl(i64 %gvl1, i64 3, i64 0)
  %gvl3 = call i64 @llvm.epi.vsetvl(i64 %gvl1, i64 3, i64 0)
  %gvl4 = call i64 @llvm.epi.vsetvl(i64 %gvl2, i64 3, i64 0)
  %gvl5 = call i64 @llvm.epi.vsetvl(i64 %gvl2, i64 3, i64 0)
  %gvl6 = call i64 @llvm.epi.vsetvl(i64 %gvl3, i64 3, i64 0)
  %gvl7 = call i64 @llvm.epi.vsetvl(i64 %gvl3, i64 3, i64 0)
  %gvl8 = call i64 @llvm.epi.vsetvl(i64 %gvl4, i64 3, i64 0)
  %gvl9 = call i64 @llvm.epi.vsetvl(i64 %gvl4, i64 3, i64 0)
  %gvl10 = call i64 @llvm.epi.vsetvl(i64 %gvl5, i64 3, i64 0)

  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 %gvl10)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 %gvl10)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 %gvl10)

  ret void
}
