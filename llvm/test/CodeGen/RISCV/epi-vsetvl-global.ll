; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+epi -verify-machineinstrs \
; RUN: -O2 < %s | FileCheck %s

; The following tests check whether the removal of reduntant VSETVLI
; instructions across basic blocks works as expected.

declare i64 @llvm.epi.vsetvl(i64, i64, i64)

declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)
declare <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>, i64)

declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)

declare <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)

declare <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double, i64)
declare <vscale x 2 x float> @llvm.epi.vbroadcast.nxv2f32.f32(float, i64)

declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64)
declare void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>* nocapture, i64)

declare <vscale x 2 x float> @llvm.epi.vload.nxv2f32(<vscale x 2 x float>* nocapture, i64)

declare float @llvm.epi.vfmv.f.s.f32.nxv2f32(<vscale x 2 x float>, i64)

define <vscale x 1 x double> @test1(i64 %avl, i8 zeroext %cond, <vscale x 1 x double> %a, <vscale x 1 x double> %b) nounwind {
; CHECK-LABEL: test1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    beqz a1, .LBB0_2
; CHECK-NEXT:  # %bb.1: # %if.then
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfadd.vv v16, v16, v17
; CHECK-NEXT:    ret
; CHECK-NEXT:  .LBB0_2: # %if.else
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfsub.vv v16, v16, v17
; CHECK-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %tobool = icmp eq i8 %cond, 0
  br i1 %tobool, label %if.else, label %if.then

if.then:                                          ; preds = %entry
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %if.end

if.else:                                          ; preds = %entry
  %2 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %c.0 = phi <vscale x 1 x double> [ %1, %if.then ], [ %2, %if.else ]
  ret <vscale x 1 x double> %c.0
}

@scratch = global i8 0, align 16

define <vscale x 1 x double> @test2(i64 %avl, i8 zeroext %cond, <vscale x 1 x double> %a, <vscale x 1 x double> %b) nounwind {
; CHECK-LABEL: test2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    beqz a1, .LBB1_2
; CHECK-NEXT:  # %bb.1: # %if.then
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v16, v17
; CHECK-NEXT:    vfmul.vv v16, v0, v16
; CHECK-NEXT:    ret
; CHECK-NEXT:  .LBB1_2: # %if.else
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfsub.vv v0, v16, v17
; CHECK-NEXT:    vfmul.vv v16, v0, v16
; CHECK-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %tobool = icmp eq i8 %cond, 0
  br i1 %tobool, label %if.else, label %if.then

if.then:                                          ; preds = %entry
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %if.end

if.else:                                          ; preds = %entry
  %2 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %c.0 = phi <vscale x 1 x double> [ %1, %if.then ], [ %2, %if.else ]
  %3 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %c.0, <vscale x 1 x double> %a, i64 %0)
  ret <vscale x 1 x double> %3
}

define <vscale x 1 x double> @test3(i64 %avl, i8 zeroext %cond, <vscale x 1 x double> %a, <vscale x 1 x double> %b) nounwind {
; CHECK-LABEL: test3:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    beqz a1, .LBB2_2
; CHECK-NEXT:  # %bb.1: # %if.then
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v16, v17
; CHECK-NEXT:    j .LBB2_3
; CHECK-NEXT:  .LBB2_2: # %if.else
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfsub.vv v0, v16, v17
; CHECK-NEXT:  .LBB2_3: # %if.end
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfmul.vv v16, v0, v16
; CHECK-NEXT:    ret
entry:
  %tobool = icmp eq i8 %cond, 0
  br i1 %tobool, label %if.else, label %if.then

if.then:                                          ; preds = %entry
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %if.end

if.else:                                          ; preds = %entry
  %2 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %3 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %2)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %gvl.0 = phi i64 [ %0, %if.then], [ %2, %if.else ]
  %c.0 = phi <vscale x 1 x double> [ %1, %if.then ], [ %3, %if.else ]
  %4 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %c.0, <vscale x 1 x double> %a, i64 %gvl.0)
  ret <vscale x 1 x double> %4
}

define <vscale x 1 x double> @test4(i64 %avl, i8 zeroext %cond, <vscale x 1 x double> %l, <vscale x 1 x double> %r) nounwind {
; CHECK-LABEL: test4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    beqz a1, .LBB3_2
; CHECK-NEXT:  # %bb.1: # %if.then
; CHECK-NEXT:    vsetvli a1, a0, e64, m1
; CHECK-NEXT:    lui a1, %hi(.LCPI3_0)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI3_0)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    lui a1, %hi(.LCPI3_1)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI3_1)
; CHECK-NEXT:    fld ft1, 0(a1)
; CHECK-NEXT:    j .LBB3_3
; CHECK-NEXT:  .LBB3_2: # %if.else
; CHECK-NEXT:    vsetvli a1, a0, e32, m1
; CHECK-NEXT:    lui a1, %hi(.LCPI3_2)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI3_2)
; CHECK-NEXT:    flw ft0, 0(a1)
; CHECK-NEXT:    lui a1, %hi(.LCPI3_3)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI3_3)
; CHECK-NEXT:    flw ft1, 0(a1)
; CHECK-NEXT:  .LBB3_3: # %if.end
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vfadd.vf v0, v0, ft1
; CHECK-NEXT:    lui a1, %hi(scratch)
; CHECK-NEXT:    addi a1, a1, %lo(scratch)
; CHECK-NEXT:    vse.v v0, (a1)
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfmul.vv v16, v16, v17
; CHECK-NEXT:    ret
entry:
  %tobool = icmp eq i8 %cond, 0
  br i1 %tobool, label %if.else, label %if.then

if.then:                                          ; preds = %entry
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %1 = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 1.000000e+00, i64 %0)
  %2 = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 2.000000e+00, i64 %0)
  %3 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %1, <vscale x 1 x double> %2, i64 %0)
  %4 = bitcast i8* @scratch to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %3, <vscale x 1 x double>* %4, i64 %0)
  br label %if.end

if.else:                                          ; preds = %entry
  %5 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 2, i64 0)
  %6 = tail call <vscale x 2 x float> @llvm.epi.vbroadcast.nxv2f32.f32(float 1.000000e+00, i64 %5)
  %7 = tail call <vscale x 2 x float> @llvm.epi.vbroadcast.nxv2f32.f32(float 2.000000e+00, i64 %5)
  %8 = tail call <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(<vscale x 2 x float> %6, <vscale x 2 x float> %7, i64 %5)
  %9 = bitcast i8* @scratch to <vscale x 2 x float>*
  tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> %8, <vscale x 2 x float>* %9, i64 %5)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %10 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %11 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %l, <vscale x 1 x double> %r, i64 %10)
  ret <vscale x 1 x double> %11
}

define <vscale x 1 x double> @test5(i64 %avl, i8 zeroext %cond, <vscale x 1 x double> %a, <vscale x 1 x double> %b) nounwind {
; CHECK-LABEL: test5:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    andi a2, a1, 1
; CHECK-NEXT:    bnez a2, .LBB4_3
; CHECK-NEXT:  # %bb.1: # %if.else
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfsub.vv v0, v16, v17
; CHECK-NEXT:    andi a0, a1, 2
; CHECK-NEXT:    beqz a0, .LBB4_4
; CHECK-NEXT:  .LBB4_2: # %if.then4
; CHECK-NEXT:    vfmul.vv v16, v0, v16
; CHECK-NEXT:    ret
; CHECK-NEXT:  .LBB4_3: # %if.then
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v16, v17
; CHECK-NEXT:    andi a0, a1, 2
; CHECK-NEXT:    bnez a0, .LBB4_2
; CHECK-NEXT:  .LBB4_4: # %if.else5
; CHECK-NEXT:    vfmul.vv v16, v16, v0
; CHECK-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %conv = zext i8 %cond to i32
  %and = and i32 %conv, 1
  %tobool = icmp eq i32 %and, 0
  br i1 %tobool, label %if.else, label %if.then

if.then:                                          ; preds = %entry
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %if.end

if.else:                                          ; preds = %entry
  %2 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %c.0 = phi <vscale x 1 x double> [ %1, %if.then ], [ %2, %if.else ]
  %and2 = and i32 %conv, 2
  %tobool3 = icmp eq i32 %and2, 0
  br i1 %tobool3, label %if.else5, label %if.then4

if.then4:                                         ; preds = %if.end
  %3 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %c.0, <vscale x 1 x double> %a, i64 %0)
  br label %if.end6

if.else5:                                         ; preds = %if.end
  %4 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %c.0, i64 %0)
  br label %if.end6

if.end6:                                          ; preds = %if.else5, %if.then4
  %c.1 = phi <vscale x 1 x double> [ %3, %if.then4 ], [ %4, %if.else5 ]
  ret <vscale x 1 x double> %c.1
}

define <vscale x 1 x double> @test6(i64 %avl, i8 zeroext %cond, <vscale x 1 x double> %a, <vscale x 1 x double> %b) nounwind {
; CHECK-LABEL: test6:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    andi a2, a1, 1
; CHECK-NEXT:    bnez a2, .LBB5_3
; CHECK-NEXT:  # %bb.1: # %if.else
; CHECK-NEXT:    vsetvli a2, a0, e64, m1
; CHECK-NEXT:    vfsub.vv v0, v16, v17
; CHECK-NEXT:    andi a1, a1, 2
; CHECK-NEXT:    beqz a1, .LBB5_4
; CHECK-NEXT:  .LBB5_2: # %if.then4
; CHECK-NEXT:    lui a1, %hi(.LCPI5_0)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI5_0)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    lui a1, %hi(.LCPI5_1)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI5_1)
; CHECK-NEXT:    fld ft1, 0(a1)
; CHECK-NEXT:    j .LBB5_5
; CHECK-NEXT:  .LBB5_3: # %if.then
; CHECK-NEXT:    vsetvli a2, a0, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v16, v17
; CHECK-NEXT:    andi a1, a1, 2
; CHECK-NEXT:    bnez a1, .LBB5_2
; CHECK-NEXT:  .LBB5_4: # %if.else5
; CHECK-NEXT:    vsetvli a1, a0, e32, m1
; CHECK-NEXT:    lui a1, %hi(.LCPI5_2)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI5_2)
; CHECK-NEXT:    flw ft0, 0(a1)
; CHECK-NEXT:    lui a1, %hi(.LCPI5_3)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI5_3)
; CHECK-NEXT:    flw ft1, 0(a1)
; CHECK-NEXT:  .LBB5_5: # %if.end10
; CHECK-NEXT:    vfmv.v.f v1, ft0
; CHECK-NEXT:    vfadd.vf v1, v1, ft1
; CHECK-NEXT:    lui a1, %hi(scratch)
; CHECK-NEXT:    addi a1, a1, %lo(scratch)
; CHECK-NEXT:    vse.v v1, (a1)
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    vfmul.vv v16, v0, v0
; CHECK-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %conv = zext i8 %cond to i32
  %and = and i32 %conv, 1
  %tobool = icmp eq i32 %and, 0
  br i1 %tobool, label %if.else, label %if.then

if.then:                                          ; preds = %entry
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %if.end

if.else:                                          ; preds = %entry
  %2 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %c.0 = phi <vscale x 1 x double> [ %1, %if.then ], [ %2, %if.else ]
  %and2 = and i32 %conv, 2
  %tobool3 = icmp eq i32 %and2, 0
  br i1 %tobool3, label %if.else5, label %if.then4

if.then4:                                         ; preds = %if.end
  %3 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %4 = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 1.000000e+00, i64 %3)
  %5 = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 2.000000e+00, i64 %3)
  %6 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %4, <vscale x 1 x double> %5, i64 %3)
  %7 = bitcast i8* @scratch to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %6, <vscale x 1 x double>* %7, i64 %3)
  br label %if.end10

if.else5:                                         ; preds = %if.end
  %8 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 2, i64 0)
  %9 = tail call <vscale x 2 x float> @llvm.epi.vbroadcast.nxv2f32.f32(float 1.000000e+00, i64 %8)
  %10 = tail call <vscale x 2 x float> @llvm.epi.vbroadcast.nxv2f32.f32(float 2.000000e+00, i64 %8)
  %11 = tail call <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(<vscale x 2 x float> %9, <vscale x 2 x float> %10, i64 %8)
  %12 = bitcast i8* @scratch to <vscale x 2 x float>*
  tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> %11, <vscale x 2 x float>* %12, i64 %8)
  br label %if.end10

if.end10:                                         ; preds = %if.else5, %if.then4
  %13 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %c.0, <vscale x 1 x double> %c.0, i64 %0)
  ret <vscale x 1 x double> %13
}

define <vscale x 1 x double> @test7(i64 %avl, i8 zeroext %cond, <vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %N) nounwind {
; CHECK-LABEL: test7:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-NEXT:    addi a3, zero, 1
; CHECK-NEXT:    blt a2, a3, .LBB6_5
; CHECK-NEXT:  # %bb.1: # %for.body.lr.ph
; CHECK-NEXT:    mv a3, zero
; CHECK-NEXT:    # implicit-def: $v0
; CHECK-NEXT:    bnez a1, .LBB6_4
; CHECK-NEXT:  .LBB6_2: # %for.inc
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    addw a3, a3, a0
; CHECK-NEXT:    bge a3, a2, .LBB6_6
; CHECK-NEXT:  # %bb.3: # %for.body
; CHECK-NEXT:    # in Loop: Header=BB6_2 Depth=1
; CHECK-NEXT:    beqz a1, .LBB6_2
; CHECK-NEXT:  .LBB6_4: # %if.then
; CHECK-NEXT:    vfadd.vv v0, v16, v17
; CHECK-NEXT:    j .LBB6_2
; CHECK-NEXT:  .LBB6_5:
; CHECK-NEXT:    # implicit-def: $v0
; CHECK-NEXT:  .LBB6_6: # %for.cond.cleanup
; CHECK-NEXT:    rdvtype t0
; CHECK-NEXT:    rdvl t1
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vmv.v.v v16, v0
; CHECK-NEXT:    vsetvl zero, t1, t0
; CHECK-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %cmp8 = icmp sgt i64 %N, 0
  br i1 %cmp8, label %for.body.lr.ph, label %for.cond.cleanup

for.body.lr.ph:                                   ; preds = %entry
  %tobool = icmp eq i8 %cond, 0
  br label %for.body

for.cond.cleanup:                                 ; preds = %for.inc, %entry
  %c.0.lcssa = phi <vscale x 1 x double> [ undef, %entry ], [ %c.1, %for.inc ]
  ret <vscale x 1 x double> %c.0.lcssa

for.body:                                         ; preds = %for.body.lr.ph, %for.inc
  %conv10 = phi i64 [ 0, %for.body.lr.ph ], [ %conv, %for.inc ]
  %c.09 = phi <vscale x 1 x double> [ undef, %for.body.lr.ph ], [ %c.1, %for.inc ]
  br i1 %tobool, label %for.inc, label %if.then

if.then:                                          ; preds = %for.body
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %for.inc

for.inc:                                          ; preds = %for.body, %if.then
  %c.1 = phi <vscale x 1 x double> [ %1, %if.then ], [ %c.09, %for.body ]
  %add = add nsw i64 %conv10, %0
  %sext = shl i64 %add, 32
  %conv = ashr exact i64 %sext, 32
  %cmp = icmp slt i64 %conv, %N
  br i1 %cmp, label %for.body, label %for.cond.cleanup
}

define <vscale x 1 x double> @test8(i64 %avl, float %cond, <vscale x 1 x double> %a, <vscale x 1 x double> %b) nounwind {
; CHECK-LABEL: test8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    fmv.w.x ft1, a1
; CHECK-NEXT:    vsetvli a1, a0, e64, m1
; CHECK-NEXT:    lui a1, %hi(.LCPI7_0)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI7_0)
; CHECK-NEXT:    flw ft0, 0(a1)
; CHECK-NEXT:    lui a1, %hi(.LCPI7_1)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI7_1)
; CHECK-NEXT:    flw ft2, 0(a1)
; CHECK-NEXT:    vfadd.vv v16, v16, v17
; CHECK-NEXT:    lui a1, %hi(scratch)
; CHECK-NEXT:    addi a1, a1, %lo(scratch)
; CHECK-NEXT:    j .LBB7_2
; CHECK-NEXT:  .LBB7_1: # %do.cond
; CHECK-NEXT:    # in Loop: Header=BB7_2 Depth=1
; CHECK-NEXT:    feq.s a2, ft1, ft2
; CHECK-NEXT:    xori a2, a2, 1
; CHECK-NEXT:    beqz a2, .LBB7_4
; CHECK-NEXT:  .LBB7_2: # %do.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    flt.s a2, ft0, ft1
; CHECK-NEXT:    xori a2, a2, 1
; CHECK-NEXT:    bnez a2, .LBB7_1
; CHECK-NEXT:  # %bb.3: # %if.then
; CHECK-NEXT:    # in Loop: Header=BB7_2 Depth=1
; CHECK-NEXT:    vsetvli a2, a0, e64, m1
; CHECK-NEXT:    vfadd.vv v16, v16, v17
; CHECK-NEXT:    vsetvli a2, a0, e32, m1
; CHECK-NEXT:    vle.v v0, (a1)
; CHECK-NEXT:    vfmv.f.s ft1, v0
; CHECK-NEXT:    j .LBB7_1
; CHECK-NEXT:  .LBB7_4: # %do.end
; CHECK-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i64 %0)
  br label %do.body

do.body:                                          ; preds = %do.cond, %entry
  %a.addr.0 = phi <vscale x 1 x double> [ %1, %entry ], [ %a.addr.1, %do.cond ]
  %cond.addr.0 = phi float [ %cond, %entry ], [ %cond.addr.1, %do.cond ]
  %cmp = fcmp ogt float %cond.addr.0, 1.000000e+00
  br i1 %cmp, label %if.then, label %do.cond

if.then:                                          ; preds = %do.body
  %2 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %a.addr.0, <vscale x 1 x double> %b, i64 %0)
  %3 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 2, i64 0)
  %4 = bitcast i8* @scratch to <vscale x 2 x float>*
  %5 = tail call <vscale x 2 x float> @llvm.epi.vload.nxv2f32(<vscale x 2 x float>* %4, i64 %3)
  %6 = tail call float @llvm.epi.vfmv.f.s.f32.nxv2f32(<vscale x 2 x float> %5, i64 %3)
  br label %do.cond

do.cond:                                          ; preds = %do.body, %if.then
  %a.addr.1 = phi <vscale x 1 x double> [ %2, %if.then ], [ %a.addr.0, %do.body ]
  %cond.addr.1 = phi float [ %6, %if.then ], [ %cond.addr.0, %do.body ]
  %tobool = fcmp une float %cond.addr.1, 0.000000e+00
  br i1 %tobool, label %do.body, label %do.end

do.end:                                           ; preds = %do.cond
  ret <vscale x 1 x double> %a.addr.1
}
