; NOTE: Tests autogenerated by utils/EPI/generate-intrinsics-tests.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+epi -verify-machineinstrs < %s \
; RUN:    | FileCheck %s

@scratch = global i8 0, align 16


declare <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vadd_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vadd_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vadd_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vadd_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vadd_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vadd_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vadd_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vadd_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vadd_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vadd_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vadd_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vadd_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vadd_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vadd_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vadd_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vadd_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vadd_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vadd_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vadd_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vadd_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vadd_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vadd_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vadd_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vadd_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vadd_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vadd_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vadd_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vadd_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vadd_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vadd_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsub.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsub_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vsub.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsub.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsub.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsub.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsub_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vsub.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsub.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsub.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsub.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsub_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vsub.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsub.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsub.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsub.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsub_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vsub.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsub.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsub.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsub.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsub_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vsub.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsub.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsub.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsub.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsub_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vsub.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsub.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsub.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsub.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsub_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vsub.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsub.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsub.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsub.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsub_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vsub.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsub.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsub.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsub.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsub_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vsub.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsub.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsub.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsub.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsub_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vsub.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsub.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsub.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsub.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsub_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vsub.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsub.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsub.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsub_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsub.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsub_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vsub.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsub.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsub.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsub.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsub_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vsub.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsub.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsub.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsub.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsub_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vsub.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsub.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsub.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsub.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsub_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsub.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsub.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsub_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsub.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsub.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsub_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsub.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsub.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsub_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsub.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsub.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsub_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsub.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsub.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsub_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsub.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsub.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsub_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsub.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsub.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsub_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsub.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsub.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsub_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsub.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsub.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsub_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsub.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsub.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsub_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsub.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsub_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsub.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsub_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsub.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsub.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsub_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsub.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsub.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsub_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsub.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrsub.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vrsub_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vrsub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrsub.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrsub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrsub.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vrsub_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vrsub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrsub.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrsub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrsub.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vrsub_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vrsub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrsub.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrsub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrsub.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vrsub_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vrsub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrsub.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrsub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrsub.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vrsub_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vrsub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrsub.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrsub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrsub.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vrsub_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vrsub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrsub.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrsub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrsub.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vrsub_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vrsub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrsub.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrsub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrsub.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vrsub_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vrsub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrsub.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrsub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrsub.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vrsub_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vrsub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrsub.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrsub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrsub.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vrsub_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vrsub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrsub.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrsub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrsub.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vrsub_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vrsub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrsub.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrsub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrsub.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vrsub_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vrsub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrsub.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrsub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrsub.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vrsub_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vrsub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrsub.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrsub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrsub.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vrsub_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vrsub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrsub.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrsub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrsub.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vrsub_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vrsub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrsub.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrsub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vrsub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrsub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vrsub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrsub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vrsub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrsub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vrsub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrsub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vrsub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrsub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vrsub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrsub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vrsub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrsub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vrsub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrsub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vrsub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrsub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vrsub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrsub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vrsub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrsub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vrsub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrsub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vrsub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrsub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vrsub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrsub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vrsub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrsub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwaddu.nxv8i16.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwaddu_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwaddu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.nxv8i16.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwaddu.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwaddu.nxv16i16.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwaddu_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwaddu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.nxv16i16.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwaddu.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwaddu.nxv32i16.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwaddu_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwaddu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.nxv32i16.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwaddu.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwaddu.nxv4i32.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwaddu_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwaddu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.nxv4i32.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwaddu.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwaddu.nxv8i32.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwaddu_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwaddu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.nxv8i32.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwaddu.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwaddu.nxv16i32.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwaddu_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwaddu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.nxv16i32.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwaddu.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwaddu.nxv2i64.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwaddu_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwaddu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.nxv2i64.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwaddu.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwaddu.nxv4i64.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwaddu_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwaddu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.nxv4i64.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwaddu.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwaddu.nxv8i64.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwaddu_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwaddu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.nxv8i64.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwaddu.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwaddu.nxv8i16.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwaddu_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwaddu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwaddu.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwaddu.nxv16i16.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwaddu_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwaddu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwaddu.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwaddu.nxv32i16.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwaddu_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwaddu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwaddu.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwaddu.nxv4i32.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwaddu_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwaddu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwaddu.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwaddu.nxv8i32.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwaddu_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwaddu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwaddu.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwaddu.nxv16i32.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwaddu_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwaddu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwaddu.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwaddu.nxv2i64.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwaddu_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwaddu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwaddu.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwaddu.nxv4i64.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwaddu_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwaddu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwaddu.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwaddu.nxv8i64.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwaddu_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwaddu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwaddu.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwadd.nxv8i16.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwadd_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwadd.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.nxv8i16.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwadd.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwadd.nxv16i16.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwadd_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwadd.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.nxv16i16.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwadd.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwadd.nxv32i16.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwadd_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwadd.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.nxv32i16.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwadd.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwadd.nxv4i32.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwadd_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwadd.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.nxv4i32.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwadd.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwadd.nxv8i32.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwadd_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwadd.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.nxv8i32.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwadd.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwadd.nxv16i32.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwadd_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwadd.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.nxv16i32.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwadd.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwadd.nxv2i64.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwadd_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwadd.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.nxv2i64.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwadd.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwadd.nxv4i64.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwadd_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwadd.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.nxv4i64.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwadd.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwadd.nxv8i64.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwadd_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwadd.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.nxv8i64.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwadd.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwadd.nxv8i16.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwadd_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwadd.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwadd.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwadd.nxv16i16.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwadd_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwadd.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwadd.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwadd.nxv32i16.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwadd_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwadd.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwadd.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwadd.nxv4i32.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwadd_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwadd.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwadd.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwadd.nxv8i32.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwadd_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwadd.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwadd.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwadd.nxv16i32.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwadd_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwadd.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwadd.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwadd.nxv2i64.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwadd_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwadd.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwadd.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwadd.nxv4i64.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwadd_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwadd.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwadd.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwadd.nxv8i64.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwadd_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwadd.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwadd.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsubu.nxv8i16.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwsubu_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwsubu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.nxv8i16.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsubu.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwsubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsubu.nxv16i16.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwsubu_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwsubu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.nxv16i16.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsubu.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwsubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsubu.nxv32i16.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwsubu_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwsubu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.nxv32i16.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsubu.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwsubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsubu.nxv4i32.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwsubu_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwsubu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.nxv4i32.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsubu.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwsubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsubu.nxv8i32.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwsubu_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwsubu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.nxv8i32.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsubu.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwsubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsubu.nxv16i32.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwsubu_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwsubu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.nxv16i32.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsubu.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwsubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsubu.nxv2i64.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwsubu_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwsubu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.nxv2i64.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsubu.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwsubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsubu.nxv4i64.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwsubu_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwsubu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.nxv4i64.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsubu.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwsubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsubu.nxv8i64.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwsubu_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwsubu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.nxv8i64.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsubu.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwsubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsubu.nxv8i16.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwsubu_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwsubu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsubu.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwsubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsubu.nxv16i16.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwsubu_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwsubu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsubu.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwsubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsubu.nxv32i16.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwsubu_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwsubu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsubu.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwsubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsubu.nxv4i32.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwsubu_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwsubu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsubu.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwsubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsubu.nxv8i32.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwsubu_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwsubu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsubu.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwsubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsubu.nxv16i32.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwsubu_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwsubu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsubu.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwsubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsubu.nxv2i64.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwsubu_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwsubu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsubu.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwsubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsubu.nxv4i64.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwsubu_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwsubu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsubu.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwsubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsubu.nxv8i64.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwsubu_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwsubu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsubu.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwsubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsub.nxv8i16.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwsub_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwsub.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.nxv8i16.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsub.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsub.nxv16i16.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwsub_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwsub.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.nxv16i16.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsub.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsub.nxv32i16.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwsub_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwsub.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.nxv32i16.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsub.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsub.nxv4i32.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwsub_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwsub.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.nxv4i32.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsub.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsub.nxv8i32.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwsub_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwsub.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.nxv8i32.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsub.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsub.nxv16i32.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwsub_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwsub.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.nxv16i32.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsub.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsub.nxv2i64.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwsub_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwsub.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.nxv2i64.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsub.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsub.nxv4i64.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwsub_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwsub.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.nxv4i64.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsub.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsub.nxv8i64.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwsub_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwsub.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.nxv8i64.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsub.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsub.nxv8i16.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwsub_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwsub.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsub.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsub.nxv16i16.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwsub_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwsub.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsub.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsub.nxv32i16.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwsub_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwsub.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsub.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsub.nxv4i32.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwsub_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwsub.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsub.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsub.nxv8i32.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwsub_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwsub.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsub.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsub.nxv16i32.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwsub_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwsub.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsub.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsub.nxv2i64.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwsub_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwsub.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsub.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsub.nxv4i64.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwsub_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwsub.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsub.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsub.nxv8i64.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwsub_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwsub.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsub.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vand_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vand_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vand_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vand_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vand_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vand_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vand_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vand_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vand_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vand_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vand_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vand_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vand_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vand_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vand_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vand_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vand_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vand_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vand_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vand_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vand_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vand_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vand_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vand_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vand_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vand_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vand_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vand_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vand_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vand_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vor_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vor_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vor_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vor_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vor_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vor_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vor_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vor_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vor_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vor_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vor_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vor_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vor_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vor_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vor_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vor_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vor_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vor_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vor_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vor_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vor_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vor_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vor_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vor_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vor_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vor_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vor_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vor_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vor_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vor_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vxor_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vxor_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vxor_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vxor_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vxor_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vxor_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vxor_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vxor_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vxor_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vxor_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vxor_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vxor_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vxor_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vxor_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vxor_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vxor_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vxor_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vxor_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vxor_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vxor_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vxor_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vxor_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vxor_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vxor_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vxor_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vxor_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vxor_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vxor_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vxor_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vxor_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsll_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsll_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsll_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsll_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsll_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsll_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsll_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsll_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsll_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsll_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsll_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsll_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsll_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsll_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsll_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsll_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsll_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsll_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsll_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsll_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsll_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsll_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsll_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsll_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsll_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsll_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsll_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsll_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsll_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsll_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsrl_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsrl_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsrl_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsrl_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsrl_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsrl_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsrl_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsrl_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsrl_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsrl_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsrl_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsrl_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsrl_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsrl_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsrl_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsrl_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsrl_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsrl_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsrl_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsrl_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsrl_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsrl_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsrl_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsrl_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsrl_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsrl_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsrl_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsrl_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsrl_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsrl_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsra_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsra_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsra_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsra_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsra_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsra_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsra_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsra_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsra_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsra_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsra_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsra_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsra_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsra_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsra_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsra_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsra_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsra_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsra_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsra_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsra_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsra_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsra_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsra_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsra_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsra_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsra_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsra_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsra_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsra_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vseq_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vseq_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vseq_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vseq_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vseq_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vseq_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vseq_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vseq_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vseq_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vseq_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vseq_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vseq_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vseq_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vseq_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vseq_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vseq_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vseq_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vseq_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vseq_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vseq_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vseq_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vseq_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vseq_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vseq_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vseq_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vseq_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vseq_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vseq_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vseq_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vseq_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vseq_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vseq_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vseq.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vseq.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vseq.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vseq.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vseq_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vseq_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vseq.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsne_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsne_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsne_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsne_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsne_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsne_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsne_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsne_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsne_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsne_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsne_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsne_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsne_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsne_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsne_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsne_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsne_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsne_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsne_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsne_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsne_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsne_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsne_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsne_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsne_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsne_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsne_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsne_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsne_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsne_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsne_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsne_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsne.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsne.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsne.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsne.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsne_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsne_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsne.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsltu_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsltu_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsltu.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsltu_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vsltu.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsltu.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsltu.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsltu_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsltu_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsltu_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsltu.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsltu_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vsltu.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsltu.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsltu.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsltu.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsltu_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vsltu.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsltu.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsltu.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsltu_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsltu_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsltu_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsltu_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsltu.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsltu_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vsltu.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsltu.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsltu.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsltu_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsltu_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsltu_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsltu_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsltu.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsltu_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsltu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsltu.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsltu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsltu_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsltu_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsltu_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsltu.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsltu_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsltu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsltu.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsltu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsltu.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsltu_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsltu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsltu.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsltu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsltu_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsltu_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsltu_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsltu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsltu_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsltu.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsltu_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsltu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsltu.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsltu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsltu_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsltu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsltu_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsltu_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsltu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vslt_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vslt_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vslt.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vslt_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vslt.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vslt.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vslt.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vslt_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vslt_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vslt_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vslt.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vslt_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vslt.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vslt.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vslt.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vslt.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vslt_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vslt.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vslt.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vslt.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vslt_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vslt_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vslt_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vslt_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vslt.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vslt_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vslt.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vslt.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vslt.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vslt_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vslt_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslt_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vslt_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vslt_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vslt.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vslt_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vslt.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vslt.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vslt.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vslt_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vslt_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vslt_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vslt.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vslt_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vslt.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vslt.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vslt.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vslt.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vslt_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vslt.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vslt.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vslt.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vslt_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vslt_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vslt_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vslt.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vslt_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vslt.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vslt_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vslt.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vslt.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vslt.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vslt_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vslt.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vslt_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslt_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vslt.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsleu_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsleu_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsleu_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsleu_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsleu_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsleu_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsleu_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsleu_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsleu_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsleu_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsleu_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsleu_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsleu_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsleu_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsleu_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsleu_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsleu_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsleu_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsleu_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsleu_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsleu_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsleu_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsleu_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsleu_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsleu_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsleu_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsleu_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsleu_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsleu_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsleu_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsleu_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsleu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsleu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsleu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsleu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsleu_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsleu_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsleu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsle_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsle_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsle_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsle_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsle_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsle_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsle_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsle_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsle_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsle_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsle_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsle_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsle_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsle_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsle_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsle_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsle_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsle_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsle_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsle_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsle_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsle_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsle_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsle_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsle_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsle_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsle_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsle_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsle_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsle_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsle_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsle_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsle.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsle.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsle.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsle.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsle_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsle_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsle.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsgtu_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsgtu.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsgtu_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsgtu.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsgtu.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsgtu_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsgtu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsgtu.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsgtu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsgtu.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsgtu_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsgtu.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsgtu_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsgtu.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsgtu_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsgtu.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsgtu.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsgtu_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsgtu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsgtu.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsgtu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsgtu.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsgtu_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsgtu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsgtu.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsgtu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsgtu.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsgtu_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsgtu.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsgtu_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsgtu.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsgtu_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsgtu.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgtu.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsgtu_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vsgtu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgtu.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsgtu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsgtu.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsgtu_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsgtu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsgtu.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsgtu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsgtu.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsgtu_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsgtu.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsgtu_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsgtu_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsgtu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsgtu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsgtu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsgtu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsgtu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsgtu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgtu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vsgtu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsgtu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsgtu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsgtu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgtu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsgtu_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsgtu_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgtu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsgt_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsgt.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsgt_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsgt.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsgt.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsgt_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsgt.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsgt.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsgt.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsgt.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsgt_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsgt.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsgt_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsgt.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsgt_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsgt.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vsgt.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsgt_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vsgt.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vsgt.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsgt.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsgt.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsgt_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsgt.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsgt.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsgt.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsgt.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsgt_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsgt.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsgt_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vsgt.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsgt_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vsgt.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgt.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsgt_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vsgt.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgt.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsgt.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vsgt.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsgt_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vsgt.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vsgt.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsgt.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vsgt.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsgt_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vsgt.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsgt_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsgt_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsgt.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsgt.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vsgt.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vsgt.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsgt.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsgt.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vsgt.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vsgt.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vsgt.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vsgt.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vsgt.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vsgt.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vsgt_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vsgt_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vsgt.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vminu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vminu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vminu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vminu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vminu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vminu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vminu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vminu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vminu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vminu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vminu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vminu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vminu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vminu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vminu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vminu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vminu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vminu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vminu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vminu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vminu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vminu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vminu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vminu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vminu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vminu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vminu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vminu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vminu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vminu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vminu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vminu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vminu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vminu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vminu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vminu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vminu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vminu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vminu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vminu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vminu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vminu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vminu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vminu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vminu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vminu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vminu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vminu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vminu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vminu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vminu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vminu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vminu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vminu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vminu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vminu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vminu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vminu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vminu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vminu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vminu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vminu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vminu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vminu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vminu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vminu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vminu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vminu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vminu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vminu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vminu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vminu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vminu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vminu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vminu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vminu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vminu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vminu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vminu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vminu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vminu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vminu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vminu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vminu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vminu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vminu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vminu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vminu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vminu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vminu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vminu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vminu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vminu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vminu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vminu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vminu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vminu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vminu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vminu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vminu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vminu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vminu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vminu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vminu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vminu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vminu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vminu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vminu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vminu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vminu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vminu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vminu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vminu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vminu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vminu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vminu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vminu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vminu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vminu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vminu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vminu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vminu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vminu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vminu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vminu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vminu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vminu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vminu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vminu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vminu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vminu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vminu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vminu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vminu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vminu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vminu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vminu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vminu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vminu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vminu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vminu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vminu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmin.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmin_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmin.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmin.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmin.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmin.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmin_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmin.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmin.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmin.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmin.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmin_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmin.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmin.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmin.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmin.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmin_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmin.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmin.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmin.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmin.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmin_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmin.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmin.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmin.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmin.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmin_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmin.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmin.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmin.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmin.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmin_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmin.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmin.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmin.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmin.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmin_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmin.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmin.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmin.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmin.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmin_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmin.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmin.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmin.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmin.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmin_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmin.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmin.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmin.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmin.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmin_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmin.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmin.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmin.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmin_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmin.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmin_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmin.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmin.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmin.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmin.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmin_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmin.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmin.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmin.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmin.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmin_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmin.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmin.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmin.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmin.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmin_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmin.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmin.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmin.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmin.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmin_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmin.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmin.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmin.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmin.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmin_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmin.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmin.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmin.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmin.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmin_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmin.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmin.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmin.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmin.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmin_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmin.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmin.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmin.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmin.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmin_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmin.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmin.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmin.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmin.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmin_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmin.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmin.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmin.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmin.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmin_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmin.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmin.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmin.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmin.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmin_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmin.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmin.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmin.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmin.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmin_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmin.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmin.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmin.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmin.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmin_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmin.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmin.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmin.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmin_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmin.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmin_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmin.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmin.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmin.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmin.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmin_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmin.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmin.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmin.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmin.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmin_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmin.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmin.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmin.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmaxu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmaxu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmaxu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmaxu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmaxu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmaxu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmaxu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmaxu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmaxu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmaxu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmaxu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmaxu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmaxu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmaxu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmaxu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmaxu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmaxu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmaxu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmaxu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmaxu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmaxu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmaxu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmaxu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmaxu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmaxu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmaxu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmaxu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmaxu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmaxu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmaxu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmaxu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmaxu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmaxu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmaxu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmaxu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmaxu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmaxu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmaxu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmaxu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmaxu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmaxu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmaxu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmaxu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmaxu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmaxu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmaxu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmaxu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmaxu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmaxu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmaxu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmaxu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmaxu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmaxu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmaxu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmaxu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmaxu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmaxu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmaxu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmaxu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmaxu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmaxu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmaxu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmaxu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmaxu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmaxu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmaxu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmaxu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmaxu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmaxu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmaxu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmaxu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmaxu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmaxu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmaxu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmaxu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmaxu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmaxu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmaxu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmaxu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmaxu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmaxu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmaxu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmaxu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmaxu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmaxu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmaxu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmaxu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmaxu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmaxu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmaxu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmaxu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmaxu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmaxu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmaxu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmaxu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmaxu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmaxu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmaxu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmaxu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmaxu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmaxu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmaxu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmaxu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmaxu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmaxu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmaxu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmaxu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmaxu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmaxu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmaxu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmaxu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmaxu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmaxu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmaxu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmaxu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmaxu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmaxu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmaxu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmaxu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmaxu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmaxu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmaxu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmaxu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmaxu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmaxu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmaxu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmaxu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmaxu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmaxu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmaxu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmaxu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmaxu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmaxu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmaxu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmaxu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmaxu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmaxu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmaxu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmaxu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmaxu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmaxu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmaxu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmax.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmax_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmax.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmax.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmax.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmax.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmax_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmax.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmax.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmax.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmax.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmax_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmax.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmax.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmax.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmax.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmax_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmax.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmax.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmax.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmax.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmax_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmax.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmax.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmax.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmax.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmax_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmax.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmax.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmax.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmax.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmax_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmax.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmax.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmax.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmax.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmax_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmax.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmax.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmax.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmax.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmax_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmax.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmax.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmax.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmax.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmax_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmax.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmax.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmax.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmax.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmax_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmax.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmax.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmax.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmax_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmax.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmax_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmax.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmax.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmax.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmax.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmax_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmax.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmax.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmax.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmax.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmax_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmax.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmax.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmax.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmax.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmax_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmax.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmax.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmax.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmax.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmax_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmax.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmax.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmax.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmax.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmax_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmax.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmax.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmax.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmax.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmax_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmax.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmax.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmax.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmax.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmax_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmax.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmax.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmax.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmax.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmax_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmax.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmax.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmax.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmax.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmax_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmax.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmax.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmax.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmax.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmax_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmax.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmax.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmax.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmax.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmax_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmax.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmax.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmax.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmax.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmax_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmax.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmax.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmax.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmax.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmax_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmax.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmax.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmax.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmax_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmax.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmax_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmax.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmax.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmax.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmax.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmax_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmax.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmax.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmax.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmax.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmax_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmax.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmax.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmax.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmul.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmul_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmul.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmul.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmul.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmul.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmul_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmul.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmul.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmul.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmul.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmul_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmul.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmul.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmul.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmul.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmul_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmul.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmul.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmul.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmul.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmul_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmul.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmul.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmul.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmul.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmul_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmul.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmul.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmul.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmul.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmul_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmul.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmul.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmul.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmul.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmul_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmul.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmul.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmul.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmul.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmul_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmul.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmul.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmul.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmul.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmul_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmul.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmul.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmul.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmul.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmul_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmul.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmul.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmul.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmul_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmul.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmul_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmul.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmul.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmul.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmul.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmul_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmul.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmul.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmul.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmul.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmul_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmul.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmul.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmul.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmul.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmul_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmul.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmul.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmul.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmul.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmul_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmul.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmul.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmul.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmul.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmul_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmul.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmul.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmul.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmul.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmul_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmul.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmul.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmul.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmul.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmul_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmul.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmul.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmul.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmul.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmul_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmul.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmul.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmul.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmul.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmul_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmul.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmul.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmul.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmul.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmul_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmul.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmul.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmul.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmul.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmul_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmul.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmul.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmul.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmul.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmul_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmul.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmul.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmul.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmul.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmul_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmul.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmul.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmul.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmul_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmul.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmul_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmul.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmul.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmul.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmul.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmul_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmul.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmul.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmul.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmul.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmul_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmul.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmul.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmul.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulh.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmulh_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmulh.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulh.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulh.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulh.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmulh_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmulh.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulh.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulh.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulh.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmulh_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmulh.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulh.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulh.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulh.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmulh_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmulh.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulh.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulh.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulh.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmulh_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmulh.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulh.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulh.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulh.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmulh_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmulh.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulh.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulh.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulh.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmulh_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmulh.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulh.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulh.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulh.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmulh_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmulh.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulh.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulh.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulh.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmulh_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmulh.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulh.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulh.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulh.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmulh_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmulh.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulh.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulh.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulh.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmulh_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmulh.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulh.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulh.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmulh_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulh.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmulh_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmulh.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulh.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulh.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulh.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmulh_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmulh.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulh.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulh.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulh.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmulh_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmulh.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulh.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulh.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulh.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmulh_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmulh.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulh.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulh.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulh.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmulh_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmulh.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulh.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulh.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulh.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmulh_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmulh.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulh.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulh.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulh.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmulh_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmulh.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulh.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulh.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulh.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmulh_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmulh.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulh.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulh.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulh.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmulh_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmulh.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulh.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulh.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulh.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmulh_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmulh.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulh.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulh.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulh.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmulh_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmulh.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulh.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulh.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulh.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmulh_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmulh.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulh.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulh.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulh.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmulh_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmulh.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulh.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulh.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulh.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmulh_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmulh.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulh.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulh.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmulh_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulh.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmulh_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmulh.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulh.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulh.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulh.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmulh_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmulh.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulh.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulh.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulh.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmulh_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmulh.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulh.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulh.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulhu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmulhu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulhu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulhu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmulhu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulhu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulhu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmulhu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulhu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulhu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmulhu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulhu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulhu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmulhu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulhu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulhu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmulhu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulhu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulhu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmulhu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulhu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulhu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmulhu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulhu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulhu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmulhu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulhu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulhu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmulhu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulhu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulhu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmulhu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulhu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmulhu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulhu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmulhu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulhu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulhu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmulhu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulhu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulhu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmulhu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulhu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulhu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulhu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulhu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulhu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulhu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulhu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulhu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulhu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulhu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulhu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulhu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulhu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulhu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulhu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulhu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulhu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulhu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulhu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulhu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulhu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulhu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulhu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulhu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulhu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulhu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulhu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulhu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulhu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulhsu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhsu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulhsu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhsu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulhsu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhsu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulhsu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhsu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulhsu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhsu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulhsu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhsu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulhsu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhsu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulhsu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhsu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulhsu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhsu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulhsu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhsu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulhsu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhsu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulhsu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhsu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulhsu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhsu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulhsu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhsu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulhsu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhsu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulhsu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhsu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulhsu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhsu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulhsu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhsu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulhsu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhsu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulhsu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhsu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulhsu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhsu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulhsu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhsu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulhsu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhsu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulhsu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhsu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulhsu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhsu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulhsu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhsu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhsu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulhsu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhsu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhsu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulhsu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhsu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulhsu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhsu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhsu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulhsu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhsu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulhsu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhsu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhsu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulhsu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhsu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhsu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulhsu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhsu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulhsu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhsu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhsu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulhsu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhsu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulhsu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhsu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhsu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulhsu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhsu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulhsu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhsu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhsu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulhsu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhsu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhsu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulhsu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhsu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulhsu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhsu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhsu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulhsu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhsu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulhsu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhsu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhsu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulhsu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhsu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhsu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulhsu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhsu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhsu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulhsu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhsu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulhsu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhsu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhsu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulhsu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhsu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulhsu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhsu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhsu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulhsu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhsu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmul.nxv8i16.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwmul_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmul.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vwmul.nxv8i16.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmul.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmul.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmul.nxv16i16.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwmul_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmul.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vwmul.nxv16i16.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmul.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmul.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmul.nxv32i16.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwmul_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmul.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vwmul.nxv32i16.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmul.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmul.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmul.nxv4i32.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwmul_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmul.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vwmul.nxv4i32.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmul.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmul.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmul.nxv8i32.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwmul_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmul.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vwmul.nxv8i32.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmul.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmul.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmul.nxv16i32.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwmul_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmul.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vwmul.nxv16i32.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmul.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmul.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmul.nxv2i64.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwmul_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmul.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vwmul.nxv2i64.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmul.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmul.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmul.nxv4i64.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwmul_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmul.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vwmul.nxv4i64.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmul.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmul.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmul.nxv8i64.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwmul_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmul.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vwmul.nxv8i64.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmul.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmul.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmul.nxv8i16.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwmul_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmul.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwmul.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmul.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmul.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmul.nxv16i16.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwmul_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmul.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwmul.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmul.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmul.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmul.nxv32i16.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwmul_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmul.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwmul.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmul.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmul.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmul.nxv4i32.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwmul_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmul.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwmul.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmul.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmul.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmul.nxv8i32.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwmul_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmul.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwmul.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmul.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmul.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmul.nxv16i32.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwmul_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmul.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwmul.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmul.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmul.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmul.nxv2i64.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwmul_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmul.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwmul.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmul.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmul.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmul.nxv4i64.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwmul_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmul.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwmul.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmul.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmul.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmul.nxv8i64.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwmul_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmul.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwmul.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmul.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmul.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmulu.nxv8i16.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwmulu_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmulu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulu.nxv8i16.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmulu.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmulu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulu.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmulu.nxv16i16.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwmulu_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmulu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulu.nxv16i16.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmulu.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmulu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulu.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmulu.nxv32i16.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwmulu_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmulu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulu.nxv32i16.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmulu.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmulu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulu.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmulu.nxv4i32.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwmulu_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmulu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulu.nxv4i32.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmulu.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmulu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulu.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmulu.nxv8i32.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwmulu_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmulu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulu.nxv8i32.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmulu.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmulu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulu.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmulu.nxv16i32.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwmulu_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmulu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulu.nxv16i32.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmulu.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmulu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulu.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmulu.nxv2i64.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwmulu_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmulu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulu.nxv2i64.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmulu.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmulu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulu.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmulu.nxv4i64.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwmulu_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmulu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulu.nxv4i64.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmulu.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmulu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulu.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmulu.nxv8i64.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwmulu_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmulu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulu.nxv8i64.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmulu.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmulu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulu.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmulu.nxv8i16.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulu_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmulu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulu.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmulu.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmulu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulu.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmulu.nxv16i16.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulu_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmulu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulu.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmulu.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmulu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulu.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmulu.nxv32i16.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulu_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmulu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulu.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmulu.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmulu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulu.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmulu.nxv4i32.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulu_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmulu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulu.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmulu.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmulu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulu.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmulu.nxv8i32.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulu_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmulu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulu.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmulu.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmulu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulu.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmulu.nxv16i32.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulu_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmulu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulu.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmulu.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmulu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulu.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmulu.nxv2i64.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulu_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmulu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulu.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmulu.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmulu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulu.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmulu.nxv4i64.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulu_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmulu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulu.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmulu.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmulu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulu.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmulu.nxv8i64.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulu_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmulu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulu.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmulu.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmulu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulu.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmulsu.nxv8i16.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmulsu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulsu.nxv8i16.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmulsu.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmulsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulsu.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmulsu.nxv16i16.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmulsu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulsu.nxv16i16.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmulsu.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmulsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulsu.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmulsu.nxv32i16.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmulsu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulsu.nxv32i16.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmulsu.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmulsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulsu.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmulsu.nxv4i32.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmulsu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulsu.nxv4i32.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmulsu.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmulsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulsu.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmulsu.nxv8i32.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmulsu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulsu.nxv8i32.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmulsu.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmulsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulsu.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmulsu.nxv16i32.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmulsu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulsu.nxv16i32.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmulsu.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmulsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulsu.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmulsu.nxv2i64.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmulsu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulsu.nxv2i64.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmulsu.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmulsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulsu.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmulsu.nxv4i64.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmulsu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulsu.nxv4i64.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmulsu.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmulsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulsu.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmulsu.nxv8i64.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmulsu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulsu.nxv8i64.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmulsu.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmulsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulsu.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmulsu.nxv8i16.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulsu_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmulsu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulsu.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmulsu.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vwmulsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulsu.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmulsu.nxv16i16.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulsu_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmulsu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulsu.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmulsu.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vwmulsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulsu.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmulsu.nxv32i16.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulsu_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmulsu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulsu.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmulsu.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vwmulsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulsu.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmulsu.nxv4i32.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulsu_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmulsu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulsu.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmulsu.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vwmulsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulsu.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmulsu.nxv8i32.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulsu_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmulsu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulsu.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmulsu.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vwmulsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulsu.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmulsu.nxv16i32.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulsu_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmulsu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulsu.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmulsu.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vwmulsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulsu.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmulsu.nxv2i64.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulsu_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmulsu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulsu.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmulsu.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vwmulsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulsu.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmulsu.nxv4i64.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulsu_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmulsu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulsu.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmulsu.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vwmulsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulsu.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmulsu.nxv8i64.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulsu_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmulsu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulsu.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmulsu.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vwmulsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulsu.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdivu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vdivu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vdivu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdivu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdivu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdivu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vdivu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vdivu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdivu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdivu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdivu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vdivu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vdivu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdivu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdivu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdivu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vdivu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vdivu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdivu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdivu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdivu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vdivu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vdivu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdivu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdivu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdivu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vdivu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vdivu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdivu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdivu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdivu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vdivu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vdivu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdivu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdivu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdivu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vdivu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vdivu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdivu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdivu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdivu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vdivu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vdivu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdivu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdivu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdivu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vdivu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vdivu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdivu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdivu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdivu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vdivu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vdivu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdivu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdivu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vdivu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdivu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vdivu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vdivu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdivu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdivu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdivu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vdivu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vdivu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdivu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdivu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdivu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vdivu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vdivu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdivu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdivu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdivu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vdivu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vdivu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdivu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdivu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdivu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vdivu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vdivu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdivu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdivu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdivu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vdivu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vdivu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdivu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdivu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdivu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vdivu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vdivu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdivu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdivu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdivu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vdivu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vdivu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdivu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdivu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdivu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vdivu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vdivu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdivu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdivu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdivu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vdivu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vdivu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdivu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdivu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdivu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vdivu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vdivu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdivu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdivu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdivu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vdivu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vdivu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdivu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdivu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdivu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vdivu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vdivu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdivu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdivu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdivu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vdivu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vdivu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdivu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdivu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vdivu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdivu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vdivu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vdivu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdivu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdivu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdivu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vdivu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vdivu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdivu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdivu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdivu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vdivu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vdivu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdivu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdivu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdiv.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vdiv_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vdiv.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdiv.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdiv.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdiv.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vdiv_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vdiv.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdiv.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdiv.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdiv.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vdiv_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vdiv.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdiv.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdiv.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdiv.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vdiv_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vdiv.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdiv.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdiv.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdiv.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vdiv_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vdiv.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdiv.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdiv.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdiv.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vdiv_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vdiv.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdiv.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdiv.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdiv.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vdiv_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vdiv.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdiv.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdiv.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdiv.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vdiv_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vdiv.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdiv.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdiv.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdiv.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vdiv_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vdiv.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdiv.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdiv.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdiv.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vdiv_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vdiv.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdiv.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdiv.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdiv.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vdiv_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vdiv.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdiv.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdiv.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vdiv_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdiv.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vdiv_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vdiv.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdiv.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdiv.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdiv.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vdiv_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vdiv.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdiv.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdiv.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdiv.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vdiv_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vdiv.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdiv.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdiv.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdiv.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vdiv_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vdiv.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdiv.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdiv.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdiv.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vdiv_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vdiv.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdiv.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdiv.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdiv.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vdiv_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vdiv.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdiv.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdiv.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdiv.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vdiv_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vdiv.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdiv.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdiv.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdiv.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vdiv_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vdiv.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdiv.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdiv.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdiv.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vdiv_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vdiv.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdiv.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdiv.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdiv.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vdiv_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vdiv.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdiv.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdiv.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdiv.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vdiv_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vdiv.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdiv.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdiv.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdiv.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vdiv_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vdiv.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdiv.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdiv.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdiv.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vdiv_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vdiv.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdiv.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdiv.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdiv.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vdiv_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vdiv.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdiv.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdiv.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vdiv_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdiv.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vdiv_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vdiv.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdiv.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdiv.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdiv.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vdiv_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vdiv.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdiv.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdiv.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdiv.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vdiv_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vdiv.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdiv.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdiv.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vremu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vremu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vremu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vremu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vremu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vremu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vremu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vremu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vremu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vremu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vremu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vremu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vremu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vremu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vremu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vremu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vremu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vremu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vremu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vremu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vremu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vremu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vremu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vremu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vremu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vremu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vremu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vremu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vremu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vremu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vremu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vremu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vremu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vremu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vremu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vremu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vremu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vremu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vremu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vremu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vremu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vremu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vremu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vremu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vremu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vremu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vremu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vremu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vremu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vremu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vremu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vremu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vremu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vremu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vremu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vremu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vremu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vremu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vremu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vremu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vremu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vremu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vremu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vremu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vremu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vremu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vremu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vremu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vremu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vremu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vremu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vremu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vremu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vremu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vremu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vremu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vremu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vremu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vremu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vremu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vremu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vremu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vremu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vremu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vremu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vremu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vremu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vremu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vremu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vremu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vremu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vremu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vremu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vremu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vremu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vremu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vremu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vremu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vremu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vremu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vremu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vremu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vremu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vremu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vremu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vremu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vremu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vremu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vremu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vremu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vremu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vremu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vremu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vremu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vremu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vremu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vremu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vremu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vremu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vremu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vremu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vremu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vremu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vremu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vremu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vremu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vremu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vremu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vremu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vremu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vremu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vremu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vremu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vremu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vremu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vremu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vremu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vremu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vremu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vremu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vremu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vremu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrem.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vrem_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vrem.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrem.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrem.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrem.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vrem_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vrem.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrem.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrem.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrem.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vrem_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vrem.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrem.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrem.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrem.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vrem_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vrem.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrem.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrem.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrem.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vrem_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vrem.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrem.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrem.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrem.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vrem_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vrem.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrem.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrem.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrem.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vrem_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vrem.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrem.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrem.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrem.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vrem_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vrem.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrem.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrem.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrem.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vrem_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vrem.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrem.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrem.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrem.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vrem_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vrem.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrem.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrem.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrem.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vrem_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vrem.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrem.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrem.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vrem_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrem.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vrem_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vrem.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrem.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrem.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrem.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vrem_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vrem.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrem.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrem.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrem.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vrem_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vrem.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrem.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrem.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrem.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vrem_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vrem.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrem.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrem.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrem.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vrem_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vrem.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrem.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrem.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrem.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vrem_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vrem.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrem.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrem.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrem.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vrem_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vrem.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrem.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrem.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrem.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vrem_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vrem.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrem.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrem.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrem.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vrem_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vrem.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrem.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrem.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrem.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vrem_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vrem.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrem.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrem.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrem.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vrem_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vrem.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrem.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrem.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrem.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vrem_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vrem.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrem.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrem.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrem.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vrem_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vrem.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrem.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrem.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrem.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vrem_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vrem.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrem.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrem.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vrem_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrem.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vrem_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vrem.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrem.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrem.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrem.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vrem_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vrem.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrem.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrem.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrem.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vrem_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vrem.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrem.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrem.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmerge_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmerge.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmerge.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmerge_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmerge.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmerge.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmerge_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmerge.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmerge.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmerge_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmerge.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmerge.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmerge_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmerge.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmerge.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmerge_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmerge.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmerge.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmerge_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmerge.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmerge.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmerge_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmerge.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmerge.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmerge_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmerge.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmerge.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmerge_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmerge.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmerge.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmerge_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmerge.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmerge.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmerge_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmerge.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmerge.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmerge_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmerge.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmerge.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmerge_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmerge.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmerge.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmerge_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmerge.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmerge.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmerge_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmerge.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmerge.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmerge_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmerge.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmerge.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmerge_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmerge.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmerge.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmerge_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmerge.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmerge.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmerge_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmerge.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmerge.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmerge_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmerge.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmerge.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmerge_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmerge.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmerge.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmerge_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmerge.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmerge.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmerge_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmerge.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmerge.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmerge_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmerge.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmerge.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmerge_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmerge.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmerge.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmerge_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmerge.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmerge.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmerge_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmerge.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmerge.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmerge_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmerge.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmerge.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmerge_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmerge.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmerge.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmerge.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmerge.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmerge.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmerge.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmerge.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmerge.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmerge.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmerge.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmerge.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmerge.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmerge.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmerge.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmerge.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmerge.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vmerge_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vmerge_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmerge.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsaddu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsaddu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsaddu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsaddu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsaddu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsaddu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsaddu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsaddu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsaddu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsaddu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsaddu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsaddu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsaddu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsaddu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsaddu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsaddu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsaddu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsaddu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsaddu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsaddu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsaddu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsaddu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsaddu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsaddu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsaddu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsaddu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsaddu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsaddu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsaddu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsaddu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsadd_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsadd_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsadd_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsadd_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsadd_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsadd_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsadd_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsadd_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsadd_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsadd_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsadd_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsadd_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsadd_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsadd_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsadd_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsadd_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsadd_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsadd_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsadd_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsadd_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsadd_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsadd_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsadd_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsadd_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsadd_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsadd_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsadd_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsadd_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsadd_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsadd_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssubu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vssubu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vssubu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssubu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssubu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssubu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vssubu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vssubu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssubu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssubu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssubu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vssubu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vssubu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssubu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssubu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssubu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vssubu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vssubu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssubu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssubu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssubu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vssubu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vssubu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssubu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssubu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssubu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vssubu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vssubu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssubu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssubu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssubu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vssubu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vssubu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssubu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssubu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssubu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vssubu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vssubu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssubu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssubu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssubu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vssubu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vssubu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssubu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssubu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssubu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vssubu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vssubu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssubu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssubu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssubu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vssubu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vssubu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssubu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssubu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vssubu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssubu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vssubu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vssubu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssubu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssubu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssubu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vssubu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vssubu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssubu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssubu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssubu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vssubu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vssubu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssubu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssubu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssubu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vssubu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vssubu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssubu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssubu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssubu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vssubu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vssubu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssubu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssubu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssubu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vssubu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vssubu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssubu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssubu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssubu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vssubu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vssubu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssubu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssubu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssubu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vssubu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vssubu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssubu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssubu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssubu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vssubu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vssubu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssubu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssubu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssubu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vssubu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vssubu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssubu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssubu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssubu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vssubu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vssubu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssubu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssubu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssubu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vssubu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vssubu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssubu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssubu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssubu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vssubu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vssubu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssubu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssubu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssubu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vssubu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vssubu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssubu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssubu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vssubu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssubu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vssubu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vssubu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssubu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssubu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssubu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vssubu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vssubu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssubu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssubu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssubu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vssubu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vssubu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssubu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssubu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssub.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vssub_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vssub.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssub.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssub.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssub.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vssub_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vssub.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssub.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssub.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssub.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vssub_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vssub.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssub.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssub.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssub.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vssub_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vssub.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssub.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssub.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssub.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vssub_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vssub.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssub.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssub.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssub.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vssub_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vssub.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssub.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssub.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssub.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vssub_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vssub.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssub.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssub.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssub.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vssub_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vssub.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssub.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssub.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssub.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vssub_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vssub.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssub.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssub.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssub.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vssub_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vssub.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssub.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssub.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssub.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vssub_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vssub.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssub.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssub.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vssub_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssub.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vssub_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vssub.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssub.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssub.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssub.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vssub_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vssub.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssub.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssub.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssub.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vssub_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vssub.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssub.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssub.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssub.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vssub_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vssub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssub.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssub.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vssub_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vssub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssub.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssub.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vssub_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vssub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssub.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssub.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vssub_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vssub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssub.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssub.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vssub_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vssub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssub.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssub.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vssub_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vssub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssub.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssub.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vssub_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vssub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssub.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssub.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vssub_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vssub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssub.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssub.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vssub_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vssub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssub.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssub.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vssub_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vssub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssub.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssub.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vssub_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vssub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssub.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vssub_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssub.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vssub_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vssub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssub.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssub.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vssub_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vssub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssub.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssub.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vssub_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vssub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssub.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vaadd_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vaadd.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vaadd_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vaadd.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vaadd.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vaadd_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vaadd.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vaadd_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vaadd.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vaadd_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vaadd.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vaadd.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vaadd_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vaadd.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vaadd.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vaadd_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vaadd.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vaadd_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vaadd.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vaadd_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vaadd.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vaadd.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vaadd_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vaadd.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vaadd.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vaadd_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vaadd.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vaadd_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vaadd.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vaadd_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vaadd.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vaadd.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vaadd_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vaadd.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vaadd.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vaadd_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vaadd.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vaadd_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vaadd.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vaadd_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vaadd.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vaadd.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vaadd_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vaadd.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vaadd_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vaadd.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vaadd_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vaadd.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vaadd.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vaadd_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vaadd.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vaadd.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vaadd_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vaadd.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vaadd_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vaadd.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vaadd_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vaadd.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vaadd.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vaadd_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vaadd.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vaadd.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vaadd_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vaadd.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vaadd_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vaadd.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vaadd_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vaadd.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vaadd.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vaadd_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vaadd.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vaadd.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vaadd_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vaadd.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vaadd_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vaadd_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vasub.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vasub_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vasub.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vasub.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vasub.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vasub.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vasub_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vasub.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vasub.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vasub.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vasub.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vasub_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vasub.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vasub.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vasub.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vasub.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vasub_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vasub.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vasub.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vasub.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vasub.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vasub_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vasub.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vasub.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vasub.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vasub.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vasub_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vasub.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vasub.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vasub.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vasub.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vasub_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vasub.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vasub.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vasub.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vasub.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vasub_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vasub.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vasub.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vasub.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vasub.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vasub_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vasub.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vasub.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vasub.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vasub.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vasub_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vasub.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vasub.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vasub.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vasub.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vasub_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vasub.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vasub.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vasub.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vasub_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vasub.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vasub_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vasub.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vasub.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vasub.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vasub.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vasub_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vasub.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vasub.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vasub.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vasub.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vasub_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vasub.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vasub.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vasub.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vasub.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vasub_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vasub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vasub.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vasub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vasub.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vasub_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vasub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vasub.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vasub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vasub.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vasub_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vasub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vasub.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vasub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vasub.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vasub_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vasub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vasub.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vasub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vasub.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vasub_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vasub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vasub.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vasub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vasub.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vasub_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vasub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vasub.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vasub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vasub.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vasub_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vasub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vasub.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vasub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vasub.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vasub_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vasub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vasub.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vasub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vasub.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vasub_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vasub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vasub.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vasub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vasub.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vasub_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vasub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vasub.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vasub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vasub.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vasub_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vasub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vasub.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vasub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vasub_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vasub.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vasub_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vasub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vasub.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vasub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vasub.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vasub_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vasub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vasub.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vasub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vasub.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vasub_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vasub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vasub.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vasub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsmul.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsmul_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vsmul.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsmul.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsmul.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsmul.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsmul_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vsmul.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsmul.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsmul.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsmul.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsmul_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vsmul.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsmul.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsmul.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsmul.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsmul_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vsmul.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsmul.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsmul.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsmul.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsmul_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vsmul.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsmul.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsmul.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsmul.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsmul_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vsmul.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsmul.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsmul.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsmul.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsmul_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vsmul.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsmul.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsmul.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsmul.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsmul_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vsmul.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsmul.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsmul.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsmul.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsmul_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vsmul.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsmul.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsmul.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsmul.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsmul_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vsmul.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsmul.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsmul.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsmul.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsmul_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vsmul.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsmul.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsmul.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsmul_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsmul.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsmul_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vsmul.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsmul.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsmul.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsmul.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsmul_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vsmul.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsmul.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsmul.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsmul.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsmul_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vsmul.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsmul.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsmul.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsmul.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsmul_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsmul.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsmul.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsmul.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsmul.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsmul_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsmul.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsmul.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsmul.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsmul.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsmul_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsmul.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsmul.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsmul.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsmul.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsmul_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsmul.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsmul.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsmul.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsmul.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsmul_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsmul.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsmul.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsmul.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsmul.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsmul_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsmul.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsmul.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsmul.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsmul.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsmul_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsmul.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsmul.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsmul.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsmul.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsmul_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsmul.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsmul.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsmul.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsmul.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsmul_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsmul.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsmul.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsmul.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsmul.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsmul_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsmul.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsmul.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsmul.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsmul.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsmul_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsmul.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsmul.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsmul.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsmul_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsmul.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsmul_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsmul.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsmul.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsmul.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsmul.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsmul_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsmul.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsmul.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsmul.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsmul.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsmul_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsmul.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsmul.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsmul.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vssrl_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vssrl_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vssrl_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vssrl_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vssrl_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vssrl_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vssrl_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vssrl_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vssrl_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vssrl_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vssrl_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vssrl_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vssrl_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vssrl_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vssrl_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vssrl_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vssrl_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vssrl_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vssrl_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vssrl_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vssrl_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vssrl_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vssrl_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vssrl_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vssrl_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vssrl_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vssrl_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vssrl_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vssrl_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vssrl_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vssra_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vssra_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vssra_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vssra_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vssra_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vssra_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vssra_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vssra_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vssra_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vssra_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vssra_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vssra_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vssra_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vssra_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vssra_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vssra_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vssra_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vssra_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vssra_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vssra_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vssra_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vssra_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vssra_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vssra_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vssra_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vssra_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vssra_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vssra_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vssra_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vssra_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfadd_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfadd.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfadd.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfadd.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfadd_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfadd.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfadd.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfadd.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfadd.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfadd_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfadd.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfadd.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfadd.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfadd.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfadd_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfadd.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfadd.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfadd.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfadd_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfadd_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfadd.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfadd.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfadd.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfadd_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfadd.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfadd.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfadd.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfadd.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfadd_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfadd.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfadd.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfadd.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfadd_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfadd.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfadd.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfadd.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfadd_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfadd.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfadd.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfadd.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfadd.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfadd_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfadd.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfadd.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfadd.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfadd.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfadd_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfadd.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfadd.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfadd.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfadd_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfadd_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfadd.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfadd.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfadd.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfadd_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfadd.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfadd.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfadd.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfadd.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfadd_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfadd.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfadd.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfadd.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsub.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfsub_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfsub.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsub.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsub.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsub.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfsub_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfsub.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsub.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsub.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsub.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfsub_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfsub.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsub.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsub.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsub.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfsub_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfsub.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsub.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsub.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfsub_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsub.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfsub_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfsub.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsub.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsub.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsub.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfsub_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfsub.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsub.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsub.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsub.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfsub_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfsub.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsub.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsub.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsub.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfsub_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfsub.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsub.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsub.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsub.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfsub_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfsub.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsub.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsub.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsub.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfsub_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfsub.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsub.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsub.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsub.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfsub_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfsub.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsub.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsub.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfsub_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsub.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfsub_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfsub.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsub.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsub.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsub.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfsub_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfsub.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsub.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsub.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsub.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfsub_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfsub.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsub.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsub.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwadd.nxv2f64.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwadd_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwadd.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.nxv2f64.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwadd.mask.nxv2f64.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.mask.nxv2f64.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwadd.nxv4f64.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwadd_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwadd.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.nxv4f64.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwadd.mask.nxv4f64.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.mask.nxv4f64.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwadd.nxv8f64.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwadd_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwadd.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.nxv8f64.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwadd.mask.nxv8f64.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.mask.nxv8f64.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwadd.nxv2f64.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfwadd_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwadd.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.nxv2f64.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwadd.mask.nxv2f64.f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.mask.nxv2f64.f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwadd.nxv4f64.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfwadd_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwadd.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.nxv4f64.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwadd.mask.nxv4f64.f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.mask.nxv4f64.f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwadd.nxv8f64.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfwadd_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwadd.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.nxv8f64.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwadd.mask.nxv8f64.f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.mask.nxv8f64.f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwsub.nxv2f64.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwsub_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwsub.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.nxv2f64.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwsub.mask.nxv2f64.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.mask.nxv2f64.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwsub.nxv4f64.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwsub_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwsub.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.nxv4f64.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwsub.mask.nxv4f64.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.mask.nxv4f64.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwsub.nxv8f64.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwsub_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwsub.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.nxv8f64.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwsub.mask.nxv8f64.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.mask.nxv8f64.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwsub.nxv2f64.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfwsub_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwsub.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.nxv2f64.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwsub.mask.nxv2f64.f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.mask.nxv2f64.f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwsub.nxv4f64.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfwsub_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwsub.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.nxv4f64.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwsub.mask.nxv4f64.f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.mask.nxv4f64.f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwsub.nxv8f64.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfwsub_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwsub.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.nxv8f64.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwsub.mask.nxv8f64.f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.mask.nxv8f64.f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmul.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmul_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfmul.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmul.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmul.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmul.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmul_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfmul.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmul.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmul.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmul.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmul_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfmul.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmul.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmul.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmul.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmul_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfmul.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmul.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmul.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmul_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmul.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmul_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfmul.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmul.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmul.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmul.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmul_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfmul.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmul.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmul.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmul.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmul_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfmul.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmul.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmul.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmul.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfmul_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfmul.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmul.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmul.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmul.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfmul_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfmul.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmul.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmul.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmul.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfmul_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfmul.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmul.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmul.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmul.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfmul_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfmul.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmul.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmul.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfmul_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmul.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfmul_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfmul.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmul.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmul.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmul.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfmul_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfmul.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmul.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmul.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmul.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfmul_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfmul.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmul.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmul.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfdiv.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfdiv_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfdiv.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfdiv.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfdiv.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfdiv.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfdiv_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfdiv.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfdiv.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfdiv.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfdiv.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfdiv_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfdiv.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfdiv.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfdiv.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfdiv.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfdiv_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfdiv.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfdiv.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfdiv.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfdiv_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfdiv.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfdiv_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfdiv.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfdiv.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfdiv.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfdiv.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfdiv_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfdiv.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfdiv.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfdiv.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfdiv.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfdiv_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfdiv.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfdiv.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfdiv.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfdiv.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfdiv_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfdiv.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfdiv.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfdiv.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfdiv.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfdiv_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfdiv.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfdiv.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfdiv.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfdiv.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfdiv_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfdiv.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfdiv.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfdiv.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfdiv.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfdiv_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfdiv.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfdiv.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfdiv.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfdiv_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfdiv.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfdiv_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfdiv.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfdiv.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfdiv.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfdiv.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfdiv_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfdiv.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfdiv.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfdiv.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfdiv.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfdiv_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfdiv.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfdiv.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfdiv.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfrdiv.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfrdiv_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfrdiv.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfrdiv.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfrdiv.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfrdiv.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfrdiv_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfrdiv.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfrdiv.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfrdiv.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfrdiv.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfrdiv_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfrdiv.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfrdiv.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfrdiv.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfrdiv.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfrdiv_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfrdiv.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfrdiv.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfrdiv.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfrdiv.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfrdiv_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfrdiv.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfrdiv.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfrdiv.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfrdiv.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfrdiv_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfrdiv.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfrdiv.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfrdiv.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfrdiv.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfrdiv_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfrdiv.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfrdiv.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfrdiv.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfrdiv.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfrdiv_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfrdiv.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfrdiv.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfrdiv.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwmul.nxv2f64.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwmul_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwmul.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfwmul.nxv2f64.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwmul.mask.nxv2f64.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwmul.mask.nxv2f64.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwmul.nxv4f64.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwmul_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwmul.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfwmul.nxv4f64.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwmul.mask.nxv4f64.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwmul.mask.nxv4f64.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwmul.nxv8f64.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwmul_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwmul.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfwmul.nxv8f64.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwmul.mask.nxv8f64.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwmul.mask.nxv8f64.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwmul.nxv2f64.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfwmul_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwmul.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfwmul.nxv2f64.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwmul.mask.nxv2f64.f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfwmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwmul.mask.nxv2f64.f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwmul.nxv4f64.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfwmul_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwmul.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfwmul.nxv4f64.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwmul.mask.nxv4f64.f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfwmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwmul.mask.nxv4f64.f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwmul.nxv8f64.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfwmul_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwmul.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfwmul.nxv8f64.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwmul.mask.nxv8f64.f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfwmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwmul.mask.nxv8f64.f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmin.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmin_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfmin.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmin.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmin.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmin.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmin_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfmin.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmin.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmin.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmin.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmin_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfmin.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmin.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmin.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmin.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmin_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfmin.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmin.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmin.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmin_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmin.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmin_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfmin.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmin.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmin.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmin.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmin_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfmin.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmin.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmin.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmin.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmin_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfmin.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmin.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmin.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmin.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfmin_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfmin.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmin.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmin.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmin.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfmin_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfmin.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmin.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmin.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmin.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfmin_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfmin.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmin.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmin.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmin.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfmin_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfmin.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmin.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmin.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfmin_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmin.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfmin_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfmin.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmin.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmin.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmin.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfmin_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfmin.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmin.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmin.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmin.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfmin_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfmin.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmin.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmin.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmax.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmax_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfmax.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmax.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmax.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmax.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmax_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfmax.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmax.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmax.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmax.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmax_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfmax.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmax.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmax.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmax.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmax_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfmax.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmax.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmax.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmax_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmax.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmax_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfmax.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmax.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmax.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmax.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmax_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfmax.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmax.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmax.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmax.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmax_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfmax.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmax.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmax.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmax.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfmax_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfmax.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmax.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmax.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmax.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfmax_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfmax.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmax.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmax.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmax.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfmax_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfmax.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmax.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmax.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmax.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfmax_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfmax.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmax.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmax.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfmax_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmax.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfmax_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfmax.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmax.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmax.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmax.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfmax_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfmax.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmax.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmax.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmax.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfmax_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfmax.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmax.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmax.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnj.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnj.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnj.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnj.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnj.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnj.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnj.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnj.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnj.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnj.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnj.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnj.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnj.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnj.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnj.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnj.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnj.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnj.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnj.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnj.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnj.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnj.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnj.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnj.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnj.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnj.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnj.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnj.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnj.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfsgnj_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnj.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnj.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnj.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnj.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfsgnj_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnj.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnj.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnj.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnj.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfsgnj_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnj.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnj.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnj.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnj.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfsgnj_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnj.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnj.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnj.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfsgnj_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnj.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfsgnj_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnj.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnj.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnj.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnj.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfsgnj_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnj.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnj.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnj.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnj.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfsgnj_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnj.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnj.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnj.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnjn.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjn.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnjn.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjn.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnjn.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjn.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnjn.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjn.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnjn.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjn.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnjn.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjn.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnjn.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjn.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnjn.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjn.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnjn.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjn.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnjn.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjn.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnjn.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjn.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnjn.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjn.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnjn.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjn.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnjn.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjn.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnjn.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjn.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnjn.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjn.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnjn.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjn.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnjn.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjn.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnjn.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjn.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnjn.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjn.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnjn.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjn.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnjn.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjn.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnjn.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjn.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnjn.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjn.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnjn.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjn.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnjn.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjn.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnjx.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjx.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnjx.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjx.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnjx.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjx.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnjx.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjx.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnjx.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjx.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnjx.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjx.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnjx.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjx.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnjx.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjx.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnjx.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjx.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnjx.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjx.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnjx.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjx.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnjx.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjx.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnjx.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjx.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnjx.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjx.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnjx.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjx.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnjx.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjx.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnjx.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjx.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnjx.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjx.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnjx.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjx.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnjx.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjx.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnjx.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjx.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnjx.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjx.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnjx.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjx.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnjx.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjx.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnjx.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjx.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnjx.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjx.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfeq.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfeq_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vfeq.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfeq.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfeq.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfeq.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfeq_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vfeq.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfeq.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfeq.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfeq.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfeq_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vfeq.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfeq.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfeq.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vfeq.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfeq_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vfeq.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vfeq.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vfeq.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfeq.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfeq_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vfeq.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfeq.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vfeq.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfeq.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfeq_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vfeq.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfeq.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfeq.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfeq.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfeq_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vfeq.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfeq.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfeq.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfeq.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfeq_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vfeq.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfeq.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfeq.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfeq.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfeq_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfeq.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfeq.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfeq.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfeq.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfeq_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfeq.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfeq.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfeq.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfeq.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfeq_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfeq.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfeq.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfeq.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vfeq.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfeq_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vfeq.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vfeq.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vfeq.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfeq.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfeq_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vfeq.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfeq.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vfeq.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfeq.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfeq_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfeq.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfeq.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfeq.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfeq.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfeq_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfeq.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfeq.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfeq.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfeq.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfeq_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfeq.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfeq.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfeq_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfeq.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfne.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfne_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vfne.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfne.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfne_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfne.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfne.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfne_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vfne.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfne.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfne_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfne.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfne.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfne_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vfne.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfne.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfne_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfne.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vfne.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfne_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vfne.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vfne.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfne_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vfne.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfne.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfne_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vfne.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfne.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfne_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vfne.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfne.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfne_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vfne.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfne.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfne_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfne.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfne.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfne_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vfne.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfne.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfne_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfne.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfne.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfne_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vfne.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfne.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfne_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfne.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfne.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfne_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfne.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfne.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfne_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfne.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfne.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfne_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfne.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfne.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfne_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfne.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfne.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfne_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfne.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfne.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfne_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfne.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vfne.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfne_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vfne.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vfne.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfne_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vfne.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfne.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfne_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vfne.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfne.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfne_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vfne.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfne.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfne_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfne.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfne.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfne_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfne.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfne.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfne_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfne.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfne.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfne_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfne.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfne.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfne_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfne.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfne.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfne_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfne.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vflt.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vflt_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vflt.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vflt.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vflt_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vflt.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vflt.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vflt_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vflt.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vflt.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vflt_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vflt.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vflt.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vflt_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vflt.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vflt.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vflt_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vflt.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vflt.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vflt_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vflt.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vflt.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vflt_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vflt.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vflt.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vflt_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vflt.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vflt.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vflt_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vflt.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vflt.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vflt_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vflt.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vflt.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vflt_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vflt.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vflt.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vflt_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vflt.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vflt.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vflt_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vflt.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vflt.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vflt_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vflt.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vflt.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vflt_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vflt.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vflt.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vflt_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vflt.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vflt.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vflt_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vflt.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vflt.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vflt_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vflt.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vflt.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vflt_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vflt.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vflt.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vflt_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vflt.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vflt.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vflt_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vflt.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vflt.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vflt_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vflt.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vflt.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vflt_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vflt.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vflt.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vflt_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vflt.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vflt.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vflt_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vflt.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vflt.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vflt_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vflt.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vflt.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vflt_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vflt.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vflt.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vflt_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vflt.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vflt.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vflt_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vflt.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vflt.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vflt_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vflt.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vflt.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vflt_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vflt.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfle.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfle_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vfle.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfle.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfle_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfle.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfle.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfle_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vfle.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfle.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfle_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfle.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfle.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfle_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vfle.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfle.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfle_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfle.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vfle.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfle_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vfle.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vfle.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfle_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vfle.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfle.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfle_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vfle.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfle.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfle_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vfle.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfle.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfle_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vfle.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfle.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfle_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfle.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfle.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfle_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vfle.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfle.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfle_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfle.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfle.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfle_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vfle.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfle.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfle_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfle.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfle.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfle_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfle.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfle.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfle_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfle.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfle.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfle_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfle.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfle.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfle_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfle.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfle.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfle_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfle.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfle.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfle_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfle.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vfle.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfle_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vfle.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vfle.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfle_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vfle.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfle.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfle_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vfle.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfle.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfle_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vfle.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfle.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfle_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfle.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfle.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfle_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfle.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfle.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfle_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfle.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfle.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfle_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfle.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfle.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfle_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfle.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfle.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfle_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfle.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfgt.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfgt_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfgt.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfgt.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfgt_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfgt.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfgt.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfgt_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfgt.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfgt.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfgt_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfgt.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfgt.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfgt_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfgt.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfgt.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfgt_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfgt.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vfgt.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfgt_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vfgt.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vfgt.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfgt_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vfgt.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfgt.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfgt_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vfgt.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfgt.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfgt_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vfgt.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfgt.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfgt_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfgt.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfgt.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfgt_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfgt.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfgt.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfgt_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfgt.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfgt.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfgt_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfgt.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfgt.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfgt_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfgt.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfgt.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfgt_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfgt.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfge.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfge_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfge.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfge.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfge_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfge.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfge.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfge_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfge.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfge.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfge_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfge.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfge.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfge_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfge.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfge.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfge_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfge.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vfge.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfge_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vfge.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vfge.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfge_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vfge.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfge.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfge_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vfge.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfge.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfge_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vfge.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vfge.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfge_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vfge.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vfge.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfge_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vfge.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vfge.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfge_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vfge.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vfge.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfge_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vfge.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vfge.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfge_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vfge.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vfge.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfge_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vfge.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vford.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vford_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vford.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vford.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vford_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vford.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vford.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vford_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vford.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vford.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vford_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vford.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vford.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vford_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vford.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vford.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vford_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vford.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vford.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vford_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vford.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vford.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vford_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vford.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vford.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vford_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vford.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vford.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vford_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vford.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vford.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vford_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vford.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vford.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vford_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vford.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vford.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vford_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vford.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vford.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vford_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vford.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vford.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vford_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vford.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vford.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vford_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vford.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vford.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vford_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vford.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vford.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vford_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vford.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vford.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vford_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vford.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vford.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vford_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vford.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vford.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vford_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vford.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vford.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vford_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vford.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vford.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vford_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vford.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vford.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vford_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vford.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vford.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vford_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vford.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vford.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vford_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vford.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vford.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vford_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vford.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vford.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vford_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vford.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vford.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vford_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vford.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vford.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vford_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vford.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vford.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vford_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vford.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vford.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vford_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vford.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmerge.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmerge_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfmerge.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmerge.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmerge.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmerge.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmerge_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfmerge.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmerge.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmerge.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmerge.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmerge_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfmerge.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmerge.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmerge.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmerge.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmerge_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfmerge.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmerge.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmerge.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmerge_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmerge.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmerge.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmerge.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmerge_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfmerge.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmerge.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmerge.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmerge.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmerge_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfmerge.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmerge.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmerge.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmerge.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmerge_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfmerge.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmerge.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmerge.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmerge.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfmerge_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfmerge.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmerge.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmerge.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmerge.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfmerge_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfmerge.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmerge.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmerge.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmerge.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfmerge_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfmerge.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmerge.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmerge.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmerge.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfmerge_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfmerge.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmerge.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmerge.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfmerge_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmerge.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmerge.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmerge.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfmerge_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfmerge.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmerge.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmerge.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmerge.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfmerge_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfmerge.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmerge.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmerge.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmerge.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfmerge_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfmerge.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmerge.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmerge_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmerge.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredsum.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredsum_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vredsum.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredsum.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredsum.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredsum.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredsum_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vredsum.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredsum.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredsum.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredsum.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredsum_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vredsum.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredsum.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredsum.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredsum.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredsum_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vredsum.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredsum.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredsum.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredsum.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredsum_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vredsum.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredsum.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredsum.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredsum.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredsum_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vredsum.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredsum.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredsum.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredsum.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredsum_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vredsum.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredsum.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredsum.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredsum.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredsum_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vredsum.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredsum.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredsum.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredsum.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredsum_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vredsum.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredsum.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredsum.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredsum.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredsum_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vredsum.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredsum.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredsum.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredsum.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredsum_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vredsum.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredsum.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredsum.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredsum.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredsum_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vredsum.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredsum.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredsum.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredsum.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredsum_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vredsum.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredsum.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredsum.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredsum.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredsum_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vredsum.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredsum.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredsum.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredsum.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredsum_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vredsum.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredsum.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredsum.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredand.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredand_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vredand.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredand.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredand.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredand.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredand_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vredand.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredand.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredand.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredand.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredand_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vredand.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredand.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredand.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredand.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredand_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vredand.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredand.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredand.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredand.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredand_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vredand.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredand.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredand.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredand.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredand_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vredand.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredand.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredand.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredand.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredand_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vredand.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredand.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredand.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredand.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredand_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vredand.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredand.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredand.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredand.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredand_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vredand.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredand.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredand.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredand.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredand_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vredand.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredand.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredand.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredand.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredand_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vredand.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredand.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredand.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredand.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredand_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vredand.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredand.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredand.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredand.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredand_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vredand.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredand.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredand.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredand.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredand_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vredand.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredand.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredand.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredand.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredand_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vredand.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredand.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredand.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredor.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredor_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vredor.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredor.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredor.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredor.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredor_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vredor.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredor.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredor.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredor.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredor_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vredor.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredor.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredor.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredor.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredor_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vredor.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredor.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredor.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredor.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredor_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vredor.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredor.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredor.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredor.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredor_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vredor.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredor.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredor.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredor.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredor_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vredor.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredor.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredor.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredor.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredor_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vredor.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredor.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredor.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredor.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredor_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vredor.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredor.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredor.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredor.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredor_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vredor.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredor.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredor.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredor.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredor_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vredor.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredor.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredor.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredor.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredor_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vredor.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredor.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredor.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredor.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredor_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vredor.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredor.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredor.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredor.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredor_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vredor.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredor.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredor.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredor.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredor_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vredor.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredor.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredor.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredxor.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredxor_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vredxor.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredxor.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredxor.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredxor.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredxor_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vredxor.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredxor.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredxor.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredxor.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredxor_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vredxor.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredxor.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredxor.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredxor.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredxor_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vredxor.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredxor.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredxor.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredxor.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredxor_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vredxor.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredxor.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredxor.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredxor.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredxor_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vredxor.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredxor.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredxor.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredxor.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredxor_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vredxor.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredxor.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredxor.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredxor.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredxor_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vredxor.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredxor.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredxor.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredxor.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredxor_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vredxor.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredxor.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredxor.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredxor.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredxor_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vredxor.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredxor.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredxor.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredxor.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredxor_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vredxor.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredxor.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredxor.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredxor.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredxor_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vredxor.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredxor.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredxor.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredxor.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredxor_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vredxor.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredxor.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredxor.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredxor.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredxor_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vredxor.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredxor.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredxor.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredxor.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredxor_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vredxor.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredxor.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredxor.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredminu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredminu_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vredminu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredminu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredminu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredminu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredminu_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vredminu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredminu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredminu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredminu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredminu_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vredminu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredminu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredminu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredminu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredminu_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vredminu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredminu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredminu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredminu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredminu_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vredminu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredminu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredminu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredminu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredminu_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vredminu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredminu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredminu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredminu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredminu_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vredminu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredminu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredminu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredminu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredminu_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vredminu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredminu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredminu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredminu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredminu_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vredminu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredminu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredminu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredminu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredminu_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vredminu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredminu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredminu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredminu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredminu_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vredminu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredminu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredminu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredminu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredminu_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vredminu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredminu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredminu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredminu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredminu_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vredminu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredminu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredminu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredminu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredminu_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vredminu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredminu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredminu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredminu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredminu_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vredminu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredminu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredminu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredmin.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredmin_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vredmin.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredmin.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredmin.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredmin.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredmin_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vredmin.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredmin.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredmin.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredmin.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredmin_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vredmin.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredmin.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredmin.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredmin.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredmin_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vredmin.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredmin.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredmin.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredmin.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredmin_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vredmin.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredmin.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredmin.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredmin.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredmin_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vredmin.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredmin.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredmin.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredmin.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredmin_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vredmin.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredmin.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredmin.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredmin.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredmin_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vredmin.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredmin.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredmin.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredmin.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredmin_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vredmin.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredmin.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredmin.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredmin.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredmin_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vredmin.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredmin.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredmin.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredmin.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredmin_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vredmin.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredmin.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredmin.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredmin.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredmin_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vredmin.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredmin.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredmin.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredmin.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredmin_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vredmin.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredmin.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredmin.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredmin.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredmin_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vredmin.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredmin.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredmin.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredmin.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredmin_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vredmin.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredmin.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredmin.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredmaxu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vredmaxu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredmaxu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredmaxu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredmaxu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vredmaxu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredmaxu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredmaxu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredmaxu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vredmaxu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredmaxu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredmaxu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredmaxu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vredmaxu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredmaxu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredmaxu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredmaxu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vredmaxu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredmaxu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredmaxu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredmaxu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vredmaxu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredmaxu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredmaxu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredmaxu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vredmaxu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredmaxu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredmaxu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredmaxu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vredmaxu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredmaxu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredmaxu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredmaxu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vredmaxu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredmaxu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredmaxu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredmaxu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vredmaxu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredmaxu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredmaxu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredmaxu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vredmaxu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredmaxu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredmaxu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredmaxu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vredmaxu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredmaxu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredmaxu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredmaxu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vredmaxu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredmaxu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredmaxu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredmaxu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vredmaxu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredmaxu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredmaxu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredmaxu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vredmaxu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredmaxu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredmaxu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredmax.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredmax_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vredmax.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredmax.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredmax.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredmax.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredmax_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vredmax.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredmax.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredmax.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredmax.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredmax_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vredmax.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredmax.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredmax.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredmax.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredmax_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vredmax.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredmax.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredmax.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredmax.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredmax_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vredmax.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredmax.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredmax.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredmax.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredmax_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vredmax.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredmax.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredmax.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredmax.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredmax_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vredmax.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredmax.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredmax.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredmax.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredmax_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vredmax.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredmax.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredmax.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredmax.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredmax_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vredmax.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredmax.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredmax.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredmax.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredmax_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vredmax.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredmax.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredmax.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredmax.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredmax_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vredmax.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredmax.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredmax.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredmax.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredmax_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vredmax.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredmax.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredmax.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredmax.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredmax_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vredmax.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredmax.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredmax.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredmax.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredmax_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vredmax.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredmax.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredmax.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredmax.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredmax_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vredmax.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredmax.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredmax.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfredsum.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfredsum_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfredsum.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfredsum.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfredsum.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfredsum.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfredsum_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfredsum.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfredsum.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfredsum.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfredsum.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfredsum_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfredsum.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfredsum.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfredsum.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfredsum.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfredsum_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfredsum.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfredsum.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfredsum.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredsum.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfredsum_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfredsum.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredsum.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfredsum.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfredsum.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfredsum_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfredsum.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfredsum.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfredsum.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfredsum.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfredsum_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfredsum.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfredsum.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfredsum.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfredsum.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfredsum_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfredsum.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfredsum.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfredsum.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfredosum.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfredosum_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfredosum.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfredosum.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfredosum.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfredosum.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfredosum_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfredosum.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfredosum.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfredosum.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfredosum.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfredosum_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfredosum.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfredosum.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfredosum.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfredosum.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfredosum_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfredosum.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfredosum.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfredosum.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredosum.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfredosum_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfredosum.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredosum.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfredosum.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfredosum.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfredosum_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfredosum.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfredosum.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfredosum.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfredosum.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfredosum_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfredosum.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfredosum.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfredosum.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfredosum.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfredosum_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfredosum.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfredosum.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfredosum.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfredmin.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfredmin_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfredmin.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfredmin.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfredmin.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfredmin.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfredmin_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfredmin.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfredmin.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfredmin.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfredmin.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfredmin_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfredmin.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfredmin.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfredmin.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfredmin.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfredmin_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfredmin.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfredmin.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfredmin.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredmin.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfredmin_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfredmin.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredmin.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfredmin.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfredmin.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfredmin_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfredmin.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfredmin.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfredmin.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfredmin.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfredmin_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfredmin.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfredmin.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfredmin.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfredmin.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfredmin_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfredmin.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfredmin.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfredmin.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfredmax.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfredmax_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfredmax.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfredmax.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfredmax.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfredmax.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfredmax_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfredmax.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfredmax.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfredmax.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfredmax.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfredmax_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfredmax.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfredmax.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfredmax.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfredmax.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfredmax_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfredmax.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfredmax.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfredmax.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredmax.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfredmax_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfredmax.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredmax.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfredmax.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfredmax.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfredmax_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfredmax.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfredmax.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfredmax.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfredmax.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfredmax_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfredmax.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfredmax.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfredmax.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfredmax.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfredmax_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfredmax.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfredmax.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfredmax.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdotu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vdotu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vdotu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdotu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdotu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdotu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vdotu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vdotu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdotu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdotu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdotu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vdotu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vdotu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdotu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdotu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdotu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vdotu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vdotu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdotu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdotu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdotu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vdotu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vdotu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdotu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdotu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdotu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vdotu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vdotu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdotu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdotu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdotu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vdotu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vdotu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdotu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdotu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdotu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vdotu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vdotu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdotu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdotu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdotu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vdotu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vdotu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdotu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdotu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdotu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vdotu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vdotu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdotu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdotu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdotu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vdotu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vdotu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdotu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdotu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdotu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vdotu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vdotu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdotu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdotu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdotu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vdotu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vdotu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdotu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdotu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdotu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vdotu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vdotu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdotu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdotu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdotu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vdotu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vdotu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdotu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdotu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdotu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdot.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vdot_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vdot.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdot.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdot.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdot.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vdot_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vdot.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdot.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdot.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdot.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vdot_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vdot.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdot.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdot.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdot.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vdot_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vdot.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdot.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdot.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdot.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vdot_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vdot.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdot.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdot.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdot.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vdot_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vdot.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdot.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdot.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdot.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vdot_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vdot.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdot.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdot.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdot.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vdot_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vdot.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdot.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdot.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdot.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vdot_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vdot.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdot.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdot.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdot.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vdot_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vdot.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdot.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdot.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdot.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vdot_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vdot.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdot.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdot.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdot.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vdot_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vdot.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdot.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdot.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdot.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vdot_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vdot.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdot.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdot.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdot.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vdot_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vdot.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdot.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdot.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdot.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vdot_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vdot.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdot.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdot_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdot.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfdot.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfdot_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfdot.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfdot.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdot_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfdot.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfdot.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfdot_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfdot.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfdot.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdot_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfdot.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfdot.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfdot_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfdot.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfdot.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdot_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfdot.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfdot.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfdot_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfdot.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfdot.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfdot_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfdot.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfdot.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfdot_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfdot.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfdot.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfdot_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfdot.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfdot.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfdot_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfdot.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfdot.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdot_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfdot.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfdot.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfdot_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfdot.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfdot.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdot_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfdot.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfdot.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfdot_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfdot.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfdot.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdot_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfdot.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vrgather_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vrgather_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vrgather_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vrgather_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vrgather_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vrgather_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vrgather_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.nxv2i32(
  <vscale x 2 x float>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv2f32_nxv2f32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv2f32_nxv2f32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.nxv2i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.nxv2i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv2f32_nxv2f32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv2f32_nxv2f32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.nxv2i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.nxv4i32(
  <vscale x 4 x float>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv4f32_nxv4f32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4f32_nxv4f32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.nxv4i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.nxv4i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4f32_nxv4f32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4f32_nxv4f32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.nxv4i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.nxv8i32(
  <vscale x 8 x float>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv8f32_nxv8f32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8f32_nxv8f32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.nxv8i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.nxv8i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8f32_nxv8f32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8f32_nxv8f32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.nxv8i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.nxv16i32(
  <vscale x 16 x float>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv16f32_nxv16f32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv16f32_nxv16f32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.nxv16i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.nxv16i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv16f32_nxv16f32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv16f32_nxv16f32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.nxv16i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(
  <vscale x 1 x double>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv1f64_nxv1f64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv1f64_nxv1f64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv1f64_nxv1f64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv1f64_nxv1f64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.nxv2i64(
  <vscale x 2 x double>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv2f64_nxv2f64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv2f64_nxv2f64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.nxv2i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.nxv2i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv2f64_nxv2f64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv2f64_nxv2f64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.nxv2i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.nxv4i64(
  <vscale x 4 x double>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv4f64_nxv4f64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4f64_nxv4f64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.nxv4i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.nxv4i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4f64_nxv4f64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4f64_nxv4f64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.nxv4i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.nxv8i64(
  <vscale x 8 x double>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv8f64_nxv8f64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8f64_nxv8f64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.nxv8i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.nxv8i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8f64_nxv8f64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8f64_nxv8f64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.nxv8i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vrgather_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vrgather_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vrgather_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vrgather_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vrgather_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vrgather_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vrgather_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vslideup.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vslideup_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vslideup.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vslideup.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslideup.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vslideup.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vslideup_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vslideup.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vslideup.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslideup.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vslideup.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vslideup_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vslideup.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vslideup.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslideup.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vslideup.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vslideup_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vslideup.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vslideup.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslideup.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vslideup.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vslideup_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vslideup.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vslideup.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslideup.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vslideup.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vslideup_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vslideup.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vslideup.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslideup.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vslideup.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vslideup_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vslideup.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vslideup.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslideup.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vslideup.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vslideup.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vslideup.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslideup.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vslideup.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vslideup.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vslideup.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslideup.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vslideup.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vslideup.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vslideup.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslideup.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vslideup.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vslideup.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vslideup.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslideup.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vslideup.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vslideup.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vslideup.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslideup.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vslideup.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vslideup.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vslideup.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslideup.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vslideup.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vslideup.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vslideup.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslideup.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vslideup.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 2 x float> @llvm.epi.vslideup.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vslideup.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslideup.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vslideup.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 4 x float> @llvm.epi.vslideup.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vslideup.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslideup.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vslideup.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 8 x float> @llvm.epi.vslideup.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vslideup.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslideup.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vslideup.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 16 x float> @llvm.epi.vslideup.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vslideup.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslideup.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vslideup.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 2 x double> @llvm.epi.vslideup.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vslideup.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslideup.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vslideup.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 4 x double> @llvm.epi.vslideup.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vslideup.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslideup.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vslideup.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 8 x double> @llvm.epi.vslideup.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vslideup.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslideup.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vslideup.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslideup.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vslideup.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslideup.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vslideup.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslideup.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vslideup.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslideup.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vslideup.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslideup.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vslideup.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslideup.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vslideup.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslideup.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vslideup.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslideup.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vslideup.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslideup.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vslideup.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslideup.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vslideup.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslideup.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vslideup.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslideup.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vslideup.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslideup.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vslideup.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslideup.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 2 x float> @llvm.epi.vslideup.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslideup.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 4 x float> @llvm.epi.vslideup.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslideup.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 8 x float> @llvm.epi.vslideup.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslideup.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 16 x float> @llvm.epi.vslideup.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslideup.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 2 x double> @llvm.epi.vslideup.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslideup.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 4 x double> @llvm.epi.vslideup.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslideup.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 8 x double> @llvm.epi.vslideup.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslideup.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vslidedown.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vslidedown_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vslidedown.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vslidedown.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslidedown.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vslidedown.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vslidedown_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vslidedown.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vslidedown.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslidedown.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vslidedown.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vslidedown_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vslidedown.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vslidedown.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslidedown.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vslidedown.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vslidedown_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vslidedown.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vslidedown.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslidedown.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vslidedown.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vslidedown_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vslidedown.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vslidedown.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslidedown.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vslidedown.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vslidedown_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vslidedown.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vslidedown.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslidedown.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vslidedown.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vslidedown_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vslidedown.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vslidedown.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslidedown.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vslidedown.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vslidedown.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vslidedown.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslidedown.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vslidedown.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vslidedown.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vslidedown.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslidedown.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vslidedown.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vslidedown.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vslidedown.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslidedown.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vslidedown.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vslidedown.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vslidedown.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslidedown.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vslidedown.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vslidedown.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vslidedown.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslidedown.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vslidedown.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vslidedown.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vslidedown.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslidedown.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vslidedown.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vslidedown.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vslidedown.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslidedown.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vslidedown.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 2 x float> @llvm.epi.vslidedown.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vslidedown.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslidedown.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vslidedown.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 4 x float> @llvm.epi.vslidedown.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vslidedown.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslidedown.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vslidedown.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 8 x float> @llvm.epi.vslidedown.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vslidedown.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslidedown.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vslidedown.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 16 x float> @llvm.epi.vslidedown.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vslidedown.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslidedown.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vslidedown.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 2 x double> @llvm.epi.vslidedown.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vslidedown.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslidedown.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vslidedown.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 4 x double> @llvm.epi.vslidedown.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vslidedown.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslidedown.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vslidedown.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 8 x double> @llvm.epi.vslidedown.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vslidedown.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslidedown.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vslidedown.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslidedown.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vslidedown.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslidedown.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vslidedown.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslidedown.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vslidedown.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslidedown.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vslidedown.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslidedown.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vslidedown.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslidedown.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vslidedown.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslidedown.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vslidedown.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslidedown.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vslidedown.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslidedown.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vslidedown.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslidedown.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vslidedown.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslidedown.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vslidedown.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslidedown.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vslidedown.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslidedown.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vslidedown.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslidedown.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 2 x float> @llvm.epi.vslidedown.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslidedown.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 4 x float> @llvm.epi.vslidedown.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslidedown.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 8 x float> @llvm.epi.vslidedown.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslidedown.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 16 x float> @llvm.epi.vslidedown.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslidedown.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 2 x double> @llvm.epi.vslidedown.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslidedown.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 4 x double> @llvm.epi.vslidedown.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslidedown.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 8 x double> @llvm.epi.vslidedown.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslidedown.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vslide1up.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1up_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vslide1up.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vslide1up.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslide1up.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vslide1up.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1up_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vslide1up.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vslide1up.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslide1up.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vslide1up.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1up_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vslide1up.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vslide1up.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslide1up.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vslide1up.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1up_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vslide1up.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vslide1up.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslide1up.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vslide1up.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1up_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vslide1up.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vslide1up.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslide1up.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vslide1up.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1up_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vslide1up.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vslide1up.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslide1up.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vslide1up.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1up_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vslide1up.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vslide1up.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslide1up.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vslide1up.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vslide1up.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vslide1up.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslide1up.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vslide1up.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vslide1up.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vslide1up.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslide1up.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vslide1up.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vslide1up.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vslide1up.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslide1up.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vslide1up.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vslide1up.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vslide1up.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslide1up.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1up.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vslide1up.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1up.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslide1up.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vslide1up.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vslide1up.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vslide1up.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslide1up.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vslide1up.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vslide1up.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vslide1up.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslide1up.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vslide1up.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vslide1up.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vslide1up.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslide1up.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vslide1up.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 2 x float> @llvm.epi.vslide1up.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vslide1up.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslide1up.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vslide1up.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 4 x float> @llvm.epi.vslide1up.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vslide1up.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslide1up.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vslide1up.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 8 x float> @llvm.epi.vslide1up.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vslide1up.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslide1up.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vslide1up.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 16 x float> @llvm.epi.vslide1up.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vslide1up.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslide1up.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1up.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x double> @llvm.epi.vslide1up.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1up.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslide1up.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vslide1up.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 2 x double> @llvm.epi.vslide1up.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vslide1up.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslide1up.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vslide1up.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 4 x double> @llvm.epi.vslide1up.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vslide1up.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslide1up.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vslide1up.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 8 x double> @llvm.epi.vslide1up.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vslide1up.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslide1up.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vslide1down.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1down_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vslide1down.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vslide1down.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslide1down.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vslide1down.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1down_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vslide1down.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vslide1down.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslide1down.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vslide1down.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1down_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vslide1down.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vslide1down.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslide1down.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vslide1down.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1down_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vslide1down.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vslide1down.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslide1down.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vslide1down.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1down_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vslide1down.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vslide1down.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslide1down.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vslide1down.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1down_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vslide1down.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vslide1down.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslide1down.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vslide1down.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1down_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vslide1down.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vslide1down.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslide1down.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vslide1down.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vslide1down.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vslide1down.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslide1down.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vslide1down.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vslide1down.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vslide1down.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslide1down.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vslide1down.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vslide1down.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vslide1down.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslide1down.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vslide1down.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vslide1down.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vslide1down.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslide1down.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1down.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vslide1down.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1down.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslide1down.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vslide1down.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vslide1down.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vslide1down.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslide1down.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vslide1down.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vslide1down.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vslide1down.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslide1down.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vslide1down.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vslide1down.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vslide1down.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslide1down.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vslide1down.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 2 x float> @llvm.epi.vslide1down.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vslide1down.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslide1down.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vslide1down.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 4 x float> @llvm.epi.vslide1down.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vslide1down.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslide1down.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vslide1down.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 8 x float> @llvm.epi.vslide1down.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vslide1down.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslide1down.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vslide1down.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 16 x float> @llvm.epi.vslide1down.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vslide1down.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslide1down.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1down.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x double> @llvm.epi.vslide1down.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1down.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslide1down.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vslide1down.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 2 x double> @llvm.epi.vslide1down.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vslide1down.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslide1down.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vslide1down.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 4 x double> @llvm.epi.vslide1down.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vslide1down.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslide1down.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vslide1down.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 8 x double> @llvm.epi.vslide1down.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vslide1down.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslide1down.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

