; NOTE: Tests autogenerated by utils/EPI/generate-intrinsics-tests.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+epi -verify-machineinstrs < %s \
; RUN:    | FileCheck %s

@scratch = global i8 0, align 16


declare i64 @llvm.epi.vmpopc(<vscale x 1 x i1>);
define void @intrinsic_vmpopc_m_i64_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmpopc_m_i64_i1
; CHECK:       vmpopc.m a0, v0
  %a = call i64 @llvm.epi.vmpopc(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p
  ret void
}

declare i64 @llvm.epi.vmpopc.mask(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmpopc_mask_m_i64_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmpopc_mask_m_i64_i1
; CHECK:       vmpopc.m a0, v0, v0.t
  %a = call i64 @llvm.epi.vmpopc.mask(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p
  ret void
}


declare i64 @llvm.epi.vmfirst(<vscale x 1 x i1>);
define void @intrinsic_vmfirst_m_i64_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfirst_m_i64_i1
; CHECK:       vmfirst.m a0, v0
  %a = call i64 @llvm.epi.vmfirst(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p
  ret void
}

declare i64 @llvm.epi.vmfirst.mask(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmfirst_mask_m_i64_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfirst_mask_m_i64_i1
; CHECK:       vmfirst.m a0, v0, v0.t
  %a = call i64 @llvm.epi.vmfirst.mask(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfsqrt.nxv1f32(<vscale x 1 x float>);
define void @intrinsic_vfsqrt_v_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_f32_f32
; CHECK:       vfsqrt.v v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfsqrt.nxv1f32(<vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfsqrt.mask.nxv1f32(<vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfsqrt_mask_v_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_f32_f32
; CHECK:       vfsqrt.v v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfsqrt.mask.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsqrt.nxv1f64(<vscale x 1 x double>);
define void @intrinsic_vfsqrt_v_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_f64_f64
; CHECK:       vfsqrt.v v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfsqrt.nxv1f64(<vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsqrt.mask.nxv1f64(<vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfsqrt_mask_v_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_f64_f64
; CHECK:       vfsqrt.v v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfsqrt.mask.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vbroadcast.nxv1i8.i8(i8);
define void @intrinsic_vbroadcast_vx_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_vx_i8_i8
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 1 x i8> @llvm.epi.vbroadcast.nxv1i8.i8(i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vbroadcast.mask.nxv1i8.i8(i8, <vscale x 1 x i1>);
define void @intrinsic_vbroadcast_mask_vx_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_mask_vx_i8_i8
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i8> @llvm.epi.vbroadcast.mask.nxv1i8.i8(i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vbroadcast.nxv1i16.i16(i16);
define void @intrinsic_vbroadcast_vx_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_vx_i16_i16
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 1 x i16> @llvm.epi.vbroadcast.nxv1i16.i16(i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vbroadcast.mask.nxv1i16.i16(i16, <vscale x 1 x i1>);
define void @intrinsic_vbroadcast_mask_vx_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_mask_vx_i16_i16
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i16> @llvm.epi.vbroadcast.mask.nxv1i16.i16(i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vbroadcast.nxv1i32.i32(i32);
define void @intrinsic_vbroadcast_vx_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_vx_i32_i32
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 1 x i32> @llvm.epi.vbroadcast.nxv1i32.i32(i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vbroadcast.mask.nxv1i32.i32(i32, <vscale x 1 x i1>);
define void @intrinsic_vbroadcast_mask_vx_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_mask_vx_i32_i32
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i32> @llvm.epi.vbroadcast.mask.nxv1i32.i32(i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64);
define void @intrinsic_vbroadcast_vx_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_vx_i64_i64
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vbroadcast.mask.nxv1i64.i64(i64, <vscale x 1 x i1>);
define void @intrinsic_vbroadcast_mask_vx_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_mask_vx_i64_i64
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vbroadcast.mask.nxv1i64.i64(i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vbroadcast.nxv1f32.f32(float);
define void @intrinsic_vbroadcast_vf_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_vf_f32_f32
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 1 x float> @llvm.epi.vbroadcast.nxv1f32.f32(float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vbroadcast.mask.nxv1f32.f32(float, <vscale x 1 x i1>);
define void @intrinsic_vbroadcast_mask_vf_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_mask_vf_f32_f32
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x float> @llvm.epi.vbroadcast.mask.nxv1f32.f32(float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double);
define void @intrinsic_vbroadcast_vf_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_vf_f64_f64
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vbroadcast.mask.nxv1f64.f64(double, <vscale x 1 x i1>);
define void @intrinsic_vbroadcast_mask_vf_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vbroadcast_mask_vf_f64_f64
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vbroadcast.mask.nxv1f64.f64(double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsbf(<vscale x 1 x i1>);
define void @intrinsic_vmsbf_m_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbf_m_i1_i1
; CHECK:       vmsbf.m v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmsbf(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsbf.mask(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmsbf_mask_m_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbf_mask_m_i1_i1
; CHECK:       vmsbf.m v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vmsbf.mask(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsof(<vscale x 1 x i1>);
define void @intrinsic_vmsof_m_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsof_m_i1_i1
; CHECK:       vmsof.m v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmsof(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsof.mask(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmsof_mask_m_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsof_mask_m_i1_i1
; CHECK:       vmsof.m v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vmsof.mask(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsif(<vscale x 1 x i1>);
define void @intrinsic_vmsif_m_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsif_m_i1_i1
; CHECK:       vmsif.m v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmsif(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsif.mask(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmsif_mask_m_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsif_mask_m_i1_i1
; CHECK:       vmsif.m v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vmsif.mask(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmiota.nxv1i8(<vscale x 1 x i1>);
define void @intrinsic_vmiota_m_i8_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmiota_m_i8_i1
; CHECK:       vmiota.m v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmiota.nxv1i8(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmiota.mask.nxv1i8(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmiota_mask_m_i8_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmiota_mask_m_i8_i1
; CHECK:       vmiota.m v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmiota.mask.nxv1i8(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmiota.nxv1i16(<vscale x 1 x i1>);
define void @intrinsic_vmiota_m_i16_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmiota_m_i16_i1
; CHECK:       vmiota.m v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmiota.nxv1i16(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmiota.mask.nxv1i16(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmiota_mask_m_i16_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmiota_mask_m_i16_i1
; CHECK:       vmiota.m v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmiota.mask.nxv1i16(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmiota.nxv1i32(<vscale x 1 x i1>);
define void @intrinsic_vmiota_m_i32_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmiota_m_i32_i1
; CHECK:       vmiota.m v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmiota.nxv1i32(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmiota.mask.nxv1i32(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmiota_mask_m_i32_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmiota_mask_m_i32_i1
; CHECK:       vmiota.m v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmiota.mask.nxv1i32(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmiota.nxv1i64(<vscale x 1 x i1>);
define void @intrinsic_vmiota_m_i64_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmiota_m_i64_i1
; CHECK:       vmiota.m v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmiota.nxv1i64(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmiota.mask.nxv1i64(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmiota_mask_m_i64_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmiota_mask_m_i64_i1
; CHECK:       vmiota.m v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmiota.mask.nxv1i64(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vid.nxv1i8();
define void @intrinsic_vid_v_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_i8
; CHECK:       vid.v v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vid.nxv1i8()
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vid.mask.nxv1i8(<vscale x 1 x i1>);
define void @intrinsic_vid_mask_v_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_i8
; CHECK:       vid.v v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vid.mask.nxv1i8(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vid.nxv1i16();
define void @intrinsic_vid_v_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_i16
; CHECK:       vid.v v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vid.nxv1i16()
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vid.mask.nxv1i16(<vscale x 1 x i1>);
define void @intrinsic_vid_mask_v_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_i16
; CHECK:       vid.v v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vid.mask.nxv1i16(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vid.nxv1i32();
define void @intrinsic_vid_v_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_i32
; CHECK:       vid.v v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vid.nxv1i32()
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vid.mask.nxv1i32(<vscale x 1 x i1>);
define void @intrinsic_vid_mask_v_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_i32
; CHECK:       vid.v v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vid.mask.nxv1i32(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vid.nxv1i64();
define void @intrinsic_vid_v_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_i64
; CHECK:       vid.v v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vid.nxv1i64()
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vid.mask.nxv1i64(<vscale x 1 x i1>);
define void @intrinsic_vid_mask_v_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_i64
; CHECK:       vid.v v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vid.mask.nxv1i64(<vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vadd.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vadd_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_i8_i8_i8
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vadd.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vadd.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vadd_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_i8_i8_i8
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vadd.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vadd.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vadd_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_i16_i16_i16
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vadd.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vadd.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vadd_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_i16_i16_i16
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vadd.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vadd.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vadd_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_i32_i32_i32
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vadd.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vadd.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vadd_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_i32_i32_i32
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vadd.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vadd_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_i64_i64_i64
; CHECK:       vadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vadd.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vadd_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_i64_i64_i64
; CHECK:       vadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vadd.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vadd.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vadd_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_i8_i8_i8
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vadd.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vadd.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vadd_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_i8_i8_i8
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vadd.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vadd.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vadd_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_i16_i16_i16
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vadd.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vadd.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vadd_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_i16_i16_i16
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vadd.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vadd.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vadd_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_i32_i32_i32
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vadd.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vadd.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vadd_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_i32_i32_i32
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vadd.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vadd_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_i64_i64_i64
; CHECK:       vadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vadd.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vadd_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_i64_i64_i64
; CHECK:       vadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vadd.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vadd_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_i8_i8_i8
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vadd.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vadd_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_i8_i8_i8
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vadd.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vadd_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_i16_i16_i16
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vadd.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vadd_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_i16_i16_i16
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vadd.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vadd_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_i32_i32_i32
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vadd.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vadd_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_i32_i32_i32
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vadd.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vadd_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_i64_i64_i64
; CHECK:       vadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vadd.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vadd_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_i64_i64_i64
; CHECK:       vadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vadd.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsub.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsub_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_i8_i8_i8
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsub.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsub.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsub_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_i8_i8_i8
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsub.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsub.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsub_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_i16_i16_i16
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsub.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsub.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsub_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_i16_i16_i16
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsub.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsub.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsub_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_i32_i32_i32
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsub.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsub.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsub_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_i32_i32_i32
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsub.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsub_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_i64_i64_i64
; CHECK:       vsub.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsub.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsub_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_i64_i64_i64
; CHECK:       vsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsub.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsub.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsub_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_i8_i8_i8
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsub.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsub.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsub_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_i8_i8_i8
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsub.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsub.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsub_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_i16_i16_i16
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsub.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsub.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsub_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_i16_i16_i16
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsub.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsub.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsub_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_i32_i32_i32
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsub.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsub.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsub_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_i32_i32_i32
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsub.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsub_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_i64_i64_i64
; CHECK:       vsub.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsub.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsub_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_i64_i64_i64
; CHECK:       vsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsub.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrsub.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vrsub_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_i8_i8_i8
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrsub.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrsub.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vrsub_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_i8_i8_i8
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrsub.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrsub.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vrsub_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_i16_i16_i16
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrsub.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrsub.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vrsub_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_i16_i16_i16
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrsub.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrsub.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vrsub_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_i32_i32_i32
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrsub.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrsub.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vrsub_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_i32_i32_i32
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrsub.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrsub.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vrsub_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_i64_i64_i64
; CHECK:       vrsub.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrsub.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrsub.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vrsub_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_i64_i64_i64
; CHECK:       vrsub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrsub.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vrsub_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_i8_i8_i8
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vrsub.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vrsub_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_i8_i8_i8
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrsub.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vrsub_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_i16_i16_i16
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vrsub.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vrsub_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_i16_i16_i16
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrsub.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vrsub_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_i32_i32_i32
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vrsub.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vrsub_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_i32_i32_i32
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrsub.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vrsub_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_i64_i64_i64
; CHECK:       vrsub.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vrsub.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vrsub_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_i64_i64_i64
; CHECK:       vrsub.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrsub.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vand.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vand_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_i8_i8_i8
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vand.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vand.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vand_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_i8_i8_i8
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vand.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vand.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vand_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_i16_i16_i16
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vand.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vand.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vand_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_i16_i16_i16
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vand.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vand.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vand_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_i32_i32_i32
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vand.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vand.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vand_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_i32_i32_i32
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vand.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vand_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_i64_i64_i64
; CHECK:       vand.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vand_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_i64_i64_i64
; CHECK:       vand.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vand.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vand.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vand_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_i8_i8_i8
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vand.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vand.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vand_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_i8_i8_i8
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vand.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vand.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vand_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_i16_i16_i16
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vand.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vand.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vand_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_i16_i16_i16
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vand.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vand.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vand_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_i32_i32_i32
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vand.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vand.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vand_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_i32_i32_i32
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vand.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vand_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_i64_i64_i64
; CHECK:       vand.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vand.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vand_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_i64_i64_i64
; CHECK:       vand.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vand.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vand_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_i8_i8_i8
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vand.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vand_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_i8_i8_i8
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vand.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vand_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_i16_i16_i16
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vand.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vand_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_i16_i16_i16
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vand.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vand_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_i32_i32_i32
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vand.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vand_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_i32_i32_i32
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vand.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vand_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_i64_i64_i64
; CHECK:       vand.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vand.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vand_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_i64_i64_i64
; CHECK:       vand.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vand.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vor.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vor_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_i8_i8_i8
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vor.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vor.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vor_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_i8_i8_i8
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vor.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vor.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vor_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_i16_i16_i16
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vor.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vor.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vor_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_i16_i16_i16
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vor.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vor.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vor_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_i32_i32_i32
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vor.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vor.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vor_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_i32_i32_i32
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vor.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vor_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_i64_i64_i64
; CHECK:       vor.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vor.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vor_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_i64_i64_i64
; CHECK:       vor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vor.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vor.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vor_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_i8_i8_i8
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vor.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vor.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vor_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_i8_i8_i8
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vor.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vor.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vor_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_i16_i16_i16
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vor.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vor.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vor_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_i16_i16_i16
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vor.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vor.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vor_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_i32_i32_i32
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vor.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vor.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vor_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_i32_i32_i32
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vor.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vor_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_i64_i64_i64
; CHECK:       vor.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vor.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vor_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_i64_i64_i64
; CHECK:       vor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vor.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vor_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_i8_i8_i8
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vor.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vor_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_i8_i8_i8
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vor.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vor_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_i16_i16_i16
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vor.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vor_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_i16_i16_i16
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vor.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vor_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_i32_i32_i32
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vor.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vor_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_i32_i32_i32
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vor.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vor_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_i64_i64_i64
; CHECK:       vor.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vor.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vor_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_i64_i64_i64
; CHECK:       vor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vor.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vxor.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vxor_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_i8_i8_i8
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vxor.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vxor.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vxor_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_i8_i8_i8
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vxor.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vxor.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vxor_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_i16_i16_i16
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vxor.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vxor.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vxor_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_i16_i16_i16
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vxor.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vxor.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vxor_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_i32_i32_i32
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vxor.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vxor.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vxor_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_i32_i32_i32
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vxor.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vxor_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_i64_i64_i64
; CHECK:       vxor.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vxor.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vxor_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_i64_i64_i64
; CHECK:       vxor.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vxor.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vxor.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vxor_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_i8_i8_i8
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vxor.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vxor.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vxor_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_i8_i8_i8
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vxor.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vxor.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vxor_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_i16_i16_i16
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vxor.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vxor.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vxor_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_i16_i16_i16
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vxor.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vxor.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vxor_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_i32_i32_i32
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vxor.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vxor.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vxor_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_i32_i32_i32
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vxor.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vxor_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_i64_i64_i64
; CHECK:       vxor.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vxor.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vxor_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_i64_i64_i64
; CHECK:       vxor.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vxor.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vxor_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_i8_i8_i8
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vxor.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vxor_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_i8_i8_i8
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vxor.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vxor_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_i16_i16_i16
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vxor.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vxor_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_i16_i16_i16
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vxor.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vxor_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_i32_i32_i32
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vxor.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vxor_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_i32_i32_i32
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vxor.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vxor_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_i64_i64_i64
; CHECK:       vxor.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vxor.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vxor_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_i64_i64_i64
; CHECK:       vxor.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vxor.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsll.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsll_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_i8_i8_i8
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsll.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsll.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsll_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_i8_i8_i8
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsll.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsll.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsll_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_i16_i16_i16
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsll.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsll.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsll_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_i16_i16_i16
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsll.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsll.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsll_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_i32_i32_i32
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsll.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsll.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsll_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_i32_i32_i32
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsll.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsll_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_i64_i64_i64
; CHECK:       vsll.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsll.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsll_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_i64_i64_i64
; CHECK:       vsll.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsll.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsll.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsll_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_i8_i8_i8
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsll.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsll.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsll_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_i8_i8_i8
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsll.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsll.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsll_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_i16_i16_i16
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsll.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsll.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsll_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_i16_i16_i16
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsll.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsll.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsll_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_i32_i32_i32
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsll.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsll.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsll_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_i32_i32_i32
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsll.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsll_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_i64_i64_i64
; CHECK:       vsll.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsll.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsll_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_i64_i64_i64
; CHECK:       vsll.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsll.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vsll_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_i8_i8_i8
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vsll.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vsll_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_i8_i8_i8
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsll.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vsll_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_i16_i16_i16
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vsll.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vsll_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_i16_i16_i16
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsll.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vsll_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_i32_i32_i32
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vsll.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vsll_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_i32_i32_i32
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsll.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vsll_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_i64_i64_i64
; CHECK:       vsll.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vsll.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vsll_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_i64_i64_i64
; CHECK:       vsll.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsll.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsrl.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsrl_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_i8_i8_i8
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsrl.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsrl.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsrl_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_i8_i8_i8
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsrl.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsrl.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsrl_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_i16_i16_i16
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsrl.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsrl.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsrl_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_i16_i16_i16
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsrl.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsrl.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsrl_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_i32_i32_i32
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsrl.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsrl.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsrl_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_i32_i32_i32
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsrl.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsrl_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_i64_i64_i64
; CHECK:       vsrl.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsrl_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_i64_i64_i64
; CHECK:       vsrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsrl.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsrl.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsrl_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_i8_i8_i8
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsrl.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsrl.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsrl_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_i8_i8_i8
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsrl.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsrl.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsrl_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_i16_i16_i16
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsrl.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsrl.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsrl_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_i16_i16_i16
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsrl.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsrl.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsrl_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_i32_i32_i32
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsrl.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsrl.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsrl_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_i32_i32_i32
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsrl.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsrl_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_i64_i64_i64
; CHECK:       vsrl.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsrl.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsrl_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_i64_i64_i64
; CHECK:       vsrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsrl.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vsrl_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_i8_i8_i8
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vsrl.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vsrl_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_i8_i8_i8
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsrl.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vsrl_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_i16_i16_i16
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vsrl.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vsrl_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_i16_i16_i16
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsrl.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vsrl_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_i32_i32_i32
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vsrl.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vsrl_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_i32_i32_i32
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsrl.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vsrl_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_i64_i64_i64
; CHECK:       vsrl.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vsrl.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vsrl_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_i64_i64_i64
; CHECK:       vsrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsrl.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsra.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsra_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_i8_i8_i8
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsra.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsra.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsra_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_i8_i8_i8
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsra.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsra.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsra_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_i16_i16_i16
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsra.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsra.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsra_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_i16_i16_i16
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsra.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsra.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsra_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_i32_i32_i32
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsra.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsra.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsra_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_i32_i32_i32
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsra.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsra_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_i64_i64_i64
; CHECK:       vsra.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsra.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsra_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_i64_i64_i64
; CHECK:       vsra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsra.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsra.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsra_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_i8_i8_i8
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsra.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsra.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsra_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_i8_i8_i8
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsra.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsra.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsra_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_i16_i16_i16
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsra.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsra.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsra_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_i16_i16_i16
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsra.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsra.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsra_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_i32_i32_i32
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsra.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsra.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsra_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_i32_i32_i32
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsra.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsra_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_i64_i64_i64
; CHECK:       vsra.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsra.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsra_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_i64_i64_i64
; CHECK:       vsra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsra.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vsra_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_i8_i8_i8
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vsra.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vsra_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_i8_i8_i8
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsra.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vsra_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_i16_i16_i16
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vsra.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vsra_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_i16_i16_i16
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsra.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vsra_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_i32_i32_i32
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vsra.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vsra_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_i32_i32_i32
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsra.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vsra_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_i64_i64_i64
; CHECK:       vsra.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vsra.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vsra_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_i64_i64_i64
; CHECK:       vsra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsra.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vseq_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_i1_i8_i8
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vseq_mask_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_i1_i8_i8
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vseq_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_i1_i16_i16
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vseq_mask_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_i1_i16_i16
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vseq_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_i1_i32_i32
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vseq_mask_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_i1_i32_i32
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vseq_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vv_i1_i64_i64
; CHECK:       vseq.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vseq_mask_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vv_i1_i64_i64
; CHECK:       vseq.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vseq_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_i1_i8_i8
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vseq_mask_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_i1_i8_i8
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vseq_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_i1_i16_i16
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vseq_mask_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_i1_i16_i16
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vseq_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_i1_i32_i32
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vseq_mask_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_i1_i32_i32
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vseq.nxv1i1.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vseq_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vx_i1_i64_i64
; CHECK:       vseq.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vseq.mask.nxv1i1.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vseq_mask_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vx_i1_i64_i64
; CHECK:       vseq.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vseq_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_i1_i8_i8
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vseq_mask_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_i1_i8_i8
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vseq_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_i1_i16_i16
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vseq_mask_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_i1_i16_i16
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vseq_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_i1_i32_i32
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vseq_mask_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_i1_i32_i32
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vseq_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_vi_i1_i64_i64
; CHECK:       vseq.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vseq_mask_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vseq_mask_vi_i1_i64_i64
; CHECK:       vseq.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vseq.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsne_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_i1_i8_i8
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsne_mask_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_i1_i8_i8
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsne_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_i1_i16_i16
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsne_mask_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_i1_i16_i16
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsne_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_i1_i32_i32
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsne_mask_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_i1_i32_i32
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsne_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vv_i1_i64_i64
; CHECK:       vsne.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsne_mask_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vv_i1_i64_i64
; CHECK:       vsne.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsne_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_i1_i8_i8
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsne_mask_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_i1_i8_i8
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsne_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_i1_i16_i16
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsne_mask_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_i1_i16_i16
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsne_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_i1_i32_i32
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsne_mask_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_i1_i32_i32
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsne.nxv1i1.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsne_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vx_i1_i64_i64
; CHECK:       vsne.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsne.mask.nxv1i1.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsne_mask_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vx_i1_i64_i64
; CHECK:       vsne.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsne_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_i1_i8_i8
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsne_mask_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_i1_i8_i8
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsne_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_i1_i16_i16
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsne_mask_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_i1_i16_i16
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsne_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_i1_i32_i32
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsne_mask_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_i1_i32_i32
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsne_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_vi_i1_i64_i64
; CHECK:       vsne.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsne_mask_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsne_mask_vi_i1_i64_i64
; CHECK:       vsne.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsne.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsltu_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_i1_i8_i8
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsltu_mask_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_i1_i8_i8
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsltu_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_i1_i16_i16
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsltu_mask_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_i1_i16_i16
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsltu_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_i1_i32_i32
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsltu_mask_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_i1_i32_i32
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsltu_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vv_i1_i64_i64
; CHECK:       vsltu.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsltu_mask_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vv_i1_i64_i64
; CHECK:       vsltu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.mask.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsltu_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_i1_i8_i8
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsltu_mask_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_i1_i8_i8
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsltu_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_i1_i16_i16
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsltu_mask_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_i1_i16_i16
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsltu_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_i1_i32_i32
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsltu_mask_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_i1_i32_i32
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsltu.nxv1i1.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsltu_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_vx_i1_i64_i64
; CHECK:       vsltu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsltu.mask.nxv1i1.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsltu_mask_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsltu_mask_vx_i1_i64_i64
; CHECK:       vsltu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsltu.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vslt_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_i1_i8_i8
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vslt_mask_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_i1_i8_i8
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vslt_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_i1_i16_i16
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vslt_mask_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_i1_i16_i16
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vslt_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_i1_i32_i32
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vslt_mask_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_i1_i32_i32
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vslt_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vv_i1_i64_i64
; CHECK:       vslt.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vslt_mask_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vv_i1_i64_i64
; CHECK:       vslt.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.mask.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vslt_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_i1_i8_i8
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslt_mask_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_i1_i8_i8
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vslt_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_i1_i16_i16
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslt_mask_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_i1_i16_i16
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vslt_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_i1_i32_i32
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslt_mask_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_i1_i32_i32
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vslt.nxv1i1.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vslt_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_vx_i1_i64_i64
; CHECK:       vslt.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vslt.mask.nxv1i1.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslt_mask_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslt_mask_vx_i1_i64_i64
; CHECK:       vslt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vslt.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsleu_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_i1_i8_i8
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsleu_mask_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_i1_i8_i8
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsleu_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_i1_i16_i16
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsleu_mask_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_i1_i16_i16
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsleu_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_i1_i32_i32
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsleu_mask_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_i1_i32_i32
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsleu_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vv_i1_i64_i64
; CHECK:       vsleu.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsleu_mask_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vv_i1_i64_i64
; CHECK:       vsleu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsleu_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_i1_i8_i8
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsleu_mask_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_i1_i8_i8
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsleu_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_i1_i16_i16
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsleu_mask_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_i1_i16_i16
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsleu_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_i1_i32_i32
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsleu_mask_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_i1_i32_i32
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsleu.nxv1i1.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsleu_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vx_i1_i64_i64
; CHECK:       vsleu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsleu.mask.nxv1i1.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsleu_mask_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vx_i1_i64_i64
; CHECK:       vsleu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsleu_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_i1_i8_i8
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsleu_mask_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_i1_i8_i8
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsleu_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_i1_i16_i16
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsleu_mask_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_i1_i16_i16
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsleu_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_i1_i32_i32
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsleu_mask_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_i1_i32_i32
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsleu_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_vi_i1_i64_i64
; CHECK:       vsleu.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsleu_mask_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsleu_mask_vi_i1_i64_i64
; CHECK:       vsleu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsleu.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsle_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_i1_i8_i8
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsle_mask_vv_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_i1_i8_i8
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsle_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_i1_i16_i16
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsle_mask_vv_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_i1_i16_i16
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsle_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_i1_i32_i32
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsle_mask_vv_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_i1_i32_i32
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsle_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vv_i1_i64_i64
; CHECK:       vsle.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsle_mask_vv_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vv_i1_i64_i64
; CHECK:       vsle.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsle_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_i1_i8_i8
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsle_mask_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_i1_i8_i8
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsle_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_i1_i16_i16
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsle_mask_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_i1_i16_i16
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsle_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_i1_i32_i32
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsle_mask_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_i1_i32_i32
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsle.nxv1i1.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsle_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vx_i1_i64_i64
; CHECK:       vsle.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsle.mask.nxv1i1.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsle_mask_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vx_i1_i64_i64
; CHECK:       vsle.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsle_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_i1_i8_i8
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsle_mask_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_i1_i8_i8
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsle_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_i1_i16_i16
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsle_mask_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_i1_i16_i16
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsle_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_i1_i32_i32
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsle_mask_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_i1_i32_i32
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsle_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_vi_i1_i64_i64
; CHECK:       vsle.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsle_mask_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsle_mask_vi_i1_i64_i64
; CHECK:       vsle.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsle.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgtu.nxv1i1.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsgtu_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_i1_i8_i8
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgtu.mask.nxv1i1.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsgtu_mask_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_i1_i8_i8
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgtu.nxv1i1.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsgtu_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_i1_i16_i16
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgtu.mask.nxv1i1.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsgtu_mask_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_i1_i16_i16
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgtu.nxv1i1.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsgtu_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_i1_i32_i32
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgtu.mask.nxv1i1.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsgtu_mask_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_i1_i32_i32
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgtu.nxv1i1.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsgtu_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vx_i1_i64_i64
; CHECK:       vsgtu.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgtu.mask.nxv1i1.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsgtu_mask_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vx_i1_i64_i64
; CHECK:       vsgtu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsgtu_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_i1_i8_i8
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsgtu_mask_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_i1_i8_i8
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsgtu_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_i1_i16_i16
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsgtu_mask_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_i1_i16_i16
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsgtu_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_i1_i32_i32
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsgtu_mask_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_i1_i32_i32
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsgtu_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_vi_i1_i64_i64
; CHECK:       vsgtu.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsgtu_mask_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgtu_mask_vi_i1_i64_i64
; CHECK:       vsgtu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgtu.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgt.nxv1i1.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsgt_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_i1_i8_i8
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgt.mask.nxv1i1.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsgt_mask_vx_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_i1_i8_i8
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgt.nxv1i1.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsgt_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_i1_i16_i16
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgt.mask.nxv1i1.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsgt_mask_vx_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_i1_i16_i16
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgt.nxv1i1.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsgt_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_i1_i32_i32
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgt.mask.nxv1i1.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsgt_mask_vx_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_i1_i32_i32
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vsgt.nxv1i1.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsgt_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vx_i1_i64_i64
; CHECK:       vsgt.vx v0, v0, a0
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vsgt.mask.nxv1i1.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsgt_mask_vx_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vx_i1_i64_i64
; CHECK:       vsgt.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsgt_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_i1_i8_i8
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsgt_mask_vi_i1_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_i1_i8_i8
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.mask.nxv1i1.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsgt_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_i1_i16_i16
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsgt_mask_vi_i1_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_i1_i16_i16
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.mask.nxv1i1.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsgt_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_i1_i32_i32
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsgt_mask_vi_i1_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_i1_i32_i32
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.mask.nxv1i1.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


define void @intrinsic_vsgt_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_vi_i1_i64_i64
; CHECK:       vsgt.vi v0, v0, 9
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

define void @intrinsic_vsgt_mask_vi_i1_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsgt_mask_vi_i1_i64_i64
; CHECK:       vsgt.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vsgt.mask.nxv1i1.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vminu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vminu_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_i8_i8_i8
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vminu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vminu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vminu_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_i8_i8_i8
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vminu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vminu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vminu_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_i16_i16_i16
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vminu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vminu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vminu_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_i16_i16_i16
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vminu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vminu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vminu_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_i32_i32_i32
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vminu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vminu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vminu_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_i32_i32_i32
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vminu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vminu_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_i64_i64_i64
; CHECK:       vminu.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vminu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vminu_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_i64_i64_i64
; CHECK:       vminu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vminu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vminu.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vminu_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_i8_i8_i8
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vminu.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vminu.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vminu_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_i8_i8_i8
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vminu.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vminu.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vminu_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_i16_i16_i16
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vminu.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vminu.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vminu_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_i16_i16_i16
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vminu.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vminu.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vminu_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_i32_i32_i32
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vminu.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vminu.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vminu_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_i32_i32_i32
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vminu.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vminu_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_i64_i64_i64
; CHECK:       vminu.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vminu.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vminu_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_i64_i64_i64
; CHECK:       vminu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vminu.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmin.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmin_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_i8_i8_i8
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmin.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmin.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmin_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_i8_i8_i8
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmin.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmin.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmin_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_i16_i16_i16
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmin.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmin.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmin_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_i16_i16_i16
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmin.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmin.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmin_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_i32_i32_i32
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmin.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmin.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmin_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_i32_i32_i32
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmin.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmin_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_i64_i64_i64
; CHECK:       vmin.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmin.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmin_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_i64_i64_i64
; CHECK:       vmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmin.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmin.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vmin_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_i8_i8_i8
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmin.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmin.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vmin_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_i8_i8_i8
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmin.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmin.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vmin_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_i16_i16_i16
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmin.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmin.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vmin_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_i16_i16_i16
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmin.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmin.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vmin_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_i32_i32_i32
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmin.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmin.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vmin_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_i32_i32_i32
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmin.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vmin_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_i64_i64_i64
; CHECK:       vmin.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmin.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vmin_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_i64_i64_i64
; CHECK:       vmin.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmin.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmaxu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmaxu_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_i8_i8_i8
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmaxu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmaxu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmaxu_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_i8_i8_i8
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmaxu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmaxu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmaxu_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_i16_i16_i16
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmaxu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmaxu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmaxu_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_i16_i16_i16
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmaxu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmaxu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmaxu_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_i32_i32_i32
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmaxu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmaxu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmaxu_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_i32_i32_i32
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmaxu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmaxu_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_i64_i64_i64
; CHECK:       vmaxu.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmaxu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmaxu_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_i64_i64_i64
; CHECK:       vmaxu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmaxu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmaxu.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vmaxu_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_i8_i8_i8
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmaxu.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmaxu.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vmaxu_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_i8_i8_i8
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmaxu.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmaxu.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vmaxu_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_i16_i16_i16
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmaxu.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmaxu.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vmaxu_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_i16_i16_i16
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmaxu.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmaxu.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vmaxu_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_i32_i32_i32
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmaxu.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmaxu.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vmaxu_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_i32_i32_i32
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmaxu.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vmaxu_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_i64_i64_i64
; CHECK:       vmaxu.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmaxu.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vmaxu_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_i64_i64_i64
; CHECK:       vmaxu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmaxu.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmax.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmax_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_i8_i8_i8
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmax.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmax.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmax_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_i8_i8_i8
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmax.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmax.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmax_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_i16_i16_i16
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmax.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmax.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmax_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_i16_i16_i16
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmax.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmax.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmax_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_i32_i32_i32
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmax.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmax.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmax_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_i32_i32_i32
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmax.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmax_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_i64_i64_i64
; CHECK:       vmax.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmax.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmax_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_i64_i64_i64
; CHECK:       vmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmax.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmax.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vmax_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_i8_i8_i8
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmax.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmax.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vmax_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_i8_i8_i8
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmax.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmax.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vmax_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_i16_i16_i16
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmax.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmax.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vmax_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_i16_i16_i16
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmax.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmax.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vmax_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_i32_i32_i32
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmax.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmax.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vmax_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_i32_i32_i32
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmax.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vmax_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_i64_i64_i64
; CHECK:       vmax.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmax.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vmax_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_i64_i64_i64
; CHECK:       vmax.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmax.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmul.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmul_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_i8_i8_i8
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmul.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmul.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmul_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_i8_i8_i8
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmul.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmul.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmul_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_i16_i16_i16
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmul.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmul.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmul_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_i16_i16_i16
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmul.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmul.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmul_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_i32_i32_i32
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmul.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmul.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmul_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_i32_i32_i32
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmul.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmul_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_i64_i64_i64
; CHECK:       vmul.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmul_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_i64_i64_i64
; CHECK:       vmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmul.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmul.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vmul_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_i8_i8_i8
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmul.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmul.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vmul_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_i8_i8_i8
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmul.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmul.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vmul_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_i16_i16_i16
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmul.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmul.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vmul_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_i16_i16_i16
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmul.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmul.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vmul_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_i32_i32_i32
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmul.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmul.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vmul_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_i32_i32_i32
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmul.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vmul_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_i64_i64_i64
; CHECK:       vmul.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmul.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vmul_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_i64_i64_i64
; CHECK:       vmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmul.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmulh.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmulh_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_i8_i8_i8
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulh.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmulh.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmulh_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_i8_i8_i8
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulh.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmulh.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmulh_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_i16_i16_i16
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulh.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmulh.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmulh_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_i16_i16_i16
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulh.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmulh.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmulh_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_i32_i32_i32
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulh.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmulh.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmulh_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_i32_i32_i32
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulh.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmulh_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_i64_i64_i64
; CHECK:       vmulh.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulh.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmulh_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_i64_i64_i64
; CHECK:       vmulh.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulh.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmulh.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vmulh_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_i8_i8_i8
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulh.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmulh.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vmulh_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_i8_i8_i8
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulh.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmulh.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vmulh_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_i16_i16_i16
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulh.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmulh.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vmulh_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_i16_i16_i16
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulh.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmulh.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vmulh_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_i32_i32_i32
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulh.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmulh.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vmulh_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_i32_i32_i32
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulh.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vmulh_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_i64_i64_i64
; CHECK:       vmulh.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulh.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vmulh_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_i64_i64_i64
; CHECK:       vmulh.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulh.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmulhu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmulhu_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_i8_i8_i8
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulhu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmulhu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmulhu_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_i8_i8_i8
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulhu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmulhu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmulhu_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_i16_i16_i16
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulhu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmulhu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmulhu_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_i16_i16_i16
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulhu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmulhu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmulhu_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_i32_i32_i32
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulhu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmulhu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmulhu_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_i32_i32_i32
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulhu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmulhu_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_i64_i64_i64
; CHECK:       vmulhu.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulhu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmulhu_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_i64_i64_i64
; CHECK:       vmulhu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulhu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmulhu.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vmulhu_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_i8_i8_i8
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulhu.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmulhu.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vmulhu_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_i8_i8_i8
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulhu.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmulhu.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vmulhu_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_i16_i16_i16
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulhu.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmulhu.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vmulhu_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_i16_i16_i16
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulhu.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmulhu.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vmulhu_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_i32_i32_i32
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulhu.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmulhu.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vmulhu_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_i32_i32_i32
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulhu.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vmulhu_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_i64_i64_i64
; CHECK:       vmulhu.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulhu.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vmulhu_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_i64_i64_i64
; CHECK:       vmulhu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulhu.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmulhsu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmulhsu_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_i8_i8_i8
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulhsu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmulhsu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmulhsu_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_i8_i8_i8
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulhsu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmulhsu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmulhsu_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_i16_i16_i16
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulhsu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmulhsu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmulhsu_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_i16_i16_i16
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulhsu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmulhsu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmulhsu_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_i32_i32_i32
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulhsu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmulhsu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmulhsu_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_i32_i32_i32
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulhsu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmulhsu_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_i64_i64_i64
; CHECK:       vmulhsu.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulhsu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmulhsu_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_i64_i64_i64
; CHECK:       vmulhsu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulhsu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmulhsu.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vmulhsu_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_i8_i8_i8
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulhsu.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmulhsu.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vmulhsu_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_i8_i8_i8
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmulhsu.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmulhsu.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vmulhsu_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_i16_i16_i16
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulhsu.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmulhsu.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vmulhsu_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_i16_i16_i16
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmulhsu.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmulhsu.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vmulhsu_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_i32_i32_i32
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulhsu.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmulhsu.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vmulhsu_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_i32_i32_i32
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmulhsu.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vmulhsu_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_i64_i64_i64
; CHECK:       vmulhsu.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulhsu.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vmulhsu_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_i64_i64_i64
; CHECK:       vmulhsu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmulhsu.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vdivu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vdivu_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_i8_i8_i8
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vdivu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vdivu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vdivu_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_i8_i8_i8
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vdivu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vdivu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vdivu_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_i16_i16_i16
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vdivu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vdivu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vdivu_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_i16_i16_i16
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vdivu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vdivu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vdivu_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_i32_i32_i32
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vdivu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vdivu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vdivu_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_i32_i32_i32
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vdivu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vdivu_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_i64_i64_i64
; CHECK:       vdivu.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vdivu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vdivu_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_i64_i64_i64
; CHECK:       vdivu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vdivu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vdivu.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vdivu_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_i8_i8_i8
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vdivu.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vdivu.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vdivu_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_i8_i8_i8
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vdivu.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vdivu.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vdivu_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_i16_i16_i16
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vdivu.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vdivu.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vdivu_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_i16_i16_i16
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vdivu.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vdivu.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vdivu_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_i32_i32_i32
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vdivu.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vdivu.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vdivu_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_i32_i32_i32
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vdivu.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vdivu_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_i64_i64_i64
; CHECK:       vdivu.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vdivu.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vdivu_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_i64_i64_i64
; CHECK:       vdivu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vdivu.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vdiv.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vdiv_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_i8_i8_i8
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vdiv.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vdiv.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vdiv_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_i8_i8_i8
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vdiv.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vdiv.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vdiv_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_i16_i16_i16
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vdiv.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vdiv.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vdiv_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_i16_i16_i16
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vdiv.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vdiv.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vdiv_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_i32_i32_i32
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vdiv.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vdiv.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vdiv_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_i32_i32_i32
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vdiv.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vdiv_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_i64_i64_i64
; CHECK:       vdiv.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vdiv.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vdiv_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_i64_i64_i64
; CHECK:       vdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vdiv.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vdiv.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vdiv_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_i8_i8_i8
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vdiv.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vdiv.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vdiv_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_i8_i8_i8
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vdiv.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vdiv.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vdiv_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_i16_i16_i16
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vdiv.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vdiv.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vdiv_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_i16_i16_i16
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vdiv.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vdiv.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vdiv_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_i32_i32_i32
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vdiv.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vdiv.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vdiv_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_i32_i32_i32
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vdiv.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vdiv_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_i64_i64_i64
; CHECK:       vdiv.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vdiv.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vdiv_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_i64_i64_i64
; CHECK:       vdiv.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vdiv.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vremu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vremu_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_i8_i8_i8
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vremu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vremu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vremu_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_i8_i8_i8
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vremu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vremu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vremu_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_i16_i16_i16
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vremu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vremu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vremu_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_i16_i16_i16
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vremu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vremu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vremu_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_i32_i32_i32
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vremu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vremu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vremu_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_i32_i32_i32
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vremu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vremu_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_i64_i64_i64
; CHECK:       vremu.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vremu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vremu_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_i64_i64_i64
; CHECK:       vremu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vremu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vremu.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vremu_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_i8_i8_i8
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vremu.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vremu.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vremu_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_i8_i8_i8
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vremu.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vremu.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vremu_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_i16_i16_i16
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vremu.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vremu.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vremu_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_i16_i16_i16
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vremu.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vremu.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vremu_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_i32_i32_i32
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vremu.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vremu.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vremu_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_i32_i32_i32
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vremu.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vremu_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_i64_i64_i64
; CHECK:       vremu.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vremu.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vremu_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_i64_i64_i64
; CHECK:       vremu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vremu.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrem.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vrem_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_i8_i8_i8
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrem.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrem.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vrem_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_i8_i8_i8
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrem.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrem.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vrem_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_i16_i16_i16
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrem.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrem.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vrem_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_i16_i16_i16
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrem.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrem.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vrem_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_i32_i32_i32
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrem.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrem.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vrem_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_i32_i32_i32
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrem.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vrem_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_i64_i64_i64
; CHECK:       vrem.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrem.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vrem_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_i64_i64_i64
; CHECK:       vrem.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrem.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrem.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vrem_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_i8_i8_i8
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrem.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrem.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vrem_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_i8_i8_i8
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrem.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrem.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vrem_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_i16_i16_i16
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrem.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrem.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vrem_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_i16_i16_i16
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrem.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrem.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vrem_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_i32_i32_i32
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrem.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrem.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vrem_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_i32_i32_i32
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrem.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vrem_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_i64_i64_i64
; CHECK:       vrem.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrem.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vrem_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_i64_i64_i64
; CHECK:       vrem.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrem.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmerge.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmerge_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_i8_i8_i8
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmerge.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmerge.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmerge_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_i8_i8_i8
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmerge.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmerge.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmerge_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_i16_i16_i16
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmerge.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmerge.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmerge_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_i16_i16_i16
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmerge.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmerge.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmerge_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_i32_i32_i32
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmerge.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmerge.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmerge_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_i32_i32_i32
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmerge.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmerge_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vv_i64_i64_i64
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmerge.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmerge.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmerge_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vv_i64_i64_i64
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmerge.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmerge.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vmerge_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_i8_i8_i8
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmerge.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmerge.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vmerge_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_i8_i8_i8
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmerge.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmerge.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vmerge_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_i16_i16_i16
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmerge.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmerge.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vmerge_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_i16_i16_i16
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmerge.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmerge.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vmerge_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_i32_i32_i32
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmerge.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmerge.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vmerge_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_i32_i32_i32
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmerge.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vmerge_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vx_i64_i64_i64
; CHECK:       vmerge.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmerge.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmerge.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vmerge_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vx_i64_i64_i64
; CHECK:       vmerge.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmerge.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vmerge_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_i8_i8_i8
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vmerge.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vmerge_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_i8_i8_i8
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmerge.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vmerge_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_i16_i16_i16
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vmerge.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vmerge_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_i16_i16_i16
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmerge.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vmerge_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_i32_i32_i32
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vmerge.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vmerge_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_i32_i32_i32
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmerge.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vmerge_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vi_i64_i64_i64
; CHECK:       vmerge.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vmerge.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vmerge_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_mask_vi_i64_i64_i64
; CHECK:       vmerge.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmerge.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsaddu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsaddu_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_i8_i8_i8
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsaddu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsaddu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsaddu_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_i8_i8_i8
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsaddu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsaddu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsaddu_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_i16_i16_i16
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsaddu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsaddu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsaddu_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_i16_i16_i16
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsaddu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsaddu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsaddu_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_i32_i32_i32
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsaddu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsaddu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsaddu_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_i32_i32_i32
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsaddu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsaddu_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_i64_i64_i64
; CHECK:       vsaddu.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsaddu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsaddu_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_i64_i64_i64
; CHECK:       vsaddu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsaddu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsaddu.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsaddu_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_i8_i8_i8
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsaddu.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsaddu.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsaddu_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_i8_i8_i8
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsaddu.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsaddu.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsaddu_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_i16_i16_i16
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsaddu.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsaddu.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsaddu_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_i16_i16_i16
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsaddu.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsaddu.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsaddu_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_i32_i32_i32
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsaddu.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsaddu.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsaddu_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_i32_i32_i32
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsaddu.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsaddu_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_i64_i64_i64
; CHECK:       vsaddu.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsaddu.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsaddu_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_i64_i64_i64
; CHECK:       vsaddu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsaddu.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vsaddu_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_i8_i8_i8
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vsaddu.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vsaddu_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_i8_i8_i8
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsaddu.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vsaddu_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_i16_i16_i16
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vsaddu.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vsaddu_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_i16_i16_i16
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsaddu.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vsaddu_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_i32_i32_i32
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vsaddu.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vsaddu_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_i32_i32_i32
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsaddu.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vsaddu_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_i64_i64_i64
; CHECK:       vsaddu.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vsaddu.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vsaddu_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_i64_i64_i64
; CHECK:       vsaddu.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsaddu.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsadd.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsadd_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_i8_i8_i8
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsadd.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsadd.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsadd_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_i8_i8_i8
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsadd.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsadd.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsadd_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_i16_i16_i16
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsadd.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsadd.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsadd_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_i16_i16_i16
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsadd.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsadd.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsadd_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_i32_i32_i32
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsadd.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsadd.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsadd_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_i32_i32_i32
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsadd.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsadd_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_i64_i64_i64
; CHECK:       vsadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsadd.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsadd_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_i64_i64_i64
; CHECK:       vsadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsadd.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsadd.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsadd_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_i8_i8_i8
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsadd.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsadd.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsadd_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_i8_i8_i8
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsadd.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsadd.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsadd_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_i16_i16_i16
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsadd.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsadd.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsadd_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_i16_i16_i16
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsadd.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsadd.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsadd_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_i32_i32_i32
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsadd.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsadd.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsadd_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_i32_i32_i32
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsadd.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsadd_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_i64_i64_i64
; CHECK:       vsadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsadd.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsadd_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_i64_i64_i64
; CHECK:       vsadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsadd.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vsadd_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_i8_i8_i8
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vsadd.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vsadd_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_i8_i8_i8
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsadd.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vsadd_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_i16_i16_i16
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vsadd.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vsadd_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_i16_i16_i16
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsadd.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vsadd_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_i32_i32_i32
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vsadd.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vsadd_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_i32_i32_i32
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsadd.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vsadd_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_i64_i64_i64
; CHECK:       vsadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vsadd.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vsadd_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_i64_i64_i64
; CHECK:       vsadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsadd.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vssubu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vssubu_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_i8_i8_i8
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vssubu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vssubu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vssubu_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_i8_i8_i8
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssubu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vssubu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vssubu_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_i16_i16_i16
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vssubu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vssubu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vssubu_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_i16_i16_i16
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssubu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vssubu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vssubu_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_i32_i32_i32
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vssubu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vssubu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vssubu_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_i32_i32_i32
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssubu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vssubu_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_i64_i64_i64
; CHECK:       vssubu.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vssubu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vssubu_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_i64_i64_i64
; CHECK:       vssubu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssubu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vssubu.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vssubu_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_i8_i8_i8
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vssubu.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vssubu.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vssubu_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_i8_i8_i8
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssubu.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vssubu.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vssubu_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_i16_i16_i16
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vssubu.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vssubu.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vssubu_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_i16_i16_i16
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssubu.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vssubu.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vssubu_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_i32_i32_i32
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vssubu.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vssubu.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vssubu_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_i32_i32_i32
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssubu.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vssubu_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_i64_i64_i64
; CHECK:       vssubu.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vssubu.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vssubu_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_i64_i64_i64
; CHECK:       vssubu.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssubu.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vssub.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vssub_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_i8_i8_i8
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vssub.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vssub.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vssub_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_i8_i8_i8
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssub.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vssub.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vssub_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_i16_i16_i16
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vssub.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vssub.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vssub_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_i16_i16_i16
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssub.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vssub.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vssub_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_i32_i32_i32
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vssub.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vssub.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vssub_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_i32_i32_i32
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssub.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vssub_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_i64_i64_i64
; CHECK:       vssub.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vssub.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vssub_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_i64_i64_i64
; CHECK:       vssub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssub.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vssub.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vssub_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_i8_i8_i8
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vssub.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vssub.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vssub_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_i8_i8_i8
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssub.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vssub.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vssub_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_i16_i16_i16
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vssub.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vssub.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vssub_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_i16_i16_i16
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssub.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vssub.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vssub_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_i32_i32_i32
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vssub.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vssub.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vssub_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_i32_i32_i32
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssub.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vssub_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_i64_i64_i64
; CHECK:       vssub.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vssub.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vssub_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_i64_i64_i64
; CHECK:       vssub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssub.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vaadd.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vaadd_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_i8_i8_i8
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vaadd.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vaadd.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vaadd_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_i8_i8_i8
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vaadd.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vaadd.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vaadd_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_i16_i16_i16
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vaadd.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vaadd.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vaadd_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_i16_i16_i16
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vaadd.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vaadd.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vaadd_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_i32_i32_i32
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vaadd.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vaadd.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vaadd_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_i32_i32_i32
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vaadd.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vaadd_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_i64_i64_i64
; CHECK:       vaadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vaadd.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vaadd_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_i64_i64_i64
; CHECK:       vaadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vaadd.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vaadd.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vaadd_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_i8_i8_i8
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vaadd.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vaadd.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vaadd_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_i8_i8_i8
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vaadd.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vaadd.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vaadd_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_i16_i16_i16
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vaadd.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vaadd.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vaadd_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_i16_i16_i16
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vaadd.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vaadd.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vaadd_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_i32_i32_i32
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vaadd.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vaadd.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vaadd_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_i32_i32_i32
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vaadd.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vaadd_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_i64_i64_i64
; CHECK:       vaadd.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vaadd.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vaadd_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_i64_i64_i64
; CHECK:       vaadd.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vaadd.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vaadd_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_i8_i8_i8
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vaadd.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vaadd_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_i8_i8_i8
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vaadd.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vaadd_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_i16_i16_i16
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vaadd.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vaadd_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_i16_i16_i16
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vaadd.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vaadd_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_i32_i32_i32
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vaadd.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vaadd_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_i32_i32_i32
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vaadd.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vaadd_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vi_i64_i64_i64
; CHECK:       vaadd.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vaadd.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vaadd_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vi_i64_i64_i64
; CHECK:       vaadd.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vaadd.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vasub.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vasub_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_i8_i8_i8
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vasub.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vasub.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vasub_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_i8_i8_i8
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vasub.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vasub.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vasub_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_i16_i16_i16
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vasub.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vasub.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vasub_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_i16_i16_i16
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vasub.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vasub.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vasub_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_i32_i32_i32
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vasub.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vasub.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vasub_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_i32_i32_i32
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vasub.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vasub_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_i64_i64_i64
; CHECK:       vasub.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vasub.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vasub_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_i64_i64_i64
; CHECK:       vasub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vasub.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vasub.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vasub_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_i8_i8_i8
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vasub.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vasub.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vasub_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_i8_i8_i8
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vasub.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vasub.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vasub_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_i16_i16_i16
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vasub.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vasub.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vasub_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_i16_i16_i16
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vasub.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vasub.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vasub_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_i32_i32_i32
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vasub.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vasub.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vasub_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_i32_i32_i32
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vasub.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vasub_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_i64_i64_i64
; CHECK:       vasub.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vasub.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vasub_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_i64_i64_i64
; CHECK:       vasub.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vasub.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsmul.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsmul_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_i8_i8_i8
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsmul.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsmul.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vsmul_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_i8_i8_i8
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsmul.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsmul.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsmul_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_i16_i16_i16
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsmul.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsmul.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vsmul_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_i16_i16_i16
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsmul.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsmul.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsmul_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_i32_i32_i32
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsmul.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsmul.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vsmul_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_i32_i32_i32
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsmul.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsmul_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_i64_i64_i64
; CHECK:       vsmul.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsmul.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vsmul_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_i64_i64_i64
; CHECK:       vsmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsmul.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsmul.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vsmul_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_i8_i8_i8
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsmul.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vsmul.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vsmul_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_i8_i8_i8
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vsmul.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsmul.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vsmul_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_i16_i16_i16
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsmul.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vsmul.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vsmul_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_i16_i16_i16
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vsmul.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsmul.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vsmul_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_i32_i32_i32
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsmul.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vsmul.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vsmul_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_i32_i32_i32
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vsmul.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vsmul_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_i64_i64_i64
; CHECK:       vsmul.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsmul.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vsmul_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_i64_i64_i64
; CHECK:       vsmul.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vsmul.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vssrl.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vssrl_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_i8_i8_i8
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vssrl.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vssrl.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vssrl_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_i8_i8_i8
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssrl.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vssrl.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vssrl_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_i16_i16_i16
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vssrl.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vssrl.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vssrl_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_i16_i16_i16
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssrl.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vssrl.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vssrl_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_i32_i32_i32
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vssrl.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vssrl.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vssrl_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_i32_i32_i32
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssrl.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vssrl_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_i64_i64_i64
; CHECK:       vssrl.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vssrl.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vssrl_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_i64_i64_i64
; CHECK:       vssrl.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssrl.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vssrl.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vssrl_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_i8_i8_i8
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vssrl.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vssrl.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vssrl_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_i8_i8_i8
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssrl.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vssrl.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vssrl_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_i16_i16_i16
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vssrl.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vssrl.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vssrl_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_i16_i16_i16
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssrl.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vssrl.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vssrl_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_i32_i32_i32
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vssrl.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vssrl.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vssrl_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_i32_i32_i32
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssrl.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vssrl_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_i64_i64_i64
; CHECK:       vssrl.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vssrl.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vssrl_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_i64_i64_i64
; CHECK:       vssrl.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssrl.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vssrl_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_i8_i8_i8
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vssrl.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vssrl_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_i8_i8_i8
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssrl.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vssrl_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_i16_i16_i16
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vssrl.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vssrl_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_i16_i16_i16
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssrl.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vssrl_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_i32_i32_i32
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vssrl.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vssrl_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_i32_i32_i32
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssrl.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vssrl_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_i64_i64_i64
; CHECK:       vssrl.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vssrl.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vssrl_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_i64_i64_i64
; CHECK:       vssrl.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssrl.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vssra.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vssra_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_i8_i8_i8
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vssra.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vssra.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vssra_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_i8_i8_i8
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssra.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vssra.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vssra_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_i16_i16_i16
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vssra.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vssra.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vssra_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_i16_i16_i16
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssra.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vssra.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vssra_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_i32_i32_i32
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vssra.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vssra.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vssra_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_i32_i32_i32
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssra.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vssra_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_i64_i64_i64
; CHECK:       vssra.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vssra.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vssra_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_i64_i64_i64
; CHECK:       vssra.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssra.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vssra.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vssra_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_i8_i8_i8
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vssra.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vssra.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vssra_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_i8_i8_i8
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssra.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vssra.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vssra_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_i16_i16_i16
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vssra.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vssra.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vssra_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_i16_i16_i16
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssra.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vssra.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vssra_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_i32_i32_i32
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vssra.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vssra.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vssra_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_i32_i32_i32
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssra.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vssra_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_i64_i64_i64
; CHECK:       vssra.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vssra.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vssra_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_i64_i64_i64
; CHECK:       vssra.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssra.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vssra_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_i8_i8_i8
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vssra.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vssra_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_i8_i8_i8
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vssra.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vssra_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_i16_i16_i16
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vssra.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vssra_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_i16_i16_i16
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vssra.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vssra_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_i32_i32_i32
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vssra.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vssra_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_i32_i32_i32
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vssra.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vssra_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_i64_i64_i64
; CHECK:       vssra.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vssra.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vssra_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_i64_i64_i64
; CHECK:       vssra.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vssra.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfadd.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfadd_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_f32_f32_f32
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfadd.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfadd.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfadd_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_f32_f32_f32
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfadd.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfadd_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_f64_f64_f64
; CHECK:       vfadd.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfadd_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_f64_f64_f64
; CHECK:       vfadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfadd.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfadd_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_f32_f32_f32
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfadd.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfadd.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfadd_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_f32_f32_f32
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfadd.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfadd_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_f64_f64_f64
; CHECK:       vfadd.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfadd.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfadd_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_f64_f64_f64
; CHECK:       vfadd.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfadd.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfsub.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfsub_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_f32_f32_f32
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfsub.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfsub.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfsub_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_f32_f32_f32
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfsub.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfsub_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_f64_f64_f64
; CHECK:       vfsub.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfsub_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_f64_f64_f64
; CHECK:       vfsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfsub.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfsub.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfsub_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_f32_f32_f32
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfsub.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfsub.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfsub_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_f32_f32_f32
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfsub.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfsub_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_f64_f64_f64
; CHECK:       vfsub.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfsub.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfsub_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_f64_f64_f64
; CHECK:       vfsub.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfsub.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmul.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfmul_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_f32_f32_f32
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmul.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmul.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmul_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_f32_f32_f32
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmul.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfmul_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_f64_f64_f64
; CHECK:       vfmul.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmul_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_f64_f64_f64
; CHECK:       vfmul.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmul.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmul.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfmul_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_f32_f32_f32
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmul.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmul.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfmul_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_f32_f32_f32
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmul.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfmul_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_f64_f64_f64
; CHECK:       vfmul.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmul.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfmul_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_f64_f64_f64
; CHECK:       vfmul.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmul.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfdiv.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfdiv_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_f32_f32_f32
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfdiv.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfdiv.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfdiv_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_f32_f32_f32
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfdiv.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfdiv_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_f64_f64_f64
; CHECK:       vfdiv.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfdiv.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfdiv_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_f64_f64_f64
; CHECK:       vfdiv.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfdiv.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfdiv.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfdiv_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_f32_f32_f32
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfdiv.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfdiv.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfdiv_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_f32_f32_f32
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfdiv.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfdiv_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_f64_f64_f64
; CHECK:       vfdiv.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfdiv.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfdiv_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_f64_f64_f64
; CHECK:       vfdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfdiv.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfrdiv.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfrdiv_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_f32_f32_f32
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfrdiv.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfrdiv.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfrdiv_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_f32_f32_f32
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfrdiv.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfrdiv.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfrdiv_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_f64_f64_f64
; CHECK:       vfrdiv.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfrdiv.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfrdiv.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfrdiv_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_f64_f64_f64
; CHECK:       vfrdiv.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfrdiv.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmin.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfmin_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_f32_f32_f32
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmin.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmin.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmin_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_f32_f32_f32
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmin.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfmin_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_f64_f64_f64
; CHECK:       vfmin.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmin.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmin_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_f64_f64_f64
; CHECK:       vfmin.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmin.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmin.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfmin_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_f32_f32_f32
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmin.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmin.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfmin_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_f32_f32_f32
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmin.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfmin_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_f64_f64_f64
; CHECK:       vfmin.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmin.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfmin_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_f64_f64_f64
; CHECK:       vfmin.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmin.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmax.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfmax_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_f32_f32_f32
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmax.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmax.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmax_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_f32_f32_f32
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmax.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfmax_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_f64_f64_f64
; CHECK:       vfmax.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmax.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmax_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_f64_f64_f64
; CHECK:       vfmax.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmax.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmax.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfmax_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_f32_f32_f32
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmax.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmax.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfmax_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_f32_f32_f32
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmax.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfmax_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_f64_f64_f64
; CHECK:       vfmax.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmax.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfmax_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_f64_f64_f64
; CHECK:       vfmax.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmax.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfsgnj.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfsgnj_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_f32_f32_f32
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnj.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfsgnj.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfsgnj_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_f32_f32_f32
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnj.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfsgnj_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_f64_f64_f64
; CHECK:       vfsgnj.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnj.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfsgnj_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_f64_f64_f64
; CHECK:       vfsgnj.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnj.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfsgnj.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfsgnj_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_f32_f32_f32
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnj.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfsgnj.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfsgnj_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_f32_f32_f32
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnj.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfsgnj_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_f64_f64_f64
; CHECK:       vfsgnj.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnj.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfsgnj_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_f64_f64_f64
; CHECK:       vfsgnj.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnj.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfsgnjn.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfsgnjn_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_f32_f32_f32
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnjn.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfsgnjn.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfsgnjn_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_f32_f32_f32
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnjn.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfsgnjn_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_f64_f64_f64
; CHECK:       vfsgnjn.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfsgnjn_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_f64_f64_f64
; CHECK:       vfsgnjn.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfsgnjn.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfsgnjn_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_f32_f32_f32
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnjn.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfsgnjn.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfsgnjn_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_f32_f32_f32
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnjn.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfsgnjn_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_f64_f64_f64
; CHECK:       vfsgnjn.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnjn.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfsgnjn_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_f64_f64_f64
; CHECK:       vfsgnjn.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnjn.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfsgnjx.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfsgnjx_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_f32_f32_f32
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnjx.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfsgnjx.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfsgnjx_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_f32_f32_f32
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnjx.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfsgnjx_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_f64_f64_f64
; CHECK:       vfsgnjx.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnjx.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfsgnjx_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_f64_f64_f64
; CHECK:       vfsgnjx.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnjx.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfsgnjx.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfsgnjx_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_f32_f32_f32
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnjx.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfsgnjx.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfsgnjx_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_f32_f32_f32
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfsgnjx.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfsgnjx_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_f64_f64_f64
; CHECK:       vfsgnjx.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnjx.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfsgnjx_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_f64_f64_f64
; CHECK:       vfsgnjx.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfsgnjx.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfeq.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfeq_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_i1_f32_f32
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfeq.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfeq.mask.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfeq_mask_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_i1_f32_f32
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfeq.mask.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfeq.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfeq_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vv_i1_f64_f64
; CHECK:       vfeq.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfeq.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfeq.mask.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfeq_mask_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vv_i1_f64_f64
; CHECK:       vfeq.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfeq.mask.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfeq.nxv1i1.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfeq_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_i1_f32_f32
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfeq.nxv1i1.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfeq.mask.nxv1i1.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfeq_mask_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_i1_f32_f32
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfeq.mask.nxv1i1.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfeq.nxv1i1.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfeq_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_vf_i1_f64_f64
; CHECK:       vfeq.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfeq.nxv1i1.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfeq.mask.nxv1i1.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfeq_mask_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfeq_mask_vf_i1_f64_f64
; CHECK:       vfeq.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfeq.mask.nxv1i1.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfne.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfne_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_i1_f32_f32
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfne.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfne.mask.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfne_mask_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_i1_f32_f32
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfne.mask.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfne.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfne_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vv_i1_f64_f64
; CHECK:       vfne.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfne.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfne.mask.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfne_mask_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vv_i1_f64_f64
; CHECK:       vfne.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfne.mask.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfne.nxv1i1.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfne_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_i1_f32_f32
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfne.nxv1i1.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfne.mask.nxv1i1.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfne_mask_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_i1_f32_f32
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfne.mask.nxv1i1.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfne.nxv1i1.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfne_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_vf_i1_f64_f64
; CHECK:       vfne.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfne.nxv1i1.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfne.mask.nxv1i1.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfne_mask_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfne_mask_vf_i1_f64_f64
; CHECK:       vfne.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfne.mask.nxv1i1.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vflt.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vflt_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_i1_f32_f32
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vflt.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vflt.mask.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vflt_mask_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_i1_f32_f32
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vflt.mask.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vflt.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vflt_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vv_i1_f64_f64
; CHECK:       vflt.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vflt.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vflt.mask.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vflt_mask_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vv_i1_f64_f64
; CHECK:       vflt.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vflt.mask.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vflt.nxv1i1.f32(<vscale x 1 x float>, float);
define void @intrinsic_vflt_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_i1_f32_f32
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vflt.nxv1i1.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vflt.mask.nxv1i1.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vflt_mask_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_i1_f32_f32
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vflt.mask.nxv1i1.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vflt.nxv1i1.f64(<vscale x 1 x double>, double);
define void @intrinsic_vflt_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_vf_i1_f64_f64
; CHECK:       vflt.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vflt.nxv1i1.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vflt.mask.nxv1i1.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vflt_mask_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vflt_mask_vf_i1_f64_f64
; CHECK:       vflt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vflt.mask.nxv1i1.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfle.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfle_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_i1_f32_f32
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfle.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfle.mask.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfle_mask_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_i1_f32_f32
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfle.mask.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfle.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfle_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vv_i1_f64_f64
; CHECK:       vfle.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfle.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfle.mask.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfle_mask_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vv_i1_f64_f64
; CHECK:       vfle.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfle.mask.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfle.nxv1i1.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfle_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_i1_f32_f32
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfle.nxv1i1.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfle.mask.nxv1i1.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfle_mask_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_i1_f32_f32
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfle.mask.nxv1i1.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfle.nxv1i1.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfle_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_vf_i1_f64_f64
; CHECK:       vfle.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfle.nxv1i1.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfle.mask.nxv1i1.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfle_mask_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfle_mask_vf_i1_f64_f64
; CHECK:       vfle.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfle.mask.nxv1i1.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfgt.nxv1i1.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfgt_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_i1_f32_f32
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfgt.nxv1i1.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfgt.mask.nxv1i1.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfgt_mask_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_i1_f32_f32
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfgt.mask.nxv1i1.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfgt.nxv1i1.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfgt_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_vf_i1_f64_f64
; CHECK:       vfgt.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfgt.nxv1i1.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfgt.mask.nxv1i1.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfgt_mask_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfgt_mask_vf_i1_f64_f64
; CHECK:       vfgt.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfgt.mask.nxv1i1.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfge.nxv1i1.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfge_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_i1_f32_f32
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfge.nxv1i1.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfge.mask.nxv1i1.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfge_mask_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_i1_f32_f32
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfge.mask.nxv1i1.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vfge.nxv1i1.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfge_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_vf_i1_f64_f64
; CHECK:       vfge.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vfge.nxv1i1.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vfge.mask.nxv1i1.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfge_mask_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfge_mask_vf_i1_f64_f64
; CHECK:       vfge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vfge.mask.nxv1i1.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vford.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vford_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_i1_f32_f32
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vford.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vford.mask.nxv1i1.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vford_mask_vv_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_i1_f32_f32
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vford.mask.nxv1i1.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vford.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vford_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vv_i1_f64_f64
; CHECK:       vford.vv v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vford.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vford.mask.nxv1i1.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vford_mask_vv_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vv_i1_f64_f64
; CHECK:       vford.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vford.mask.nxv1i1.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vford.nxv1i1.f32(<vscale x 1 x float>, float);
define void @intrinsic_vford_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_i1_f32_f32
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vford.nxv1i1.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vford.mask.nxv1i1.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vford_mask_vf_i1_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_i1_f32_f32
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vford.mask.nxv1i1.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vford.nxv1i1.f64(<vscale x 1 x double>, double);
define void @intrinsic_vford_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_vf_i1_f64_f64
; CHECK:       vford.vf v0, v0, ft0
  %a = call <vscale x 1 x  i1> @llvm.epi.vford.nxv1i1.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vford.mask.nxv1i1.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vford_mask_vf_i1_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vford_mask_vf_i1_f64_f64
; CHECK:       vford.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  i1> @llvm.epi.vford.mask.nxv1i1.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmerge.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfmerge_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_f32_f32_f32
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmerge.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmerge.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmerge_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_f32_f32_f32
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmerge.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfmerge_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vv_f64_f64_f64
; CHECK:       vmerge.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmerge.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmerge.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmerge_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vv_f64_f64_f64
; CHECK:       vmerge.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmerge.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmerge.nxv1f32.f32(<vscale x 1 x float>, float);
define void @intrinsic_vfmerge_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_f32_f32_f32
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmerge.nxv1f32.f32(<vscale x 1 x float> undef, float undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmerge.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x i1>);
define void @intrinsic_vfmerge_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_f32_f32_f32
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmerge.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.f64(<vscale x 1 x double>, double);
define void @intrinsic_vfmerge_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vf_f64_f64_f64
; CHECK:       vfmerge.vf v0, v0, ft0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmerge.nxv1f64.f64(<vscale x 1 x double> undef, double undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmerge.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x i1>);
define void @intrinsic_vfmerge_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_mask_vf_f64_f64_f64
; CHECK:       vfmerge.vf v0, v0, ft0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmerge.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vredsum.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vredsum_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_i8_i8_i8
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vredsum.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vredsum.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vredsum_mask_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_i8_i8_i8
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vredsum.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vredsum.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vredsum_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_i16_i16_i16
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vredsum.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vredsum.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vredsum_mask_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_i16_i16_i16
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vredsum.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vredsum.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vredsum_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_i32_i32_i32
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vredsum.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vredsum.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vredsum_mask_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_i32_i32_i32
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vredsum.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredsum.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vredsum_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_i64_i64_i64
; CHECK:       vredsum.vs v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vredsum.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredsum.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vredsum_mask_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_i64_i64_i64
; CHECK:       vredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vredsum.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vredand.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vredand_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_i8_i8_i8
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vredand.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vredand.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vredand_mask_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_i8_i8_i8
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vredand.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vredand.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vredand_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_i16_i16_i16
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vredand.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vredand.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vredand_mask_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_i16_i16_i16
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vredand.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vredand.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vredand_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_i32_i32_i32
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vredand.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vredand.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vredand_mask_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_i32_i32_i32
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vredand.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredand.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vredand_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_i64_i64_i64
; CHECK:       vredand.vs v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vredand.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredand.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vredand_mask_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_i64_i64_i64
; CHECK:       vredand.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vredand.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vredor.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vredor_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_i8_i8_i8
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vredor.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vredor.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vredor_mask_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_i8_i8_i8
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vredor.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vredor.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vredor_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_i16_i16_i16
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vredor.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vredor.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vredor_mask_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_i16_i16_i16
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vredor.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vredor.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vredor_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_i32_i32_i32
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vredor.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vredor.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vredor_mask_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_i32_i32_i32
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vredor.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredor.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vredor_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_i64_i64_i64
; CHECK:       vredor.vs v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vredor.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredor.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vredor_mask_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_i64_i64_i64
; CHECK:       vredor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vredor.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vredxor.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vredxor_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_i8_i8_i8
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vredxor.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vredxor.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vredxor_mask_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_i8_i8_i8
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vredxor.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vredxor.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vredxor_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_i16_i16_i16
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vredxor.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vredxor.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vredxor_mask_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_i16_i16_i16
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vredxor.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vredxor.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vredxor_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_i32_i32_i32
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vredxor.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vredxor.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vredxor_mask_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_i32_i32_i32
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vredxor.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredxor.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vredxor_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_i64_i64_i64
; CHECK:       vredxor.vs v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vredxor.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredxor.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vredxor_mask_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_i64_i64_i64
; CHECK:       vredxor.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vredxor.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vredminu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vredminu_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_i8_i8_i8
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vredminu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vredminu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vredminu_mask_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_i8_i8_i8
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vredminu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vredminu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vredminu_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_i16_i16_i16
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vredminu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vredminu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vredminu_mask_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_i16_i16_i16
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vredminu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vredminu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vredminu_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_i32_i32_i32
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vredminu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vredminu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vredminu_mask_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_i32_i32_i32
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vredminu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredminu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vredminu_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_i64_i64_i64
; CHECK:       vredminu.vs v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vredminu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredminu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vredminu_mask_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_i64_i64_i64
; CHECK:       vredminu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vredminu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vredmin.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vredmin_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_i8_i8_i8
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vredmin.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vredmin.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vredmin_mask_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_i8_i8_i8
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vredmin.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vredmin.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vredmin_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_i16_i16_i16
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vredmin.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vredmin.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vredmin_mask_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_i16_i16_i16
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vredmin.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vredmin.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vredmin_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_i32_i32_i32
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vredmin.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vredmin.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vredmin_mask_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_i32_i32_i32
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vredmin.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredmin.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vredmin_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_i64_i64_i64
; CHECK:       vredmin.vs v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vredmin.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredmin.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vredmin_mask_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_i64_i64_i64
; CHECK:       vredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vredmin.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vredmaxu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vredmaxu_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_i8_i8_i8
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vredmaxu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vredmaxu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vredmaxu_mask_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_i8_i8_i8
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vredmaxu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vredmaxu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vredmaxu_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_i16_i16_i16
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vredmaxu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vredmaxu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vredmaxu_mask_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_i16_i16_i16
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vredmaxu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vredmaxu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vredmaxu_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_i32_i32_i32
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vredmaxu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vredmaxu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vredmaxu_mask_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_i32_i32_i32
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vredmaxu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredmaxu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vredmaxu_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_i64_i64_i64
; CHECK:       vredmaxu.vs v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vredmaxu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredmaxu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vredmaxu_mask_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_i64_i64_i64
; CHECK:       vredmaxu.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vredmaxu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vredmax.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vredmax_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_i8_i8_i8
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vredmax.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vredmax.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vredmax_mask_vs_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_i8_i8_i8
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vredmax.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vredmax.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vredmax_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_i16_i16_i16
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vredmax.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vredmax.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vredmax_mask_vs_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_i16_i16_i16
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vredmax.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vredmax.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vredmax_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_i32_i32_i32
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vredmax.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vredmax.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vredmax_mask_vs_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_i32_i32_i32
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vredmax.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredmax.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vredmax_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_i64_i64_i64
; CHECK:       vredmax.vs v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vredmax.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredmax.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vredmax_mask_vs_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_i64_i64_i64
; CHECK:       vredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vredmax.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfredsum.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfredsum_vs_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_f32_f32_f32
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfredsum.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfredsum.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfredsum_mask_vs_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_f32_f32_f32
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfredsum.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredsum.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfredsum_vs_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_f64_f64_f64
; CHECK:       vfredsum.vs v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfredsum.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredsum.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfredsum_mask_vs_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_f64_f64_f64
; CHECK:       vfredsum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfredsum.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfredosum.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfredosum_vs_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_f32_f32_f32
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfredosum.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfredosum.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfredosum_mask_vs_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_f32_f32_f32
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfredosum.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredosum.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfredosum_vs_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_f64_f64_f64
; CHECK:       vfredosum.vs v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfredosum.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredosum.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfredosum_mask_vs_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_f64_f64_f64
; CHECK:       vfredosum.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfredosum.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfredmin.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfredmin_vs_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_f32_f32_f32
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfredmin.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfredmin.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfredmin_mask_vs_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_f32_f32_f32
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfredmin.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredmin.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfredmin_vs_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_f64_f64_f64
; CHECK:       vfredmin.vs v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfredmin.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredmin.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfredmin_mask_vs_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_f64_f64_f64
; CHECK:       vfredmin.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfredmin.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfredmax.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfredmax_vs_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_f32_f32_f32
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfredmax.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfredmax.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfredmax_mask_vs_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_f32_f32_f32
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfredmax.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredmax.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfredmax_vs_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_f64_f64_f64
; CHECK:       vfredmax.vs v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfredmax.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredmax.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfredmax_mask_vs_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_f64_f64_f64
; CHECK:       vfredmax.vs v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfredmax.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmandnot.nxv1i1.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmandnot_mm_i1_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmandnot_mm_i1_i1_i1
; CHECK:       vmandnot.mm v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmandnot.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmand.nxv1i1.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmand_mm_i1_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmand_mm_i1_i1_i1
; CHECK:       vmand.mm v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmand.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmor.nxv1i1.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmor_mm_i1_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmor_mm_i1_i1_i1
; CHECK:       vmor.mm v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmor.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmxor.nxv1i1.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmxor_mm_i1_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxor_mm_i1_i1_i1
; CHECK:       vmxor.mm v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmxor.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmornot.nxv1i1.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmornot_mm_i1_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmornot_mm_i1_i1_i1
; CHECK:       vmornot.mm v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmornot.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmnand.nxv1i1.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmnand_mm_i1_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnand_mm_i1_i1_i1
; CHECK:       vmnand.mm v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmnand.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmnor.nxv1i1.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmnor_mm_i1_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnor_mm_i1_i1_i1
; CHECK:       vmnor.mm v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmnor.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmxnor.nxv1i1.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>);
define void @intrinsic_vmxnor_mm_i1_i1_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxnor_mm_i1_i1_i1
; CHECK:       vmxnor.mm v0, v0, v0
  %a = call <vscale x 1 x  i1> @llvm.epi.vmxnor.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vdotu.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vdotu_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_i8_i8_i8
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vdotu.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vdotu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vdotu_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_i8_i8_i8
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vdotu.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vdotu.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vdotu_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_i16_i16_i16
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vdotu.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vdotu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vdotu_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_i16_i16_i16
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vdotu.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vdotu.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vdotu_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_i32_i32_i32
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vdotu.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vdotu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vdotu_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_i32_i32_i32
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vdotu.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdotu.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vdotu_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_vv_i64_i64_i64
; CHECK:       vdotu.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vdotu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdotu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vdotu_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdotu_mask_vv_i64_i64_i64
; CHECK:       vdotu.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vdotu.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vdot.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vdot_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_i8_i8_i8
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vdot.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vdot.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vdot_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_i8_i8_i8
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vdot.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vdot.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vdot_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_i16_i16_i16
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vdot.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vdot.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vdot_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_i16_i16_i16
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vdot.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vdot.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vdot_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_i32_i32_i32
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vdot.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vdot.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vdot_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_i32_i32_i32
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vdot.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdot.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vdot_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_vv_i64_i64_i64
; CHECK:       vdot.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vdot.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdot.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vdot_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdot_mask_vv_i64_i64_i64
; CHECK:       vdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vdot.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfdot.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfdot_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_f32_f32_f32
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfdot.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfdot.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfdot_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_f32_f32_f32
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfdot.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfdot.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfdot_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_vv_f64_f64_f64
; CHECK:       vfdot.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfdot.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfdot.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfdot_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdot_mask_vv_f64_f64_f64
; CHECK:       vfdot.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfdot.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vcompress.nxv1i8.nxv1i1(<vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vcompress_vm_i8_i8_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_i8_i8_i1
; CHECK:       vcompress.vm v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vcompress.nxv1i8.nxv1i1(<vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vcompress.nxv1i16.nxv1i1(<vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vcompress_vm_i16_i16_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_i16_i16_i1
; CHECK:       vcompress.vm v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vcompress.nxv1i16.nxv1i1(<vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vcompress.nxv1i32.nxv1i1(<vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vcompress_vm_i32_i32_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_i32_i32_i1
; CHECK:       vcompress.vm v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vcompress.nxv1i32.nxv1i1(<vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vcompress.nxv1i64.nxv1i1(<vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vcompress_vm_i64_i64_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_i64_i64_i1
; CHECK:       vcompress.vm v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vcompress.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vcompress.nxv1f32.nxv1i1(<vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vcompress_vm_f32_f32_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_f32_f32_i1
; CHECK:       vcompress.vm v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vcompress.nxv1f32.nxv1i1(<vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vcompress.nxv1f64.nxv1i1(<vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vcompress_vm_f64_f64_i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_f64_f64_i1
; CHECK:       vcompress.vm v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vcompress.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrgather.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vrgather_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i8_i8_i8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrgather.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i8_i8_i8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrgather.nxv1i8.nxv1i16(<vscale x 1 x i8>, <vscale x 1 x i16>);
define void @intrinsic_vrgather_vv_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i8_i8_i16
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.nxv1i16(<vscale x 1 x i8> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrgather.mask.nxv1i8.nxv1i16(<vscale x 1 x i8>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i8_i8_i16
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.nxv1i16(<vscale x 1 x i8> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrgather.nxv1i8.nxv1i32(<vscale x 1 x i8>, <vscale x 1 x i32>);
define void @intrinsic_vrgather_vv_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i8_i8_i32
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.nxv1i32(<vscale x 1 x i8> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrgather.mask.nxv1i8.nxv1i32(<vscale x 1 x i8>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i8_i8_i32
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.nxv1i32(<vscale x 1 x i8> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrgather.nxv1i8.nxv1i64(<vscale x 1 x i8>, <vscale x 1 x i64>);
define void @intrinsic_vrgather_vv_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i8_i8_i64
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.nxv1i64(<vscale x 1 x i8> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrgather.mask.nxv1i8.nxv1i64(<vscale x 1 x i8>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i8_i8_i64
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.nxv1i64(<vscale x 1 x i8> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrgather.nxv1i16.nxv1i8(<vscale x 1 x i16>, <vscale x 1 x i8>);
define void @intrinsic_vrgather_vv_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i16_i16_i8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.nxv1i8(<vscale x 1 x i16> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrgather.mask.nxv1i16.nxv1i8(<vscale x 1 x i16>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i16_i16_i8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.nxv1i8(<vscale x 1 x i16> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrgather.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vrgather_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i16_i16_i16
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrgather.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i16_i16_i16
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrgather.nxv1i16.nxv1i32(<vscale x 1 x i16>, <vscale x 1 x i32>);
define void @intrinsic_vrgather_vv_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i16_i16_i32
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.nxv1i32(<vscale x 1 x i16> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrgather.mask.nxv1i16.nxv1i32(<vscale x 1 x i16>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i16_i16_i32
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.nxv1i32(<vscale x 1 x i16> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrgather.nxv1i16.nxv1i64(<vscale x 1 x i16>, <vscale x 1 x i64>);
define void @intrinsic_vrgather_vv_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i16_i16_i64
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.nxv1i64(<vscale x 1 x i16> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrgather.mask.nxv1i16.nxv1i64(<vscale x 1 x i16>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i16_i16_i64
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.nxv1i64(<vscale x 1 x i16> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrgather.nxv1i32.nxv1i8(<vscale x 1 x i32>, <vscale x 1 x i8>);
define void @intrinsic_vrgather_vv_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i32_i32_i8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.nxv1i8(<vscale x 1 x i32> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrgather.mask.nxv1i32.nxv1i8(<vscale x 1 x i32>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i32_i32_i8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.nxv1i8(<vscale x 1 x i32> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrgather.nxv1i32.nxv1i16(<vscale x 1 x i32>, <vscale x 1 x i16>);
define void @intrinsic_vrgather_vv_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i32_i32_i16
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.nxv1i16(<vscale x 1 x i32> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrgather.mask.nxv1i32.nxv1i16(<vscale x 1 x i32>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i32_i32_i16
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.nxv1i16(<vscale x 1 x i32> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrgather.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vrgather_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i32_i32_i32
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrgather.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i32_i32_i32
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrgather.nxv1i32.nxv1i64(<vscale x 1 x i32>, <vscale x 1 x i64>);
define void @intrinsic_vrgather_vv_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i32_i32_i64
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.nxv1i64(<vscale x 1 x i32> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrgather.mask.nxv1i32.nxv1i64(<vscale x 1 x i32>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i32_i32_i64
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.nxv1i64(<vscale x 1 x i32> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.nxv1i8(<vscale x 1 x i64>, <vscale x 1 x i8>);
define void @intrinsic_vrgather_vv_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i64_i64_i8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.nxv1i8(<vscale x 1 x i64> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i8(<vscale x 1 x i64>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i64_i64_i8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i8(<vscale x 1 x i64> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.nxv1i16(<vscale x 1 x i64>, <vscale x 1 x i16>);
define void @intrinsic_vrgather_vv_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i64_i64_i16
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.nxv1i16(<vscale x 1 x i64> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i16(<vscale x 1 x i64>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i64_i64_i16
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i16(<vscale x 1 x i64> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.nxv1i32(<vscale x 1 x i64>, <vscale x 1 x i32>);
define void @intrinsic_vrgather_vv_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i64_i64_i32
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.nxv1i32(<vscale x 1 x i64> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i32(<vscale x 1 x i64>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i64_i64_i32
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i32(<vscale x 1 x i64> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vrgather_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_i64_i64_i64
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_i64_i64_i64
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vrgather.nxv1f32.nxv1i8(<vscale x 1 x float>, <vscale x 1 x i8>);
define void @intrinsic_vrgather_vv_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_f32_f32_i8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.nxv1i8(<vscale x 1 x float> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vrgather.mask.nxv1f32.nxv1i8(<vscale x 1 x float>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_f32_f32_i8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.nxv1i8(<vscale x 1 x float> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vrgather.nxv1f32.nxv1i16(<vscale x 1 x float>, <vscale x 1 x i16>);
define void @intrinsic_vrgather_vv_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_f32_f32_i16
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.nxv1i16(<vscale x 1 x float> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vrgather.mask.nxv1f32.nxv1i16(<vscale x 1 x float>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_f32_f32_i16
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.nxv1i16(<vscale x 1 x float> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vrgather.nxv1f32.nxv1i32(<vscale x 1 x float>, <vscale x 1 x i32>);
define void @intrinsic_vrgather_vv_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_f32_f32_i32
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.nxv1i32(<vscale x 1 x float> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vrgather.mask.nxv1f32.nxv1i32(<vscale x 1 x float>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_f32_f32_i32
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.nxv1i32(<vscale x 1 x float> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vrgather.nxv1f32.nxv1i64(<vscale x 1 x float>, <vscale x 1 x i64>);
define void @intrinsic_vrgather_vv_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_f32_f32_i64
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.nxv1i64(<vscale x 1 x float> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vrgather.mask.nxv1f32.nxv1i64(<vscale x 1 x float>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_f32_f32_i64
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.nxv1i64(<vscale x 1 x float> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i8(<vscale x 1 x double>, <vscale x 1 x i8>);
define void @intrinsic_vrgather_vv_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_f64_f64_i8
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.nxv1i8(<vscale x 1 x double> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i8(<vscale x 1 x double>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_f64_f64_i8
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i8(<vscale x 1 x double> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i16(<vscale x 1 x double>, <vscale x 1 x i16>);
define void @intrinsic_vrgather_vv_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_f64_f64_i16
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.nxv1i16(<vscale x 1 x double> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i16(<vscale x 1 x double>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_f64_f64_i16
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i16(<vscale x 1 x double> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i32(<vscale x 1 x double>, <vscale x 1 x i32>);
define void @intrinsic_vrgather_vv_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_f64_f64_i32
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.nxv1i32(<vscale x 1 x double> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i32(<vscale x 1 x double>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_f64_f64_i32
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i32(<vscale x 1 x double> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double>, <vscale x 1 x i64>);
define void @intrinsic_vrgather_vv_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_f64_f64_i64
; CHECK:       vrgather.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i64(<vscale x 1 x double>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vv_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_f64_f64_i64
; CHECK:       vrgather.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrgather.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vrgather_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i8_i8_i8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrgather.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i8_i8_i8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrgather.nxv1i8.i16(<vscale x 1 x i8>, i16);
define void @intrinsic_vrgather_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i8_i8_i16
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrgather.mask.nxv1i8.i16(<vscale x 1 x i8>, i16, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i8_i8_i16
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrgather.nxv1i8.i32(<vscale x 1 x i8>, i32);
define void @intrinsic_vrgather_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i8_i8_i32
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrgather.mask.nxv1i8.i32(<vscale x 1 x i8>, i32, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i8_i8_i32
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vrgather.nxv1i8.i64(<vscale x 1 x i8>, i64);
define void @intrinsic_vrgather_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i8_i8_i64
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vrgather.mask.nxv1i8.i64(<vscale x 1 x i8>, i64, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i8_i8_i64
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrgather.nxv1i16.i8(<vscale x 1 x i16>, i8);
define void @intrinsic_vrgather_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i16_i16_i8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrgather.mask.nxv1i16.i8(<vscale x 1 x i16>, i8, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i16_i16_i8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrgather.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vrgather_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i16_i16_i16
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrgather.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i16_i16_i16
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrgather.nxv1i16.i32(<vscale x 1 x i16>, i32);
define void @intrinsic_vrgather_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i16_i16_i32
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrgather.mask.nxv1i16.i32(<vscale x 1 x i16>, i32, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i16_i16_i32
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vrgather.nxv1i16.i64(<vscale x 1 x i16>, i64);
define void @intrinsic_vrgather_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i16_i16_i64
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vrgather.mask.nxv1i16.i64(<vscale x 1 x i16>, i64, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i16_i16_i64
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrgather.nxv1i32.i8(<vscale x 1 x i32>, i8);
define void @intrinsic_vrgather_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i32_i32_i8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrgather.mask.nxv1i32.i8(<vscale x 1 x i32>, i8, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i32_i32_i8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrgather.nxv1i32.i16(<vscale x 1 x i32>, i16);
define void @intrinsic_vrgather_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i32_i32_i16
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrgather.mask.nxv1i32.i16(<vscale x 1 x i32>, i16, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i32_i32_i16
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrgather.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vrgather_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i32_i32_i32
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrgather.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i32_i32_i32
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vrgather.nxv1i32.i64(<vscale x 1 x i32>, i64);
define void @intrinsic_vrgather_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i32_i32_i64
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vrgather.mask.nxv1i32.i64(<vscale x 1 x i32>, i64, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i32_i32_i64
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i8(<vscale x 1 x i64>, i8);
define void @intrinsic_vrgather_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i64_i64_i8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i8(<vscale x 1 x i64>, i8, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i64_i64_i8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i16(<vscale x 1 x i64>, i16);
define void @intrinsic_vrgather_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i64_i64_i16
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i16(<vscale x 1 x i64>, i16, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i64_i64_i16
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i32(<vscale x 1 x i64>, i32);
define void @intrinsic_vrgather_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i64_i64_i32
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i32(<vscale x 1 x i64>, i32, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i64_i64_i32
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vrgather_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_i64_i64_i64
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_i64_i64_i64
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vrgather.nxv1f32.i8(<vscale x 1 x float>, i8);
define void @intrinsic_vrgather_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_f32_f32_i8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vrgather.mask.nxv1f32.i8(<vscale x 1 x float>, i8, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_f32_f32_i8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vrgather.nxv1f32.i16(<vscale x 1 x float>, i16);
define void @intrinsic_vrgather_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_f32_f32_i16
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vrgather.mask.nxv1f32.i16(<vscale x 1 x float>, i16, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_f32_f32_i16
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vrgather.nxv1f32.i32(<vscale x 1 x float>, i32);
define void @intrinsic_vrgather_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_f32_f32_i32
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vrgather.mask.nxv1f32.i32(<vscale x 1 x float>, i32, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_f32_f32_i32
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vrgather.nxv1f32.i64(<vscale x 1 x float>, i64);
define void @intrinsic_vrgather_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_f32_f32_i64
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vrgather.mask.nxv1f32.i64(<vscale x 1 x float>, i64, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_f32_f32_i64
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i8(<vscale x 1 x double>, i8);
define void @intrinsic_vrgather_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_f64_f64_i8
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i8(<vscale x 1 x double>, i8, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_f64_f64_i8
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i16(<vscale x 1 x double>, i16);
define void @intrinsic_vrgather_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_f64_f64_i16
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i16(<vscale x 1 x double>, i16, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_f64_f64_i16
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i32(<vscale x 1 x double>, i32);
define void @intrinsic_vrgather_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_f64_f64_i32
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i32(<vscale x 1 x double>, i32, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_f64_f64_i32
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i64(<vscale x 1 x double>, i64);
define void @intrinsic_vrgather_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_f64_f64_i64
; CHECK:       vrgather.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i64(<vscale x 1 x double>, i64, <vscale x 1 x i1>);
define void @intrinsic_vrgather_mask_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_f64_f64_i64
; CHECK:       vrgather.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i8_i8_i8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i8_i8_i8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i8_i8_i16
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.i16(<vscale x 1 x i8> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i8_i8_i16
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.i16(<vscale x 1 x i8> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i8_i8_i32
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.i32(<vscale x 1 x i8> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i8_i8_i32
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.i32(<vscale x 1 x i8> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i8_i8_i64
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.nxv1i8.i64(<vscale x 1 x i8> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i8_i8_i64
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vrgather.mask.nxv1i8.i64(<vscale x 1 x i8> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i16_i16_i8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.i8(<vscale x 1 x i16> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i16_i16_i8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.i8(<vscale x 1 x i16> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i16_i16_i16
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i16_i16_i16
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i16_i16_i32
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.i32(<vscale x 1 x i16> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i16_i16_i32
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.i32(<vscale x 1 x i16> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i16_i16_i64
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.nxv1i16.i64(<vscale x 1 x i16> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i16_i16_i64
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vrgather.mask.nxv1i16.i64(<vscale x 1 x i16> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i32_i32_i8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.i8(<vscale x 1 x i32> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i32_i32_i8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.i8(<vscale x 1 x i32> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i32_i32_i16
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.i16(<vscale x 1 x i32> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i32_i32_i16
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.i16(<vscale x 1 x i32> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i32_i32_i32
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i32_i32_i32
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i32_i32_i64
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.nxv1i32.i64(<vscale x 1 x i32> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i32_i32_i64
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vrgather.mask.nxv1i32.i64(<vscale x 1 x i32> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i64_i64_i8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.i8(<vscale x 1 x i64> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i64_i64_i8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.i8(<vscale x 1 x i64> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i64_i64_i16
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.i16(<vscale x 1 x i64> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i64_i64_i16
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.i16(<vscale x 1 x i64> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i64_i64_i32
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.i32(<vscale x 1 x i64> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i64_i64_i32
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.i32(<vscale x 1 x i64> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vrgather_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_i64_i64_i64
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_i64_i64_i64
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vrgather.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vrgather_vi_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_f32_f32_i8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.i8(<vscale x 1 x float> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_f32_f32_i8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.i8(<vscale x 1 x float> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vrgather_vi_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_f32_f32_i16
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.i16(<vscale x 1 x float> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_f32_f32_i16
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.i16(<vscale x 1 x float> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vrgather_vi_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_f32_f32_i32
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.i32(<vscale x 1 x float> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_f32_f32_i32
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.i32(<vscale x 1 x float> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vrgather_vi_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_f32_f32_i64
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.nxv1f32.i64(<vscale x 1 x float> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_f32_f32_i64
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vrgather.mask.nxv1f32.i64(<vscale x 1 x float> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vrgather_vi_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_f64_f64_i8
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.i8(<vscale x 1 x double> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_f64_f64_i8
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.i8(<vscale x 1 x double> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vrgather_vi_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_f64_f64_i16
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.i16(<vscale x 1 x double> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_f64_f64_i16
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.i16(<vscale x 1 x double> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vrgather_vi_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_f64_f64_i32
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.i32(<vscale x 1 x double> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_f64_f64_i32
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.i32(<vscale x 1 x double> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vrgather_vi_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_f64_f64_i64
; CHECK:       vrgather.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.nxv1f64.i64(<vscale x 1 x double> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vrgather_mask_vi_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_f64_f64_i64
; CHECK:       vrgather.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vrgather.mask.nxv1f64.i64(<vscale x 1 x double> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslideup.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vslideup_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i8_i8_i8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslideup.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i8_i8_i8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslideup.nxv1i8.i16(<vscale x 1 x i8>, i16);
define void @intrinsic_vslideup_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i8_i8_i16
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslideup.mask.nxv1i8.i16(<vscale x 1 x i8>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i8_i8_i16
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.mask.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslideup.nxv1i8.i32(<vscale x 1 x i8>, i32);
define void @intrinsic_vslideup_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i8_i8_i32
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslideup.mask.nxv1i8.i32(<vscale x 1 x i8>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i8_i8_i32
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.mask.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslideup.nxv1i8.i64(<vscale x 1 x i8>, i64);
define void @intrinsic_vslideup_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i8_i8_i64
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslideup.mask.nxv1i8.i64(<vscale x 1 x i8>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i8_i8_i64
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.mask.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslideup.nxv1i16.i8(<vscale x 1 x i16>, i8);
define void @intrinsic_vslideup_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i16_i16_i8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslideup.mask.nxv1i16.i8(<vscale x 1 x i16>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i16_i16_i8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.mask.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslideup.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vslideup_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i16_i16_i16
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslideup.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i16_i16_i16
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslideup.nxv1i16.i32(<vscale x 1 x i16>, i32);
define void @intrinsic_vslideup_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i16_i16_i32
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslideup.mask.nxv1i16.i32(<vscale x 1 x i16>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i16_i16_i32
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.mask.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslideup.nxv1i16.i64(<vscale x 1 x i16>, i64);
define void @intrinsic_vslideup_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i16_i16_i64
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslideup.mask.nxv1i16.i64(<vscale x 1 x i16>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i16_i16_i64
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.mask.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslideup.nxv1i32.i8(<vscale x 1 x i32>, i8);
define void @intrinsic_vslideup_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i32_i32_i8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslideup.mask.nxv1i32.i8(<vscale x 1 x i32>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i32_i32_i8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.mask.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslideup.nxv1i32.i16(<vscale x 1 x i32>, i16);
define void @intrinsic_vslideup_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i32_i32_i16
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslideup.mask.nxv1i32.i16(<vscale x 1 x i32>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i32_i32_i16
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.mask.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslideup.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vslideup_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i32_i32_i32
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslideup.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i32_i32_i32
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslideup.nxv1i32.i64(<vscale x 1 x i32>, i64);
define void @intrinsic_vslideup_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i32_i32_i64
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslideup.mask.nxv1i32.i64(<vscale x 1 x i32>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i32_i32_i64
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.mask.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i8(<vscale x 1 x i64>, i8);
define void @intrinsic_vslideup_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i64_i64_i8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i8(<vscale x 1 x i64>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i64_i64_i8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.mask.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i16(<vscale x 1 x i64>, i16);
define void @intrinsic_vslideup_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i64_i64_i16
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i16(<vscale x 1 x i64>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i64_i64_i16
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.mask.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i32(<vscale x 1 x i64>, i32);
define void @intrinsic_vslideup_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i64_i64_i32
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i32(<vscale x 1 x i64>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i64_i64_i32
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.mask.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vslideup_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_i64_i64_i64
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_i64_i64_i64
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslideup.nxv1f32.i8(<vscale x 1 x float>, i8);
define void @intrinsic_vslideup_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_f32_f32_i8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslideup.mask.nxv1f32.i8(<vscale x 1 x float>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_f32_f32_i8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.mask.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslideup.nxv1f32.i16(<vscale x 1 x float>, i16);
define void @intrinsic_vslideup_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_f32_f32_i16
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslideup.mask.nxv1f32.i16(<vscale x 1 x float>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_f32_f32_i16
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.mask.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslideup.nxv1f32.i32(<vscale x 1 x float>, i32);
define void @intrinsic_vslideup_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_f32_f32_i32
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslideup.mask.nxv1f32.i32(<vscale x 1 x float>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_f32_f32_i32
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.mask.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslideup.nxv1f32.i64(<vscale x 1 x float>, i64);
define void @intrinsic_vslideup_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_f32_f32_i64
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslideup.mask.nxv1f32.i64(<vscale x 1 x float>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_f32_f32_i64
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.mask.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i8(<vscale x 1 x double>, i8);
define void @intrinsic_vslideup_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_f64_f64_i8
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i8(<vscale x 1 x double>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_f64_f64_i8
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.mask.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i16(<vscale x 1 x double>, i16);
define void @intrinsic_vslideup_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_f64_f64_i16
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i16(<vscale x 1 x double>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_f64_f64_i16
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.mask.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i32(<vscale x 1 x double>, i32);
define void @intrinsic_vslideup_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_f64_f64_i32
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i32(<vscale x 1 x double>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_f64_f64_i32
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.mask.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i64(<vscale x 1 x double>, i64);
define void @intrinsic_vslideup_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_f64_f64_i64
; CHECK:       vslideup.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i64(<vscale x 1 x double>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslideup_mask_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_f64_f64_i64
; CHECK:       vslideup.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.mask.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i8_i8_i8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i8_i8_i8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i8_i8_i16
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.nxv1i8.i16(<vscale x 1 x i8> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i8_i8_i16
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.mask.nxv1i8.i16(<vscale x 1 x i8> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i8_i8_i32
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.nxv1i8.i32(<vscale x 1 x i8> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i8_i8_i32
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.mask.nxv1i8.i32(<vscale x 1 x i8> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i8_i8_i64
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.nxv1i8.i64(<vscale x 1 x i8> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i8_i8_i64
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslideup.mask.nxv1i8.i64(<vscale x 1 x i8> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i16_i16_i8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.nxv1i16.i8(<vscale x 1 x i16> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i16_i16_i8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.mask.nxv1i16.i8(<vscale x 1 x i16> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i16_i16_i16
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i16_i16_i16
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i16_i16_i32
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.nxv1i16.i32(<vscale x 1 x i16> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i16_i16_i32
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.mask.nxv1i16.i32(<vscale x 1 x i16> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i16_i16_i64
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.nxv1i16.i64(<vscale x 1 x i16> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i16_i16_i64
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslideup.mask.nxv1i16.i64(<vscale x 1 x i16> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i32_i32_i8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.nxv1i32.i8(<vscale x 1 x i32> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i32_i32_i8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.mask.nxv1i32.i8(<vscale x 1 x i32> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i32_i32_i16
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.nxv1i32.i16(<vscale x 1 x i32> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i32_i32_i16
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.mask.nxv1i32.i16(<vscale x 1 x i32> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i32_i32_i32
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i32_i32_i32
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i32_i32_i64
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.nxv1i32.i64(<vscale x 1 x i32> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i32_i32_i64
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslideup.mask.nxv1i32.i64(<vscale x 1 x i32> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i64_i64_i8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.nxv1i64.i8(<vscale x 1 x i64> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i64_i64_i8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.mask.nxv1i64.i8(<vscale x 1 x i64> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i64_i64_i16
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.nxv1i64.i16(<vscale x 1 x i64> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i64_i64_i16
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.mask.nxv1i64.i16(<vscale x 1 x i64> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i64_i64_i32
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.nxv1i64.i32(<vscale x 1 x i64> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i64_i64_i32
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.mask.nxv1i64.i32(<vscale x 1 x i64> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vslideup_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_i64_i64_i64
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_i64_i64_i64
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslideup.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vslideup_vi_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_f32_f32_i8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.nxv1f32.i8(<vscale x 1 x float> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_f32_f32_i8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.mask.nxv1f32.i8(<vscale x 1 x float> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vslideup_vi_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_f32_f32_i16
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.nxv1f32.i16(<vscale x 1 x float> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_f32_f32_i16
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.mask.nxv1f32.i16(<vscale x 1 x float> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vslideup_vi_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_f32_f32_i32
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.nxv1f32.i32(<vscale x 1 x float> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_f32_f32_i32
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.mask.nxv1f32.i32(<vscale x 1 x float> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vslideup_vi_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_f32_f32_i64
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.nxv1f32.i64(<vscale x 1 x float> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_f32_f32_i64
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslideup.mask.nxv1f32.i64(<vscale x 1 x float> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vslideup_vi_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_f64_f64_i8
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.nxv1f64.i8(<vscale x 1 x double> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_f64_f64_i8
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.mask.nxv1f64.i8(<vscale x 1 x double> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vslideup_vi_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_f64_f64_i16
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.nxv1f64.i16(<vscale x 1 x double> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_f64_f64_i16
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.mask.nxv1f64.i16(<vscale x 1 x double> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vslideup_vi_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_f64_f64_i32
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.nxv1f64.i32(<vscale x 1 x double> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_f64_f64_i32
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.mask.nxv1f64.i32(<vscale x 1 x double> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vslideup_vi_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_f64_f64_i64
; CHECK:       vslideup.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.nxv1f64.i64(<vscale x 1 x double> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vslideup_mask_vi_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_f64_f64_i64
; CHECK:       vslideup.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslideup.mask.nxv1f64.i64(<vscale x 1 x double> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslidedown.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vslidedown_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i8_i8_i8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslidedown.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i8_i8_i8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslidedown.nxv1i8.i16(<vscale x 1 x i8>, i16);
define void @intrinsic_vslidedown_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i8_i8_i16
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslidedown.mask.nxv1i8.i16(<vscale x 1 x i8>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i8_i8_i16
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.mask.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslidedown.nxv1i8.i32(<vscale x 1 x i8>, i32);
define void @intrinsic_vslidedown_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i8_i8_i32
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslidedown.mask.nxv1i8.i32(<vscale x 1 x i8>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i8_i8_i32
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.mask.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslidedown.nxv1i8.i64(<vscale x 1 x i8>, i64);
define void @intrinsic_vslidedown_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i8_i8_i64
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslidedown.mask.nxv1i8.i64(<vscale x 1 x i8>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i8_i8_i64
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.mask.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslidedown.nxv1i16.i8(<vscale x 1 x i16>, i8);
define void @intrinsic_vslidedown_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i16_i16_i8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslidedown.mask.nxv1i16.i8(<vscale x 1 x i16>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i16_i16_i8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.mask.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslidedown.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vslidedown_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i16_i16_i16
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslidedown.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i16_i16_i16
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslidedown.nxv1i16.i32(<vscale x 1 x i16>, i32);
define void @intrinsic_vslidedown_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i16_i16_i32
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslidedown.mask.nxv1i16.i32(<vscale x 1 x i16>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i16_i16_i32
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.mask.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslidedown.nxv1i16.i64(<vscale x 1 x i16>, i64);
define void @intrinsic_vslidedown_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i16_i16_i64
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslidedown.mask.nxv1i16.i64(<vscale x 1 x i16>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i16_i16_i64
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.mask.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslidedown.nxv1i32.i8(<vscale x 1 x i32>, i8);
define void @intrinsic_vslidedown_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i32_i32_i8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslidedown.mask.nxv1i32.i8(<vscale x 1 x i32>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i32_i32_i8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.mask.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslidedown.nxv1i32.i16(<vscale x 1 x i32>, i16);
define void @intrinsic_vslidedown_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i32_i32_i16
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslidedown.mask.nxv1i32.i16(<vscale x 1 x i32>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i32_i32_i16
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.mask.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslidedown.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vslidedown_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i32_i32_i32
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslidedown.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i32_i32_i32
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslidedown.nxv1i32.i64(<vscale x 1 x i32>, i64);
define void @intrinsic_vslidedown_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i32_i32_i64
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslidedown.mask.nxv1i32.i64(<vscale x 1 x i32>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i32_i32_i64
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.mask.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i8(<vscale x 1 x i64>, i8);
define void @intrinsic_vslidedown_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i64_i64_i8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i8(<vscale x 1 x i64>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i64_i64_i8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.mask.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i16(<vscale x 1 x i64>, i16);
define void @intrinsic_vslidedown_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i64_i64_i16
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i16(<vscale x 1 x i64>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i64_i64_i16
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.mask.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i32(<vscale x 1 x i64>, i32);
define void @intrinsic_vslidedown_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i64_i64_i32
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i32(<vscale x 1 x i64>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i64_i64_i32
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.mask.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vslidedown_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_i64_i64_i64
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_i64_i64_i64
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslidedown.nxv1f32.i8(<vscale x 1 x float>, i8);
define void @intrinsic_vslidedown_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_f32_f32_i8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslidedown.mask.nxv1f32.i8(<vscale x 1 x float>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_f32_f32_i8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.mask.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslidedown.nxv1f32.i16(<vscale x 1 x float>, i16);
define void @intrinsic_vslidedown_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_f32_f32_i16
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslidedown.mask.nxv1f32.i16(<vscale x 1 x float>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_f32_f32_i16
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.mask.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslidedown.nxv1f32.i32(<vscale x 1 x float>, i32);
define void @intrinsic_vslidedown_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_f32_f32_i32
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslidedown.mask.nxv1f32.i32(<vscale x 1 x float>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_f32_f32_i32
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.mask.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslidedown.nxv1f32.i64(<vscale x 1 x float>, i64);
define void @intrinsic_vslidedown_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_f32_f32_i64
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslidedown.mask.nxv1f32.i64(<vscale x 1 x float>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_f32_f32_i64
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.mask.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i8(<vscale x 1 x double>, i8);
define void @intrinsic_vslidedown_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_f64_f64_i8
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i8(<vscale x 1 x double>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_f64_f64_i8
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.mask.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i16(<vscale x 1 x double>, i16);
define void @intrinsic_vslidedown_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_f64_f64_i16
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i16(<vscale x 1 x double>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_f64_f64_i16
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.mask.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i32(<vscale x 1 x double>, i32);
define void @intrinsic_vslidedown_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_f64_f64_i32
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i32(<vscale x 1 x double>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_f64_f64_i32
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.mask.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i64(<vscale x 1 x double>, i64);
define void @intrinsic_vslidedown_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_f64_f64_i64
; CHECK:       vslidedown.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i64(<vscale x 1 x double>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslidedown_mask_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_f64_f64_i64
; CHECK:       vslidedown.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.mask.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i8_i8_i8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i8_i8_i8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i8_i8_i16
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.nxv1i8.i16(<vscale x 1 x i8> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i8_i8_i16
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.mask.nxv1i8.i16(<vscale x 1 x i8> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i8_i8_i32
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.nxv1i8.i32(<vscale x 1 x i8> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i8_i8_i32
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.mask.nxv1i8.i32(<vscale x 1 x i8> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i8_i8_i64
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.nxv1i8.i64(<vscale x 1 x i8> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i8_i8_i64
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslidedown.mask.nxv1i8.i64(<vscale x 1 x i8> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i16_i16_i8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.nxv1i16.i8(<vscale x 1 x i16> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i16_i16_i8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.mask.nxv1i16.i8(<vscale x 1 x i16> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i16_i16_i16
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i16_i16_i16
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i16_i16_i32
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.nxv1i16.i32(<vscale x 1 x i16> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i16_i16_i32
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.mask.nxv1i16.i32(<vscale x 1 x i16> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i16_i16_i64
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.nxv1i16.i64(<vscale x 1 x i16> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i16_i16_i64
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslidedown.mask.nxv1i16.i64(<vscale x 1 x i16> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i32_i32_i8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.nxv1i32.i8(<vscale x 1 x i32> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i32_i32_i8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.mask.nxv1i32.i8(<vscale x 1 x i32> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i32_i32_i16
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.nxv1i32.i16(<vscale x 1 x i32> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i32_i32_i16
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.mask.nxv1i32.i16(<vscale x 1 x i32> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i32_i32_i32
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i32_i32_i32
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i32_i32_i64
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.nxv1i32.i64(<vscale x 1 x i32> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i32_i32_i64
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslidedown.mask.nxv1i32.i64(<vscale x 1 x i32> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i64_i64_i8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.nxv1i64.i8(<vscale x 1 x i64> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i64_i64_i8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.mask.nxv1i64.i8(<vscale x 1 x i64> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i64_i64_i16
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.nxv1i64.i16(<vscale x 1 x i64> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i64_i64_i16
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.mask.nxv1i64.i16(<vscale x 1 x i64> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i64_i64_i32
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.nxv1i64.i32(<vscale x 1 x i64> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i64_i64_i32
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.mask.nxv1i64.i32(<vscale x 1 x i64> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_i64_i64_i64
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_i64_i64_i64
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslidedown.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_f32_f32_i8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.nxv1f32.i8(<vscale x 1 x float> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_f32_f32_i8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.mask.nxv1f32.i8(<vscale x 1 x float> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_f32_f32_i16
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.nxv1f32.i16(<vscale x 1 x float> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_f32_f32_i16
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.mask.nxv1f32.i16(<vscale x 1 x float> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_f32_f32_i32
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.nxv1f32.i32(<vscale x 1 x float> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_f32_f32_i32
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.mask.nxv1f32.i32(<vscale x 1 x float> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_f32_f32_i64
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.nxv1f32.i64(<vscale x 1 x float> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_f32_f32_i64
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslidedown.mask.nxv1f32.i64(<vscale x 1 x float> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_f64_f64_i8
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.nxv1f64.i8(<vscale x 1 x double> undef, i8 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_f64_f64_i8
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.mask.nxv1f64.i8(<vscale x 1 x double> undef, i8 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_f64_f64_i16
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.nxv1f64.i16(<vscale x 1 x double> undef, i16 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_f64_f64_i16
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.mask.nxv1f64.i16(<vscale x 1 x double> undef, i16 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_f64_f64_i32
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.nxv1f64.i32(<vscale x 1 x double> undef, i32 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_f64_f64_i32
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.mask.nxv1f64.i32(<vscale x 1 x double> undef, i32 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


define void @intrinsic_vslidedown_vi_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_f64_f64_i64
; CHECK:       vslidedown.vi v0, v0, 9
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.nxv1f64.i64(<vscale x 1 x double> undef, i64 9)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

define void @intrinsic_vslidedown_mask_vi_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_f64_f64_i64
; CHECK:       vslidedown.vi v0, v0, 9, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslidedown.mask.nxv1f64.i64(<vscale x 1 x double> undef, i64 9, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslide1up.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vslide1up_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i8_i8_i8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1up.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslide1up.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i8_i8_i8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1up.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslide1up.nxv1i8.i16(<vscale x 1 x i8>, i16);
define void @intrinsic_vslide1up_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i8_i8_i16
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1up.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslide1up.mask.nxv1i8.i16(<vscale x 1 x i8>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i8_i8_i16
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1up.mask.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslide1up.nxv1i8.i32(<vscale x 1 x i8>, i32);
define void @intrinsic_vslide1up_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i8_i8_i32
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1up.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslide1up.mask.nxv1i8.i32(<vscale x 1 x i8>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i8_i8_i32
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1up.mask.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslide1up.nxv1i8.i64(<vscale x 1 x i8>, i64);
define void @intrinsic_vslide1up_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i8_i8_i64
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1up.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslide1up.mask.nxv1i8.i64(<vscale x 1 x i8>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i8_i8_i64
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1up.mask.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslide1up.nxv1i16.i8(<vscale x 1 x i16>, i8);
define void @intrinsic_vslide1up_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i16_i16_i8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1up.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslide1up.mask.nxv1i16.i8(<vscale x 1 x i16>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i16_i16_i8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1up.mask.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslide1up.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vslide1up_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i16_i16_i16
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1up.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslide1up.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i16_i16_i16
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1up.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslide1up.nxv1i16.i32(<vscale x 1 x i16>, i32);
define void @intrinsic_vslide1up_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i16_i16_i32
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1up.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslide1up.mask.nxv1i16.i32(<vscale x 1 x i16>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i16_i16_i32
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1up.mask.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslide1up.nxv1i16.i64(<vscale x 1 x i16>, i64);
define void @intrinsic_vslide1up_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i16_i16_i64
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1up.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslide1up.mask.nxv1i16.i64(<vscale x 1 x i16>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i16_i16_i64
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1up.mask.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslide1up.nxv1i32.i8(<vscale x 1 x i32>, i8);
define void @intrinsic_vslide1up_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i32_i32_i8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1up.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslide1up.mask.nxv1i32.i8(<vscale x 1 x i32>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i32_i32_i8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1up.mask.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslide1up.nxv1i32.i16(<vscale x 1 x i32>, i16);
define void @intrinsic_vslide1up_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i32_i32_i16
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1up.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslide1up.mask.nxv1i32.i16(<vscale x 1 x i32>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i32_i32_i16
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1up.mask.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslide1up.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vslide1up_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i32_i32_i32
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1up.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslide1up.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i32_i32_i32
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1up.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslide1up.nxv1i32.i64(<vscale x 1 x i32>, i64);
define void @intrinsic_vslide1up_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i32_i32_i64
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1up.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslide1up.mask.nxv1i32.i64(<vscale x 1 x i32>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i32_i32_i64
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1up.mask.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1up.nxv1i64.i8(<vscale x 1 x i64>, i8);
define void @intrinsic_vslide1up_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i64_i64_i8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1up.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1up.mask.nxv1i64.i8(<vscale x 1 x i64>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i64_i64_i8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1up.mask.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1up.nxv1i64.i16(<vscale x 1 x i64>, i16);
define void @intrinsic_vslide1up_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i64_i64_i16
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1up.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1up.mask.nxv1i64.i16(<vscale x 1 x i64>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i64_i64_i16
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1up.mask.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1up.nxv1i64.i32(<vscale x 1 x i64>, i32);
define void @intrinsic_vslide1up_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i64_i64_i32
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1up.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1up.mask.nxv1i64.i32(<vscale x 1 x i64>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i64_i64_i32
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1up.mask.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1up.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vslide1up_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_i64_i64_i64
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1up.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1up.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_i64_i64_i64
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1up.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslide1up.nxv1f32.i8(<vscale x 1 x float>, i8);
define void @intrinsic_vslide1up_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_f32_f32_i8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1up.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslide1up.mask.nxv1f32.i8(<vscale x 1 x float>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_f32_f32_i8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1up.mask.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslide1up.nxv1f32.i16(<vscale x 1 x float>, i16);
define void @intrinsic_vslide1up_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_f32_f32_i16
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1up.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslide1up.mask.nxv1f32.i16(<vscale x 1 x float>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_f32_f32_i16
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1up.mask.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslide1up.nxv1f32.i32(<vscale x 1 x float>, i32);
define void @intrinsic_vslide1up_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_f32_f32_i32
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1up.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslide1up.mask.nxv1f32.i32(<vscale x 1 x float>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_f32_f32_i32
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1up.mask.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslide1up.nxv1f32.i64(<vscale x 1 x float>, i64);
define void @intrinsic_vslide1up_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_f32_f32_i64
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1up.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslide1up.mask.nxv1f32.i64(<vscale x 1 x float>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_f32_f32_i64
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1up.mask.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1up.nxv1f64.i8(<vscale x 1 x double>, i8);
define void @intrinsic_vslide1up_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_f64_f64_i8
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1up.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1up.mask.nxv1f64.i8(<vscale x 1 x double>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_f64_f64_i8
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1up.mask.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1up.nxv1f64.i16(<vscale x 1 x double>, i16);
define void @intrinsic_vslide1up_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_f64_f64_i16
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1up.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1up.mask.nxv1f64.i16(<vscale x 1 x double>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_f64_f64_i16
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1up.mask.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1up.nxv1f64.i32(<vscale x 1 x double>, i32);
define void @intrinsic_vslide1up_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_f64_f64_i32
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1up.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1up.mask.nxv1f64.i32(<vscale x 1 x double>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_f64_f64_i32
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1up.mask.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1up.nxv1f64.i64(<vscale x 1 x double>, i64);
define void @intrinsic_vslide1up_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_f64_f64_i64
; CHECK:       vslide1up.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1up.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1up.mask.nxv1f64.i64(<vscale x 1 x double>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1up_mask_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_f64_f64_i64
; CHECK:       vslide1up.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1up.mask.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslide1down.nxv1i8.i8(<vscale x 1 x i8>, i8);
define void @intrinsic_vslide1down_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i8_i8_i8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1down.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslide1down.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i8_i8_i8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1down.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslide1down.nxv1i8.i16(<vscale x 1 x i8>, i16);
define void @intrinsic_vslide1down_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i8_i8_i16
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1down.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslide1down.mask.nxv1i8.i16(<vscale x 1 x i8>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i8_i8_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i8_i8_i16
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1down.mask.nxv1i8.i16(<vscale x 1 x i8> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslide1down.nxv1i8.i32(<vscale x 1 x i8>, i32);
define void @intrinsic_vslide1down_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i8_i8_i32
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1down.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslide1down.mask.nxv1i8.i32(<vscale x 1 x i8>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i8_i8_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i8_i8_i32
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1down.mask.nxv1i8.i32(<vscale x 1 x i8> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vslide1down.nxv1i8.i64(<vscale x 1 x i8>, i64);
define void @intrinsic_vslide1down_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i8_i8_i64
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1down.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vslide1down.mask.nxv1i8.i64(<vscale x 1 x i8>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i8_i8_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i8_i8_i64
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vslide1down.mask.nxv1i8.i64(<vscale x 1 x i8> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslide1down.nxv1i16.i8(<vscale x 1 x i16>, i8);
define void @intrinsic_vslide1down_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i16_i16_i8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1down.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslide1down.mask.nxv1i16.i8(<vscale x 1 x i16>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i16_i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i16_i16_i8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1down.mask.nxv1i16.i8(<vscale x 1 x i16> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslide1down.nxv1i16.i16(<vscale x 1 x i16>, i16);
define void @intrinsic_vslide1down_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i16_i16_i16
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1down.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslide1down.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i16_i16_i16
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1down.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslide1down.nxv1i16.i32(<vscale x 1 x i16>, i32);
define void @intrinsic_vslide1down_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i16_i16_i32
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1down.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslide1down.mask.nxv1i16.i32(<vscale x 1 x i16>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i16_i16_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i16_i16_i32
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1down.mask.nxv1i16.i32(<vscale x 1 x i16> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vslide1down.nxv1i16.i64(<vscale x 1 x i16>, i64);
define void @intrinsic_vslide1down_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i16_i16_i64
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1down.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vslide1down.mask.nxv1i16.i64(<vscale x 1 x i16>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i16_i16_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i16_i16_i64
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vslide1down.mask.nxv1i16.i64(<vscale x 1 x i16> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslide1down.nxv1i32.i8(<vscale x 1 x i32>, i8);
define void @intrinsic_vslide1down_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i32_i32_i8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1down.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslide1down.mask.nxv1i32.i8(<vscale x 1 x i32>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i32_i32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i32_i32_i8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1down.mask.nxv1i32.i8(<vscale x 1 x i32> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslide1down.nxv1i32.i16(<vscale x 1 x i32>, i16);
define void @intrinsic_vslide1down_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i32_i32_i16
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1down.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslide1down.mask.nxv1i32.i16(<vscale x 1 x i32>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i32_i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i32_i32_i16
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1down.mask.nxv1i32.i16(<vscale x 1 x i32> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslide1down.nxv1i32.i32(<vscale x 1 x i32>, i32);
define void @intrinsic_vslide1down_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i32_i32_i32
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1down.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslide1down.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i32_i32_i32
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1down.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vslide1down.nxv1i32.i64(<vscale x 1 x i32>, i64);
define void @intrinsic_vslide1down_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i32_i32_i64
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1down.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vslide1down.mask.nxv1i32.i64(<vscale x 1 x i32>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i32_i32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i32_i32_i64
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vslide1down.mask.nxv1i32.i64(<vscale x 1 x i32> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1down.nxv1i64.i8(<vscale x 1 x i64>, i8);
define void @intrinsic_vslide1down_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i64_i64_i8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1down.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1down.mask.nxv1i64.i8(<vscale x 1 x i64>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i64_i64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i64_i64_i8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1down.mask.nxv1i64.i8(<vscale x 1 x i64> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1down.nxv1i64.i16(<vscale x 1 x i64>, i16);
define void @intrinsic_vslide1down_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i64_i64_i16
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1down.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1down.mask.nxv1i64.i16(<vscale x 1 x i64>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i64_i64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i64_i64_i16
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1down.mask.nxv1i64.i16(<vscale x 1 x i64> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1down.nxv1i64.i32(<vscale x 1 x i64>, i32);
define void @intrinsic_vslide1down_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i64_i64_i32
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1down.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1down.mask.nxv1i64.i32(<vscale x 1 x i64>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i64_i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i64_i64_i32
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1down.mask.nxv1i64.i32(<vscale x 1 x i64> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1down.nxv1i64.i64(<vscale x 1 x i64>, i64);
define void @intrinsic_vslide1down_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_i64_i64_i64
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1down.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1down.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_i64_i64_i64
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vslide1down.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslide1down.nxv1f32.i8(<vscale x 1 x float>, i8);
define void @intrinsic_vslide1down_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_f32_f32_i8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1down.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslide1down.mask.nxv1f32.i8(<vscale x 1 x float>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_f32_f32_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_f32_f32_i8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1down.mask.nxv1f32.i8(<vscale x 1 x float> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslide1down.nxv1f32.i16(<vscale x 1 x float>, i16);
define void @intrinsic_vslide1down_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_f32_f32_i16
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1down.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslide1down.mask.nxv1f32.i16(<vscale x 1 x float>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_f32_f32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_f32_f32_i16
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1down.mask.nxv1f32.i16(<vscale x 1 x float> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslide1down.nxv1f32.i32(<vscale x 1 x float>, i32);
define void @intrinsic_vslide1down_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_f32_f32_i32
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1down.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslide1down.mask.nxv1f32.i32(<vscale x 1 x float>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_f32_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_f32_f32_i32
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1down.mask.nxv1f32.i32(<vscale x 1 x float> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vslide1down.nxv1f32.i64(<vscale x 1 x float>, i64);
define void @intrinsic_vslide1down_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_f32_f32_i64
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1down.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vslide1down.mask.nxv1f32.i64(<vscale x 1 x float>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_f32_f32_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_f32_f32_i64
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vslide1down.mask.nxv1f32.i64(<vscale x 1 x float> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1down.nxv1f64.i8(<vscale x 1 x double>, i8);
define void @intrinsic_vslide1down_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_f64_f64_i8
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1down.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1down.mask.nxv1f64.i8(<vscale x 1 x double>, i8, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_f64_f64_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_f64_f64_i8
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1down.mask.nxv1f64.i8(<vscale x 1 x double> undef, i8 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1down.nxv1f64.i16(<vscale x 1 x double>, i16);
define void @intrinsic_vslide1down_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_f64_f64_i16
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1down.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1down.mask.nxv1f64.i16(<vscale x 1 x double>, i16, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_f64_f64_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_f64_f64_i16
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1down.mask.nxv1f64.i16(<vscale x 1 x double> undef, i16 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1down.nxv1f64.i32(<vscale x 1 x double>, i32);
define void @intrinsic_vslide1down_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_f64_f64_i32
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1down.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1down.mask.nxv1f64.i32(<vscale x 1 x double>, i32, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_f64_f64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_f64_f64_i32
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1down.mask.nxv1f64.i32(<vscale x 1 x double> undef, i32 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1down.nxv1f64.i64(<vscale x 1 x double>, i64);
define void @intrinsic_vslide1down_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_f64_f64_i64
; CHECK:       vslide1down.vx v0, v0, a0
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1down.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1down.mask.nxv1f64.i64(<vscale x 1 x double>, i64, <vscale x 1 x i1>);
define void @intrinsic_vslide1down_mask_vx_f64_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_f64_f64_i64
; CHECK:       vslide1down.vx v0, v0, a0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vslide1down.mask.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vadc.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vadc_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vv_i8_i8_i8
; CHECK:       vadc.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vadc.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vadc.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vadc_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vv_i16_i16_i16
; CHECK:       vadc.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vadc.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vadc.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vadc_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vv_i32_i32_i32
; CHECK:       vadc.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vadc.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadc.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vadc_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vv_i64_i64_i64
; CHECK:       vadc.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vadc.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vadc.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>);
define void @intrinsic_vadc_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vx_i8_i8_i8
; CHECK:       vadc.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vadc.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vadc.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>);
define void @intrinsic_vadc_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vx_i16_i16_i16
; CHECK:       vadc.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vadc.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vadc.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>);
define void @intrinsic_vadc_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vx_i32_i32_i32
; CHECK:       vadc.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vadc.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadc.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>);
define void @intrinsic_vadc_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vx_i64_i64_i64
; CHECK:       vadc.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vadc.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


define void @intrinsic_vadc_vi_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vi_i8_i8_i8
; CHECK:       vadc.vi v0, v0, 9
  %a = call <vscale x 1 x  i8> @llvm.epi.vadc.nxv1i8.i8(<vscale x 1 x i8> undef, i8 9, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


define void @intrinsic_vadc_vi_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vi_i16_i16_i16
; CHECK:       vadc.vi v0, v0, 9
  %a = call <vscale x 1 x  i16> @llvm.epi.vadc.nxv1i16.i16(<vscale x 1 x i16> undef, i16 9, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


define void @intrinsic_vadc_vi_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vi_i32_i32_i32
; CHECK:       vadc.vi v0, v0, 9
  %a = call <vscale x 1 x  i32> @llvm.epi.vadc.nxv1i32.i32(<vscale x 1 x i32> undef, i32 9, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


define void @intrinsic_vadc_vi_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vi_i64_i64_i64
; CHECK:       vadc.vi v0, v0, 9
  %a = call <vscale x 1 x  i64> @llvm.epi.vadc.nxv1i64.i64(<vscale x 1 x i64> undef, i64 9, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsbc.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vsbc_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vv_i8_i8_i8
; CHECK:       vsbc.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsbc.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsbc.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vsbc_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vv_i16_i16_i16
; CHECK:       vsbc.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsbc.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsbc.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vsbc_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vv_i32_i32_i32
; CHECK:       vsbc.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsbc.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsbc.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vsbc_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vv_i64_i64_i64
; CHECK:       vsbc.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsbc.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vsbc.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>);
define void @intrinsic_vsbc_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vx_i8_i8_i8
; CHECK:       vsbc.vx v0, v0, a0
  %a = call <vscale x 1 x  i8> @llvm.epi.vsbc.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vsbc.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>);
define void @intrinsic_vsbc_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vx_i16_i16_i16
; CHECK:       vsbc.vx v0, v0, a0
  %a = call <vscale x 1 x  i16> @llvm.epi.vsbc.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vsbc.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>);
define void @intrinsic_vsbc_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vx_i32_i32_i32
; CHECK:       vsbc.vx v0, v0, a0
  %a = call <vscale x 1 x  i32> @llvm.epi.vsbc.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsbc.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>);
define void @intrinsic_vsbc_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vx_i64_i64_i64
; CHECK:       vsbc.vx v0, v0, a0
  %a = call <vscale x 1 x  i64> @llvm.epi.vsbc.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmacc.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmacc_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_i8_i8_i8
; CHECK:       vmacc.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmacc.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmacc.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmacc_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_i8_i8_i8
; CHECK:       vmacc.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmacc.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmacc.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmacc_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_i16_i16_i16
; CHECK:       vmacc.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmacc.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmacc.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmacc_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_i16_i16_i16
; CHECK:       vmacc.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmacc.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmacc.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmacc_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_i32_i32_i32
; CHECK:       vmacc.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmacc.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmacc.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmacc_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_i32_i32_i32
; CHECK:       vmacc.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmacc.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmacc.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmacc_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_i64_i64_i64
; CHECK:       vmacc.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmacc.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmacc.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmacc_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_i64_i64_i64
; CHECK:       vmacc.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmacc.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmacc.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>);
define void @intrinsic_vmacc_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_i8_i8_i8
; CHECK:       vmacc.vx v0, a0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmacc.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmacc.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmacc_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_i8_i8_i8
; CHECK:       vmacc.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmacc.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmacc.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>);
define void @intrinsic_vmacc_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_i16_i16_i16
; CHECK:       vmacc.vx v0, a0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmacc.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmacc.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmacc_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_i16_i16_i16
; CHECK:       vmacc.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmacc.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmacc.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>);
define void @intrinsic_vmacc_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_i32_i32_i32
; CHECK:       vmacc.vx v0, a0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmacc.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmacc.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmacc_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_i32_i32_i32
; CHECK:       vmacc.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmacc.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmacc.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>);
define void @intrinsic_vmacc_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_i64_i64_i64
; CHECK:       vmacc.vx v0, a0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmacc.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmacc.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmacc_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_i64_i64_i64
; CHECK:       vmacc.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmacc.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmsac.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmsac_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_vv_i8_i8_i8
; CHECK:       vmsac.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmsac.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmsac.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmsac_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_mask_vv_i8_i8_i8
; CHECK:       vmsac.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmsac.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmsac.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmsac_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_vv_i16_i16_i16
; CHECK:       vmsac.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmsac.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmsac.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmsac_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_mask_vv_i16_i16_i16
; CHECK:       vmsac.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmsac.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmsac.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmsac_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_vv_i32_i32_i32
; CHECK:       vmsac.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmsac.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmsac.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmsac_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_mask_vv_i32_i32_i32
; CHECK:       vmsac.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmsac.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmsac.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmsac_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_vv_i64_i64_i64
; CHECK:       vmsac.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmsac.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmsac.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmsac_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_mask_vv_i64_i64_i64
; CHECK:       vmsac.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmsac.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmsac.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>);
define void @intrinsic_vmsac_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_vx_i8_i8_i8
; CHECK:       vmsac.vx v0, a0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmsac.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmsac.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmsac_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_mask_vx_i8_i8_i8
; CHECK:       vmsac.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmsac.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmsac.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>);
define void @intrinsic_vmsac_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_vx_i16_i16_i16
; CHECK:       vmsac.vx v0, a0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmsac.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmsac.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmsac_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_mask_vx_i16_i16_i16
; CHECK:       vmsac.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmsac.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmsac.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>);
define void @intrinsic_vmsac_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_vx_i32_i32_i32
; CHECK:       vmsac.vx v0, a0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmsac.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmsac.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmsac_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_mask_vx_i32_i32_i32
; CHECK:       vmsac.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmsac.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmsac.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>);
define void @intrinsic_vmsac_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_vx_i64_i64_i64
; CHECK:       vmsac.vx v0, a0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmsac.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmsac.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmsac_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsac_mask_vx_i64_i64_i64
; CHECK:       vmsac.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmsac.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmadd.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmadd_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_i8_i8_i8
; CHECK:       vmadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmadd.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmadd.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmadd_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_i8_i8_i8
; CHECK:       vmadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmadd.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmadd.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmadd_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_i16_i16_i16
; CHECK:       vmadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmadd.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmadd.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmadd_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_i16_i16_i16
; CHECK:       vmadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmadd.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmadd.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmadd_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_i32_i32_i32
; CHECK:       vmadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmadd.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmadd.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmadd_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_i32_i32_i32
; CHECK:       vmadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmadd.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmadd.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmadd_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_i64_i64_i64
; CHECK:       vmadd.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmadd.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmadd.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmadd_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_i64_i64_i64
; CHECK:       vmadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmadd.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmadd.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>);
define void @intrinsic_vmadd_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_i8_i8_i8
; CHECK:       vmadd.vx v0, a0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmadd.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmadd.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmadd_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_i8_i8_i8
; CHECK:       vmadd.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmadd.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmadd.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>);
define void @intrinsic_vmadd_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_i16_i16_i16
; CHECK:       vmadd.vx v0, a0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmadd.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmadd.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmadd_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_i16_i16_i16
; CHECK:       vmadd.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmadd.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmadd.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>);
define void @intrinsic_vmadd_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_i32_i32_i32
; CHECK:       vmadd.vx v0, a0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmadd.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmadd.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmadd_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_i32_i32_i32
; CHECK:       vmadd.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmadd.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmadd.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>);
define void @intrinsic_vmadd_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_i64_i64_i64
; CHECK:       vmadd.vx v0, a0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmadd.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmadd.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmadd_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_i64_i64_i64
; CHECK:       vmadd.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmadd.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmsub.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>);
define void @intrinsic_vmsub_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_vv_i8_i8_i8
; CHECK:       vmsub.vv v0, v0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmsub.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmsub.mask.nxv1i8.nxv1i8(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmsub_mask_vv_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_mask_vv_i8_i8_i8
; CHECK:       vmsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmsub.mask.nxv1i8.nxv1i8(<vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmsub.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>);
define void @intrinsic_vmsub_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_vv_i16_i16_i16
; CHECK:       vmsub.vv v0, v0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmsub.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmsub.mask.nxv1i16.nxv1i16(<vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmsub_mask_vv_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_mask_vv_i16_i16_i16
; CHECK:       vmsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmsub.mask.nxv1i16.nxv1i16(<vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmsub.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>);
define void @intrinsic_vmsub_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_vv_i32_i32_i32
; CHECK:       vmsub.vv v0, v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmsub.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmsub.mask.nxv1i32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmsub_mask_vv_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_mask_vv_i32_i32_i32
; CHECK:       vmsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmsub.mask.nxv1i32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmsub.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>);
define void @intrinsic_vmsub_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_vv_i64_i64_i64
; CHECK:       vmsub.vv v0, v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmsub.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmsub.mask.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmsub_mask_vv_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_mask_vv_i64_i64_i64
; CHECK:       vmsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmsub.mask.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i8> @llvm.epi.vmsub.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>);
define void @intrinsic_vmsub_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_vx_i8_i8_i8
; CHECK:       vmsub.vx v0, a0, v0
  %a = call <vscale x 1 x  i8> @llvm.epi.vmsub.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}

declare <vscale x 1 x i8> @llvm.epi.vmsub.mask.nxv1i8.i8(<vscale x 1 x i8>, i8, <vscale x 1 x i8>, <vscale x 1 x i1>);
define void @intrinsic_vmsub_mask_vx_i8_i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_mask_vx_i8_i8_i8
; CHECK:       vmsub.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i8> @llvm.epi.vmsub.mask.nxv1i8.i8(<vscale x 1 x i8> undef, i8 undef, <vscale x 1 x i8> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i8>*
  store <vscale x 1 x i8> %a, <vscale x 1 x i8>* %p
  ret void
}


declare <vscale x 1 x i16> @llvm.epi.vmsub.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>);
define void @intrinsic_vmsub_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_vx_i16_i16_i16
; CHECK:       vmsub.vx v0, a0, v0
  %a = call <vscale x 1 x  i16> @llvm.epi.vmsub.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}

declare <vscale x 1 x i16> @llvm.epi.vmsub.mask.nxv1i16.i16(<vscale x 1 x i16>, i16, <vscale x 1 x i16>, <vscale x 1 x i1>);
define void @intrinsic_vmsub_mask_vx_i16_i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_mask_vx_i16_i16_i16
; CHECK:       vmsub.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i16> @llvm.epi.vmsub.mask.nxv1i16.i16(<vscale x 1 x i16> undef, i16 undef, <vscale x 1 x i16> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i16>*
  store <vscale x 1 x i16> %a, <vscale x 1 x i16>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vmsub.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>);
define void @intrinsic_vmsub_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_vx_i32_i32_i32
; CHECK:       vmsub.vx v0, a0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vmsub.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vmsub.mask.nxv1i32.i32(<vscale x 1 x i32>, i32, <vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vmsub_mask_vx_i32_i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_mask_vx_i32_i32_i32
; CHECK:       vmsub.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vmsub.mask.nxv1i32.i32(<vscale x 1 x i32> undef, i32 undef, <vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmsub.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>);
define void @intrinsic_vmsub_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_vx_i64_i64_i64
; CHECK:       vmsub.vx v0, a0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vmsub.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmsub.mask.nxv1i64.i64(<vscale x 1 x i64>, i64, <vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vmsub_mask_vx_i64_i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsub_mask_vx_i64_i64_i64
; CHECK:       vmsub.vx v0, a0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vmsub.mask.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmadd.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfmadd_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_f32_f32_f32
; CHECK:       vfmadd.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmadd.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmadd.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmadd_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_f32_f32_f32
; CHECK:       vfmadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmadd.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfmadd_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_f64_f64_f64
; CHECK:       vfmadd.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmadd.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmadd_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_f64_f64_f64
; CHECK:       vfmadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmadd.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmadd.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>);
define void @intrinsic_vfmadd_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_f32_f32_f32
; CHECK:       vfmadd.vf v0, ft0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmadd.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmadd.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmadd_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_f32_f32_f32
; CHECK:       vfmadd.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmadd.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmadd.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>);
define void @intrinsic_vfmadd_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_f64_f64_f64
; CHECK:       vfmadd.vf v0, ft0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmadd.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmadd.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmadd_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_f64_f64_f64
; CHECK:       vfmadd.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmadd.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfnmadd.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfnmadd_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_f32_f32_f32
; CHECK:       vfnmadd.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmadd.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfnmadd.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfnmadd_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_f32_f32_f32
; CHECK:       vfnmadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmadd.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfnmadd_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_f64_f64_f64
; CHECK:       vfnmadd.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmadd.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfnmadd_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_f64_f64_f64
; CHECK:       vfnmadd.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmadd.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfnmadd.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>);
define void @intrinsic_vfnmadd_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_f32_f32_f32
; CHECK:       vfnmadd.vf v0, ft0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmadd.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfnmadd.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfnmadd_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_f32_f32_f32
; CHECK:       vfnmadd.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmadd.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmadd.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>);
define void @intrinsic_vfnmadd_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_f64_f64_f64
; CHECK:       vfnmadd.vf v0, ft0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmadd.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmadd.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfnmadd_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_f64_f64_f64
; CHECK:       vfnmadd.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmadd.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmsub.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfmsub_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_f32_f32_f32
; CHECK:       vfmsub.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmsub.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmsub.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmsub_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_f32_f32_f32
; CHECK:       vfmsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmsub.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmsub.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfmsub_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_f64_f64_f64
; CHECK:       vfmsub.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmsub.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmsub_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_f64_f64_f64
; CHECK:       vfmsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmsub.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmsub.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>);
define void @intrinsic_vfmsub_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_f32_f32_f32
; CHECK:       vfmsub.vf v0, ft0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmsub.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmsub.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmsub_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_f32_f32_f32
; CHECK:       vfmsub.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmsub.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmsub.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>);
define void @intrinsic_vfmsub_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_f64_f64_f64
; CHECK:       vfmsub.vf v0, ft0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmsub.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmsub.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmsub_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_f64_f64_f64
; CHECK:       vfmsub.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmsub.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfnmsub.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfnmsub_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_f32_f32_f32
; CHECK:       vfnmsub.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmsub.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfnmsub.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfnmsub_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_f32_f32_f32
; CHECK:       vfnmsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmsub.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmsub.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfnmsub_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_f64_f64_f64
; CHECK:       vfnmsub.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmsub.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfnmsub_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_f64_f64_f64
; CHECK:       vfnmsub.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmsub.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfnmsub.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>);
define void @intrinsic_vfnmsub_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_f32_f32_f32
; CHECK:       vfnmsub.vf v0, ft0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmsub.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfnmsub.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfnmsub_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_f32_f32_f32
; CHECK:       vfnmsub.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmsub.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmsub.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>);
define void @intrinsic_vfnmsub_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_f64_f64_f64
; CHECK:       vfnmsub.vf v0, ft0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmsub.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmsub.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfnmsub_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_f64_f64_f64
; CHECK:       vfnmsub.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmsub.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmacc.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfmacc_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_f32_f32_f32
; CHECK:       vfmacc.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmacc.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmacc.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmacc_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_f32_f32_f32
; CHECK:       vfmacc.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmacc.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfmacc_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_f64_f64_f64
; CHECK:       vfmacc.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmacc_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_f64_f64_f64
; CHECK:       vfmacc.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmacc.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmacc.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>);
define void @intrinsic_vfmacc_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_f32_f32_f32
; CHECK:       vfmacc.vf v0, ft0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmacc.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmacc.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmacc_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_f32_f32_f32
; CHECK:       vfmacc.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmacc.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>);
define void @intrinsic_vfmacc_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_f64_f64_f64
; CHECK:       vfmacc.vf v0, ft0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmacc.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmacc_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_f64_f64_f64
; CHECK:       vfmacc.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmacc.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfnmacc.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfnmacc_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_f32_f32_f32
; CHECK:       vfnmacc.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmacc.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfnmacc.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfnmacc_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_f32_f32_f32
; CHECK:       vfnmacc.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmacc.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmacc.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfnmacc_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_f64_f64_f64
; CHECK:       vfnmacc.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmacc.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmacc.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfnmacc_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_f64_f64_f64
; CHECK:       vfnmacc.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmacc.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfnmacc.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>);
define void @intrinsic_vfnmacc_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_f32_f32_f32
; CHECK:       vfnmacc.vf v0, ft0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmacc.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfnmacc.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfnmacc_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_f32_f32_f32
; CHECK:       vfnmacc.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmacc.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmacc.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>);
define void @intrinsic_vfnmacc_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_f64_f64_f64
; CHECK:       vfnmacc.vf v0, ft0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmacc.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmacc.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfnmacc_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_f64_f64_f64
; CHECK:       vfnmacc.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmacc.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmsac.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfmsac_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_f32_f32_f32
; CHECK:       vfmsac.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmsac.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmsac.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmsac_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_f32_f32_f32
; CHECK:       vfmsac.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmsac.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfmsac_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_f64_f64_f64
; CHECK:       vfmsac.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmsac.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmsac_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_f64_f64_f64
; CHECK:       vfmsac.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmsac.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfmsac.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>);
define void @intrinsic_vfmsac_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_f32_f32_f32
; CHECK:       vfmsac.vf v0, ft0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfmsac.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfmsac.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfmsac_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_f32_f32_f32
; CHECK:       vfmsac.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfmsac.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>);
define void @intrinsic_vfmsac_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_f64_f64_f64
; CHECK:       vfmsac.vf v0, ft0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfmsac.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmsac.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfmsac_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_f64_f64_f64
; CHECK:       vfmsac.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfmsac.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfnmsac.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>);
define void @intrinsic_vfnmsac_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_f32_f32_f32
; CHECK:       vfnmsac.vv v0, v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmsac.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfnmsac.mask.nxv1f32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfnmsac_mask_vv_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_f32_f32_f32
; CHECK:       vfnmsac.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmsac.mask.nxv1f32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>);
define void @intrinsic_vfnmsac_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_f64_f64_f64
; CHECK:       vfnmsac.vv v0, v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmsac.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfnmsac_mask_vv_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_f64_f64_f64
; CHECK:       vfnmsac.vv v0, v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmsac.mask.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfnmsac.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>);
define void @intrinsic_vfnmsac_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_f32_f32_f32
; CHECK:       vfnmsac.vf v0, ft0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmsac.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfnmsac.mask.nxv1f32.f32(<vscale x 1 x float>, float, <vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfnmsac_mask_vf_f32_f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_f32_f32_f32
; CHECK:       vfnmsac.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfnmsac.mask.nxv1f32.f32(<vscale x 1 x float> undef, float undef, <vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>);
define void @intrinsic_vfnmsac_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_f64_f64_f64
; CHECK:       vfnmsac.vf v0, ft0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmsac.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmsac.mask.nxv1f64.f64(<vscale x 1 x double>, double, <vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfnmsac_mask_vf_f64_f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_f64_f64_f64
; CHECK:       vfnmsac.vf v0, ft0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfnmsac.mask.nxv1f64.f64(<vscale x 1 x double> undef, double undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vfcvt.xu.f.nxv1i32.nxv1f32(<vscale x 1 x float>);
define void @intrinsic_vfcvt_xu.f_i32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_i32_f32
; CHECK:       vfcvt.xu.f.v v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vfcvt.xu.f.nxv1i32.nxv1f32(<vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv1i32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfcvt_mask_xu.f_i32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_i32_f32
; CHECK:       vfcvt.xu.f.v v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vfcvt.xu.f.mask.nxv1i32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.nxv1i64.nxv1f64(<vscale x 1 x double>);
define void @intrinsic_vfcvt_xu.f_i64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_i64_f64
; CHECK:       vfcvt.xu.f.v v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vfcvt.xu.f.nxv1i64.nxv1f64(<vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv1i64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfcvt_mask_xu.f_i64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_i64_f64
; CHECK:       vfcvt.xu.f.v v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vfcvt.xu.f.mask.nxv1i64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x i32> @llvm.epi.vfcvt.x.f.nxv1i32.nxv1f32(<vscale x 1 x float>);
define void @intrinsic_vfcvt_x.f_i32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_i32_f32
; CHECK:       vfcvt.x.f.v v0, v0
  %a = call <vscale x 1 x  i32> @llvm.epi.vfcvt.x.f.nxv1i32.nxv1f32(<vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}

declare <vscale x 1 x i32> @llvm.epi.vfcvt.x.f.mask.nxv1i32.nxv1f32(<vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfcvt_mask_x.f_i32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_i32_f32
; CHECK:       vfcvt.x.f.v v0, v0, v0.t
  %a = call <vscale x 1 x  i32> @llvm.epi.vfcvt.x.f.mask.nxv1i32.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i32>*
  store <vscale x 1 x i32> %a, <vscale x 1 x i32>* %p
  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.nxv1i64.nxv1f64(<vscale x 1 x double>);
define void @intrinsic_vfcvt_x.f_i64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_i64_f64
; CHECK:       vfcvt.x.f.v v0, v0
  %a = call <vscale x 1 x  i64> @llvm.epi.vfcvt.x.f.nxv1i64.nxv1f64(<vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.mask.nxv1i64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfcvt_mask_x.f_i64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_i64_f64
; CHECK:       vfcvt.x.f.v v0, v0, v0.t
  %a = call <vscale x 1 x  i64> @llvm.epi.vfcvt.x.f.mask.nxv1i64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfcvt.f.xu.nxv1f32.nxv1i32(<vscale x 1 x i32>);
define void @intrinsic_vfcvt_f.xu_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_f32_i32
; CHECK:       vfcvt.f.xu.v v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfcvt.f.xu.nxv1f32.nxv1i32(<vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfcvt.f.xu.mask.nxv1f32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vfcvt_mask_f.xu_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_f32_i32
; CHECK:       vfcvt.f.xu.v v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfcvt.f.xu.mask.nxv1f32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.nxv1f64.nxv1i64(<vscale x 1 x i64>);
define void @intrinsic_vfcvt_f.xu_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_f64_i64
; CHECK:       vfcvt.f.xu.v v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfcvt.f.xu.nxv1f64.nxv1i64(<vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.mask.nxv1f64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vfcvt_mask_f.xu_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_f64_i64
; CHECK:       vfcvt.f.xu.v v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfcvt.f.xu.mask.nxv1f64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfcvt.f.x.nxv1f32.nxv1i32(<vscale x 1 x i32>);
define void @intrinsic_vfcvt_f.x_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_f32_i32
; CHECK:       vfcvt.f.x.v v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfcvt.f.x.nxv1f32.nxv1i32(<vscale x 1 x i32> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfcvt.f.x.mask.nxv1f32.nxv1i32(<vscale x 1 x i32>, <vscale x 1 x i1>);
define void @intrinsic_vfcvt_mask_f.x_f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_f32_i32
; CHECK:       vfcvt.f.x.v v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfcvt.f.x.mask.nxv1f32.nxv1i32(<vscale x 1 x i32> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfcvt.f.x.nxv1f64.nxv1i64(<vscale x 1 x i64>);
define void @intrinsic_vfcvt_f.x_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_f64_i64
; CHECK:       vfcvt.f.x.v v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfcvt.f.x.nxv1f64.nxv1i64(<vscale x 1 x i64> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfcvt.f.x.mask.nxv1f64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i1>);
define void @intrinsic_vfcvt_mask_f.x_f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_f64_i64
; CHECK:       vfcvt.f.x.v v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfcvt.f.x.mask.nxv1f64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfwcvt.f.f.nxv1f64.nxv1f32(<vscale x 1 x float>);
define void @intrinsic_vfwcvt_f.f_f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.f_f64_f32
; CHECK:       vfwcvt.f.f.v v0, v0
  %a = call <vscale x 1 x  double> @llvm.epi.vfwcvt.f.f.nxv1f64.nxv1f32(<vscale x 1 x float> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfwcvt.f.f.mask.nxv1f64.nxv1f32(<vscale x 1 x float>, <vscale x 1 x i1>);
define void @intrinsic_vfwcvt_mask_f.f_f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.f_f64_f32
; CHECK:       vfwcvt.f.f.v v0, v0, v0.t
  %a = call <vscale x 1 x  double> @llvm.epi.vfwcvt.f.f.mask.nxv1f64.nxv1f32(<vscale x 1 x float> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p
  ret void
}


declare <vscale x 1 x float> @llvm.epi.vfncvt.f.f.nxv1f32.nxv1f64(<vscale x 1 x double>);
define void @intrinsic_vfncvt_f.f_f32_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.f_f32_f64
; CHECK:       vfncvt.f.f.v v0, v0
  %a = call <vscale x 1 x  float> @llvm.epi.vfncvt.f.f.nxv1f32.nxv1f64(<vscale x 1 x double> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

declare <vscale x 1 x float> @llvm.epi.vfncvt.f.f.mask.nxv1f32.nxv1f64(<vscale x 1 x double>, <vscale x 1 x i1>);
define void @intrinsic_vfncvt_mask_f.f_f32_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.f_f32_f64
; CHECK:       vfncvt.f.f.v v0, v0, v0.t
  %a = call <vscale x 1 x  float> @llvm.epi.vfncvt.f.f.mask.nxv1f32.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x i1> undef)
  %p = bitcast i8* @scratch to <vscale x 1 x float>*
  store <vscale x 1 x float> %a, <vscale x 1 x float>* %p
  ret void
}

