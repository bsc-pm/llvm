; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+epi -verify-machineinstrs -O0 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+epi -verify-machineinstrs -O2 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O2 %s

declare i64 @llvm.epi.vsetvl(i64, i64, i64)

declare <vscale x 1 x double> @llvm.epi.vload.v1f64(
  <vscale x 1 x double>*,
  i64)

declare <vscale x 1 x i1> @llvm.epi.vflt.v1i1.v1f64.v1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64)

declare <vscale x 1 x double> @llvm.epi.vfsub.mask.v1f64.v1f64.v1i1(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64)

declare void @llvm.epi.vstore.v1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64)

define void @merge_mask(i64 %vl, double* %c, double* %a, double* %b) {
; CHECK-O0-LABEL: merge_mask:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -64
; CHECK-O0-NEXT:    sd ra, 56(sp)
; CHECK-O0-NEXT:    sd s0, 48(sp)
; CHECK-O0-NEXT:    addi s0, sp, 64
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 64
; CHECK-O0-NEXT:    .cfi_offset ra, -8
; CHECK-O0-NEXT:    .cfi_offset s0, -16
; CHECK-O0-NEXT:    rdvtype a5
; CHECK-O0-NEXT:    rdvl a4
; CHECK-O0-NEXT:    vsetvli a6, zero, e64, m1
; CHECK-O0-NEXT:    slli a6, a6, 3
; CHECK-O0-NEXT:    sub sp, sp, a6
; CHECK-O0-NEXT:    andi sp, sp, -16
; CHECK-O0-NEXT:    sd sp, -24(s0)
; CHECK-O0-NEXT:    vsetvl zero, a4, a5
; CHECK-O0-NEXT:    vsetvli a4, a0, e64, m1
; CHECK-O0-NEXT:    vle.v v0, (a3)
; CHECK-O0-NEXT:    vle.v v1, (a2)
; CHECK-O0-NEXT:    vflt.vv v2, v1, v0
; CHECK-O0-NEXT:    rdvtype a6
; CHECK-O0-NEXT:    rdvl a5
; CHECK-O0-NEXT:    ld a7, -24(s0)
; CHECK-O0-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-O0-NEXT:    vse.v v0, (a7)
; CHECK-O0-NEXT:    vsetvl zero, a5, a6
; CHECK-O0-NEXT:    vadd.vi v0, v2, 0
; CHECK-O0-NEXT:    rdvtype a6
; CHECK-O0-NEXT:    rdvl a5
; CHECK-O0-NEXT:    ld a7, -24(s0)
; CHECK-O0-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-O0-NEXT:    vle.v v2, (a7)
; CHECK-O0-NEXT:    vsetvl zero, a5, a6
; CHECK-O0-NEXT:    vfsub.vv v2, v2, v1, v0.t
; CHECK-O0-NEXT:    vse.v v2, (a1)
; CHECK-O0-NEXT:    sd a3, -32(s0)
; CHECK-O0-NEXT:    sd a2, -40(s0)
; CHECK-O0-NEXT:    sd a1, -48(s0)
; CHECK-O0-NEXT:    sd a0, -56(s0)
; CHECK-O0-NEXT:    addi sp, s0, -64
; CHECK-O0-NEXT:    ld s0, 48(sp)
; CHECK-O0-NEXT:    ld ra, 56(sp)
; CHECK-O0-NEXT:    addi sp, sp, 64
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: merge_mask:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1
; CHECK-O2-NEXT:    vle.v v1, (a3)
; CHECK-O2-NEXT:    vle.v v2, (a2)
; CHECK-O2-NEXT:    vflt.vv v0, v2, v1
; CHECK-O2-NEXT:    vfsub.vv v1, v1, v2, v0.t
; CHECK-O2-NEXT:    vse.v v1, (a1)
; CHECK-O2-NEXT:    ret
entry:

  %gvl = call i64 @llvm.epi.vsetvl(i64 %vl, i64 3, i64 0)

  %addr_a = bitcast double* %a to <vscale x 1 x double>*
  %vec_a = call <vscale x 1 x double> @llvm.epi.vload.v1f64(
    <vscale x 1 x double>* %addr_a,
    i64 %gvl)

  %addr_b = bitcast double* %b to <vscale x 1 x double>*
  %vec_b = call <vscale x 1 x double> @llvm.epi.vload.v1f64(
    <vscale x 1 x double>* %addr_b,
    i64 %gvl)

  %cmp = call <vscale x 1 x i1> @llvm.epi.vflt.v1i1.v1f64.v1f64(
    <vscale x 1 x double> %vec_a,
    <vscale x 1 x double> %vec_b,
    i64 %gvl)

  %sub = call <vscale x 1 x double> @llvm.epi.vfsub.mask.v1f64.v1f64.v1i1(
    <vscale x 1 x double> %vec_b,
    <vscale x 1 x double> %vec_b,
    <vscale x 1 x double> %vec_a,
    <vscale x 1 x i1> %cmp,
    i64 %gvl)

  %addr_c = bitcast double* %c to <vscale x 1 x double>*
  call void @llvm.epi.vstore.v1f64(
    <vscale x 1 x double> %sub,
    <vscale x 1 x double>* %addr_c,
    i64 %gvl)

  ret void
}
