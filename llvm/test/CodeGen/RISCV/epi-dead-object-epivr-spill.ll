; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -target-abi lp64d -mattr +m,+a,+f,+d,+epi < %s \
; RUN:     | FileCheck %s

define void @n1fv_32(double* %ri, double* %ii, double* %ro, double* %io, i64 %is, i64 %os, i64 %v, i64 %ivs, i64 %ovs) nounwind {
; CHECK-LABEL: n1fv_32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi sp, sp, -128
; CHECK-NEXT:    sd ra, 120(sp)
; CHECK-NEXT:    sd s0, 112(sp)
; CHECK-NEXT:    sd s1, 104(sp)
; CHECK-NEXT:    sd s2, 96(sp)
; CHECK-NEXT:    sd s3, 88(sp)
; CHECK-NEXT:    sd s4, 80(sp)
; CHECK-NEXT:    sd s5, 72(sp)
; CHECK-NEXT:    addi s0, sp, 128
; CHECK-NEXT:    rdvtype a6
; CHECK-NEXT:    rdvl a3
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vsetvl zero, a3, a6
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    andi sp, sp, -16
; CHECK-NEXT:    sd sp, -88(s0)
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    andi sp, sp, -16
; CHECK-NEXT:    sd sp, -96(s0)
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    andi sp, sp, -16
; CHECK-NEXT:    sd sp, -104(s0)
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    andi sp, sp, -16
; CHECK-NEXT:    sd sp, -112(s0)
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    andi sp, sp, -16
; CHECK-NEXT:    sd sp, -120(s0)
; CHECK-NEXT:    addi s5, zero, 8
; CHECK-NEXT:    vsetvli a1, s5, e64, m1
; CHECK-NEXT:    mv s3, a5
; CHECK-NEXT:    mv s4, a4
; CHECK-NEXT:    mv s2, a2
; CHECK-NEXT:    mv s1, a0
; CHECK-NEXT:    vid.v v1
; CHECK-NEXT:    ld a2, -112(s0)
; CHECK-NEXT:    rdvtype a1
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vse.v v1, (a2)
; CHECK-NEXT:    vsetvl zero, a0, a1
; CHECK-NEXT:    vsrl.vi v0, v1, 1
; CHECK-NEXT:    ld a2, -96(s0)
; CHECK-NEXT:    rdvtype a1
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a2)
; CHECK-NEXT:    vsetvl zero, a0, a1
; CHECK-NEXT:    vmul.vx v0, v0, a0
; CHECK-NEXT:    vand.vi v16, v1, 1
; CHECK-NEXT:    ld a2, -88(s0)
; CHECK-NEXT:    rdvtype a1
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vse.v v16, (a2)
; CHECK-NEXT:    vsetvl zero, a0, a1
; CHECK-NEXT:    vmul.vv v1, v16, v0
; CHECK-NEXT:    ld a2, -120(s0)
; CHECK-NEXT:    rdvtype a1
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vse.v v1, (a2)
; CHECK-NEXT:    vsetvl zero, a0, a1
; CHECK-NEXT:    vadd.vv v0, v0, v1
; CHECK-NEXT:    ld a2, -104(s0)
; CHECK-NEXT:    rdvtype a1
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a2)
; CHECK-NEXT:    vsetvl zero, a0, a1
; CHECK-NEXT:    call llvm.epi.mask.cast.nxv1i1.nxv1i64
; CHECK-NEXT:    vsetvli a0, s5, e64, m1
; CHECK-NEXT:    ld a2, -112(s0)
; CHECK-NEXT:    rdvtype a1
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vle.v v1, (a2)
; CHECK-NEXT:    vsetvl zero, a0, a1
; CHECK-NEXT:    vxor.vi v2, v1, 1
; CHECK-NEXT:    ld a2, -96(s0)
; CHECK-NEXT:    rdvtype a1
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vle.v v1, (a2)
; CHECK-NEXT:    vsetvl zero, a0, a1
; CHECK-NEXT:    vmul.vv v1, v1, v0
; CHECK-NEXT:    ld a2, -120(s0)
; CHECK-NEXT:    rdvtype a1
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vle.v v3, (a2)
; CHECK-NEXT:    vsetvl zero, a0, a1
; CHECK-NEXT:    vadd.vv v1, v1, v3
; CHECK-NEXT:    ld a2, -104(s0)
; CHECK-NEXT:    rdvtype a1
; CHECK-NEXT:    rdvl a0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vle.v v13, (a2)
; CHECK-NEXT:    vsetvl zero, a0, a1
; CHECK-NEXT:    vlxe.v v20, (a0), v13
; CHECK-NEXT:    slli a0, s4, 7
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vlxe.v v3, (a0), v13
; CHECK-NEXT:    addi a0, zero, 192
; CHECK-NEXT:    mul a0, s4, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vlxe.v v4, (a0), v13
; CHECK-NEXT:    slli a1, s4, 4
; CHECK-NEXT:    vfsub.vv v5, v20, v3
; CHECK-NEXT:    vfadd.vv v6, v20, v3
; CHECK-NEXT:    vfsub.vv v3, v0, v4
; CHECK-NEXT:    slli a0, s4, 5
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vlxe.v v7, (a0), v13
; CHECK-NEXT:    addi a0, zero, 160
; CHECK-NEXT:    mul a0, s4, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vlxe.v v16, (a0), v13
; CHECK-NEXT:    addi a0, zero, 224
; CHECK-NEXT:    mul a0, s4, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vlxe.v v17, (a0), v13
; CHECK-NEXT:    addi a0, zero, 96
; CHECK-NEXT:    mul a0, s4, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vlxe.v v18, (a0), v13
; CHECK-NEXT:    vfadd.vv v4, v0, v4
; CHECK-NEXT:    vfsub.vv v19, v7, v16
; CHECK-NEXT:    vfadd.vv v7, v7, v16
; CHECK-NEXT:    vfsub.vv v16, v17, v18
; CHECK-NEXT:    vfadd.vv v21, v17, v18
; CHECK-NEXT:    vfadd.vv v17, v6, v4
; CHECK-NEXT:    vfadd.vv v18, v7, v21
; CHECK-NEXT:    vfsub.vv v6, v21, v7
; CHECK-NEXT:    vfsub.vv v4, v16, v19
; CHECK-NEXT:    vfmacc.vv v3, v0, v4
; CHECK-NEXT:    vfmsac.vv v5, v0, v0
; CHECK-NEXT:    vfsgnjn.vv v4, v5, v5
; CHECK-NEXT:    vfadd.vv v7, v0, v20
; CHECK-NEXT:    vfadd.vv v16, v20, v20
; CHECK-NEXT:    vfadd.vv v19, v0, v0
; CHECK-NEXT:    vfmsac.vv v5, v0, v0
; CHECK-NEXT:    vfsgnjn.vv v5, v5, v5
; CHECK-NEXT:    vfadd.vv v21, v16, v19
; CHECK-NEXT:    lui a0, %hi(.LCPI0_0)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI0_0)
; CHECK-NEXT:    fld ft0, 0(a0)
; CHECK-NEXT:    vfsub.vv v8, v0, v21
; CHECK-NEXT:    vfsub.vv v7, v0, v7
; CHECK-NEXT:    vfsub.vv v16, v19, v16
; CHECK-NEXT:    rdvtype ra
; CHECK-NEXT:    rdvl t0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vmv.v.v v19, v7
; CHECK-NEXT:    vsetvl zero, t0, ra
; CHECK-NEXT:    vfmsac.vf v19, ft0, v16
; CHECK-NEXT:    vfsgnjn.vv v19, v19, v19
; CHECK-NEXT:    addi a0, zero, 136
; CHECK-NEXT:    mul a0, s4, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vlxe.v v21, (a0), v13
; CHECK-NEXT:    vlxe.v v22, (s1), v13
; CHECK-NEXT:    addi a0, zero, 104
; CHECK-NEXT:    mul a2, s4, a0
; CHECK-NEXT:    add a2, s1, a2
; CHECK-NEXT:    vlxe.v v23, (a2), v13
; CHECK-NEXT:    vfmacc.vf v16, ft0, v7
; CHECK-NEXT:    vfsub.vv v7, v20, v21
; CHECK-NEXT:    vfadd.vv v21, v20, v21
; CHECK-NEXT:    vfsub.vv v9, v22, v23
; CHECK-NEXT:    vfadd.vv v22, v22, v23
; CHECK-NEXT:    vfadd.vv v23, v0, v9
; CHECK-NEXT:    vfmsac.vv v7, v0, v23
; CHECK-NEXT:    vfsgnjn.vv v7, v7, v7
; CHECK-NEXT:    vfsub.vv v23, v21, v0
; CHECK-NEXT:    vfsub.vv v21, v0, v22
; CHECK-NEXT:    rdvtype ra
; CHECK-NEXT:    rdvl t0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vmv.v.v v22, v23
; CHECK-NEXT:    vsetvl zero, t0, ra
; CHECK-NEXT:    vfmsac.vf v22, ft0, v21
; CHECK-NEXT:    vfsgnjn.vv v22, v22, v22
; CHECK-NEXT:    add a1, s1, a1
; CHECK-NEXT:    vlxe.v v9, (a1), v13
; CHECK-NEXT:    addi a1, zero, 48
; CHECK-NEXT:    mul a1, s4, a1
; CHECK-NEXT:    add a1, s1, a1
; CHECK-NEXT:    vlxe.v v10, (a1), v13
; CHECK-NEXT:    vfmacc.vf v21, ft0, v23
; CHECK-NEXT:    vfsub.vv v23, v9, v20
; CHECK-NEXT:    vfadd.vv v9, v9, v20
; CHECK-NEXT:    vfsub.vv v11, v0, v10
; CHECK-NEXT:    addi a1, zero, 80
; CHECK-NEXT:    mul a1, s4, a1
; CHECK-NEXT:    add a1, s1, a1
; CHECK-NEXT:    vlxe.v v12, (a1), v13
; CHECK-NEXT:    addi a1, zero, 240
; CHECK-NEXT:    mul a1, s4, a1
; CHECK-NEXT:    add a1, s1, a1
; CHECK-NEXT:    vlxe.v v13, (a1), v13
; CHECK-NEXT:    vfadd.vv v10, v10, v0
; CHECK-NEXT:    vfsub.vv v14, v12, v0
; CHECK-NEXT:    vfadd.vv v12, v12, v0
; CHECK-NEXT:    vfsub.vv v15, v13, v20
; CHECK-NEXT:    vfadd.vv v20, v13, v20
; CHECK-NEXT:    vfadd.vv v10, v20, v10
; CHECK-NEXT:    vfadd.vv v13, v9, v12
; CHECK-NEXT:    rdvtype ra
; CHECK-NEXT:    rdvl t0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vmv.v.v v20, v23
; CHECK-NEXT:    vsetvl zero, t0, ra
; CHECK-NEXT:    vfmsac.vf v20, ft0, v14
; CHECK-NEXT:    vfsgnjn.vv v20, v20, v20
; CHECK-NEXT:    rdvtype ra
; CHECK-NEXT:    rdvl t0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vmv.v.v v24, v15
; CHECK-NEXT:    vsetvl zero, t0, ra
; CHECK-NEXT:    vfmsac.vf v24, ft0, v11
; CHECK-NEXT:    vfsgnjn.vv v24, v24, v24
; CHECK-NEXT:    vfsub.vv v20, v24, v20
; CHECK-NEXT:    vfmacc.vf v14, ft0, v23
; CHECK-NEXT:    vfmacc.vf v11, ft0, v15
; CHECK-NEXT:    vfadd.vv v23, v14, v11
; CHECK-NEXT:    vfsub.vv v9, v9, v12
; CHECK-NEXT:    vfsub.vv v11, v17, v18
; CHECK-NEXT:    vfsub.vv v9, v0, v9
; CHECK-NEXT:    rdvtype ra
; CHECK-NEXT:    rdvl t0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vmv.v.v v12, v11
; CHECK-NEXT:    vsetvl zero, t0, ra
; CHECK-NEXT:    vfmsac.vv v12, v0, v0
; CHECK-NEXT:    vfsgnjn.vv v12, v12, v12
; CHECK-NEXT:    vfmacc.vv v11, v0, v0
; CHECK-NEXT:    vfsub.vv v14, v10, v13
; CHECK-NEXT:    vfsub.vv v8, v8, v0
; CHECK-NEXT:    rdvtype ra
; CHECK-NEXT:    rdvl t0
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vmv.v.v v15, v14
; CHECK-NEXT:    vsetvl zero, t0, ra
; CHECK-NEXT:    vfmsac.vv v15, v0, v8
; CHECK-NEXT:    vfsgnjn.vv v15, v15, v15
; CHECK-NEXT:    vfmacc.vv v14, v0, v8
; CHECK-NEXT:    vfsgnjn.vv v15, v15, v15, v0.t
; CHECK-NEXT:    vrgather.vv v8, v15, v2
; CHECK-NEXT:    vfsub.vv v15, v12, v8
; CHECK-NEXT:    ld a3, -88(s0)
; CHECK-NEXT:    rdvtype a2
; CHECK-NEXT:    rdvl a1
; CHECK-NEXT:    vsetvli zero, zero, e64, m1
; CHECK-NEXT:    vle.v v24, (a3)
; CHECK-NEXT:    vsetvl zero, a1, a2
; CHECK-NEXT:    vrgather.vv v15, v15, v24
; CHECK-NEXT:    vsxe.v v15, (a0), v1
; CHECK-NEXT:    slli a1, s3, 5
; CHECK-NEXT:    add a1, s2, a1
; CHECK-NEXT:    vfsgnjn.vv v14, v14, v14, v0.t
; CHECK-NEXT:    vrgather.vv v14, v14, v2
; CHECK-NEXT:    vfadd.vv v15, v11, v14
; CHECK-NEXT:    vrgather.vv v15, v15, v24
; CHECK-NEXT:    vsxe.v v15, (a1), v1
; CHECK-NEXT:    vfadd.vv v8, v12, v8
; CHECK-NEXT:    vrgather.vv v8, v8, v24
; CHECK-NEXT:    vsxe.v v8, (a0), v1
; CHECK-NEXT:    vfsub.vv v8, v11, v14
; CHECK-NEXT:    vrgather.vv v8, v8, v24
; CHECK-NEXT:    vsxe.v v8, (a0), v1
; CHECK-NEXT:    vfadd.vv v17, v17, v18
; CHECK-NEXT:    vfadd.vv v18, v13, v10
; CHECK-NEXT:    vfadd.vv v17, v17, v18
; CHECK-NEXT:    vrgather.vv v18, v0, v24
; CHECK-NEXT:    vsxe.v v0, (a0), v1
; CHECK-NEXT:    vfadd.vv v17, v17, v0
; CHECK-NEXT:    vrgather.vv v17, v17, v24
; CHECK-NEXT:    vsxe.v v17, (a0), v1
; CHECK-NEXT:    vsxe.v v18, (a0), v1
; CHECK-NEXT:    vsxe.v v0, (a0), v1
; CHECK-NEXT:    addi a1, zero, 208
; CHECK-NEXT:    mul a1, s3, a1
; CHECK-NEXT:    lui a2, %hi(.LCPI0_1)
; CHECK-NEXT:    addi a2, a2, %lo(.LCPI0_1)
; CHECK-NEXT:    fld ft0, 0(a2)
; CHECK-NEXT:    add a1, s2, a1
; CHECK-NEXT:    vsxe.v v18, (a1), v1
; CHECK-NEXT:    vfadd.vv v17, v22, v19
; CHECK-NEXT:    vfmsac.vf v17, ft0, v17
; CHECK-NEXT:    vfsgnjn.vv v17, v17, v17
; CHECK-NEXT:    vfmacc.vv v6, v0, v9
; CHECK-NEXT:    vfsub.vv v16, v16, v21
; CHECK-NEXT:    vfmacc.vf v6, ft0, v16
; CHECK-NEXT:    addi a1, zero, 112
; CHECK-NEXT:    mul a1, s3, a1
; CHECK-NEXT:    add a1, s2, a1
; CHECK-NEXT:    vrgather.vv v16, v0, v2
; CHECK-NEXT:    vfsub.vv v16, v17, v16
; CHECK-NEXT:    vrgather.vv v16, v16, v24
; CHECK-NEXT:    vsxe.v v16, (a1), v1
; CHECK-NEXT:    vfsgnjn.vv v6, v6, v6, v0.t
; CHECK-NEXT:    vrgather.vv v6, v6, v2
; CHECK-NEXT:    vfadd.vv v6, v0, v6
; CHECK-NEXT:    vrgather.vv v6, v6, v24
; CHECK-NEXT:    vfmacc.vf v4, ft0, v23
; CHECK-NEXT:    vfmacc.vf v3, ft0, v20
; CHECK-NEXT:    lui a1, %hi(.LCPI0_2)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_2)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    vsxe.v v6, (a0), v1
; CHECK-NEXT:    vsxe.v v0, (a0), v1
; CHECK-NEXT:    vfsgnjn.vv v6, v0, v0
; CHECK-NEXT:    vfmsac.vf v5, ft0, v0
; CHECK-NEXT:    vfmacc.vf v7, ft0, v0
; CHECK-NEXT:    lui a1, %hi(.LCPI0_3)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_3)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    vfsgnjn.vv v5, v5, v5
; CHECK-NEXT:    vfsub.vv v5, v6, v5
; CHECK-NEXT:    vfadd.vv v6, v7, v0
; CHECK-NEXT:    vfmsac.vf v4, ft0, v6
; CHECK-NEXT:    vfsgnjn.vv v4, v4, v4
; CHECK-NEXT:    vfmsac.vf v3, ft0, v5
; CHECK-NEXT:    vfsgnjn.vv v3, v3, v3
; CHECK-NEXT:    mul a0, s3, a0
; CHECK-NEXT:    add a0, s2, a0
; CHECK-NEXT:    vfsgnjn.vv v3, v3, v3, v0.t
; CHECK-NEXT:    vrgather.vv v0, v3, v2
; CHECK-NEXT:    vfsub.vv v0, v4, v0
; CHECK-NEXT:    vrgather.vv v0, v0, v24
; CHECK-NEXT:    vsxe.v v0, (a0), v1
entry:
  %0 = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FEA9B66290EA1A3, i64 8)
  %1 = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FE561B82AB7F990, i64 8)
  %2 = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FED906BCF328D46, i64 8)
  %3 = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FDA827999FCEF32, i64 8)
  %cmp1384 = icmp sgt i64 %v, 0
  %4 = tail call <vscale x 1 x i64> @llvm.epi.vid.nxv1i64(i64 8) #4
  %5 = tail call <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64 1, i64 8) #4
  %6 = tail call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(<vscale x 1 x i64> %4, <vscale x 1 x i64> %5, i64 8) #4
  %7 = tail call <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64 undef, i64 8) #4
  %8 = tail call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64> %6, <vscale x 1 x i64> %7, i64 8) #4
  %9 = tail call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64> %4, <vscale x 1 x i64> %5, i64 8) #4
  %10 = tail call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64> %9, <vscale x 1 x i64> undef, i64 8) #4
  %11 = tail call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(<vscale x 1 x i64> %8, <vscale x 1 x i64> %10, i64 8) #4
  %12 = tail call <vscale x 1 x i1> @llvm.epi.mask.cast.nxv1i1.nxv1i64(<vscale x 1 x i64> %9) #4
  %13 = tail call <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(<vscale x 1 x i64> %4, <vscale x 1 x i64> %5, i64 8) #4
  %14 = tail call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64> %6, <vscale x 1 x i64> undef, i64 8) #4
  %15 = tail call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(<vscale x 1 x i64> %14, <vscale x 1 x i64> %10, i64 8) #4
  %mul283 = shl nsw i64 %ovs, 2
  %16 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %mul = shl nsw i64 %is, 4
  %arrayidx2 = getelementptr inbounds double, double* %ri, i64 %mul
  %17 = bitcast double* %arrayidx2 to <vscale x 1 x double>*
  %18 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %17, <vscale x 1 x i64> %11, i64 8) #4
  %19 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %16, <vscale x 1 x double> %18, i64 8)
  %20 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %16, <vscale x 1 x double> %18, i64 8)
  %mul5 = shl nsw i64 %is, 3
  %arrayidx6 = getelementptr inbounds double, double* %ri, i64 %mul5
  %21 = bitcast double* %arrayidx6 to <vscale x 1 x double>*
  %mul9 = mul nsw i64 %is, 24
  %arrayidx10 = getelementptr inbounds double, double* %ri, i64 %mul9
  %22 = bitcast double* %arrayidx10 to <vscale x 1 x double>*
  %23 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %22, <vscale x 1 x i64> %11, i64 8) #4
  %24 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %23, i64 8)
  %25 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %23, i64 8)
  %mul13 = shl nsw i64 %is, 2
  %arrayidx14 = getelementptr inbounds double, double* %ri, i64 %mul13
  %26 = bitcast double* %arrayidx14 to <vscale x 1 x double>*
  %27 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %26, <vscale x 1 x i64> %11, i64 8) #4
  %mul17 = mul nsw i64 %is, 20
  %arrayidx18 = getelementptr inbounds double, double* %ri, i64 %mul17
  %28 = bitcast double* %arrayidx18 to <vscale x 1 x double>*
  %29 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %28, <vscale x 1 x i64> %11, i64 8) #4
  %30 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %27, <vscale x 1 x double> %29, i64 8)
  %31 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %27, <vscale x 1 x double> %29, i64 8)
  %mul21 = mul nsw i64 %is, 28
  %arrayidx22 = getelementptr inbounds double, double* %ri, i64 %mul21
  %32 = bitcast double* %arrayidx22 to <vscale x 1 x double>*
  %33 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %32, <vscale x 1 x i64> %11, i64 8) #4
  %mul25 = mul nsw i64 %is, 12
  %arrayidx26 = getelementptr inbounds double, double* %ri, i64 %mul25
  %34 = bitcast double* %arrayidx26 to <vscale x 1 x double>*
  %35 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %34, <vscale x 1 x i64> %11, i64 8) #4
  %36 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %33, <vscale x 1 x double> %35, i64 8)
  %37 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %33, <vscale x 1 x double> %35, i64 8)
  %38 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %20, <vscale x 1 x double> %25, i64 8)
  %39 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %31, <vscale x 1 x double> %37, i64 8)
  %40 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %37, <vscale x 1 x double> %31, i64 8)
  %41 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %36, <vscale x 1 x double> %30, i64 8)
  %42 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %24, <vscale x 1 x double> undef, <vscale x 1 x double> %41, i64 8)
  %43 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %24, <vscale x 1 x double> undef, <vscale x 1 x double> %41, i64 8)
  %44 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %19, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %45 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %19, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %46 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %45, <vscale x 1 x double> %45, i64 8)
  %47 = bitcast double* undef to <vscale x 1 x double>*
  %48 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %47, <vscale x 1 x i64> %11, i64 8) #4
  %arrayidx45 = getelementptr inbounds double, double* %ri, i64 undef
  %49 = bitcast double* %arrayidx45 to <vscale x 1 x double>*
  %50 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %48, i64 8)
  %51 = bitcast double* undef to <vscale x 1 x double>*
  %52 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %51, <vscale x 1 x i64> %11, i64 8) #4
  %mul54 = mul nsw i64 %is, 19
  %53 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %54 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %52, <vscale x 1 x double> %53, i64 8)
  %55 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %52, <vscale x 1 x double> %53, i64 8)
  %mul64 = mul nsw i64 %is, 11
  %56 = bitcast double* undef to <vscale x 1 x double>*
  %57 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %58 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %54, <vscale x 1 x double> undef, i64 8)
  %59 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> %58, i64 8)
  %60 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> %58, i64 8)
  %61 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %62 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %61, <vscale x 1 x double> %61, i64 8)
  %63 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %55, <vscale x 1 x double> %57, i64 8)
  %64 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %63, i64 8)
  %65 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %50, i64 8)
  %66 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %57, <vscale x 1 x double> %55, i64 8)
  %67 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %65, <vscale x 1 x double> %3, <vscale x 1 x double> %66, i64 8)
  %68 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %67, <vscale x 1 x double> %67, i64 8)
  %69 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %66, <vscale x 1 x double> %3, <vscale x 1 x double> %65, i64 8)
  %70 = bitcast double* undef to <vscale x 1 x double>*
  %71 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %70, <vscale x 1 x i64> %11, i64 8) #4
  %mul74 = mul nsw i64 %is, 17
  %arrayidx75 = getelementptr inbounds double, double* %ri, i64 %mul74
  %72 = bitcast double* %arrayidx75 to <vscale x 1 x double>*
  %73 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %72, <vscale x 1 x i64> %11, i64 8) #4
  %74 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %71, <vscale x 1 x double> %73, i64 8)
  %75 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %71, <vscale x 1 x double> %73, i64 8)
  %mul79 = mul nsw i64 %is, 9
  %arrayidx85 = getelementptr inbounds double, double* %ri, i64 undef
  %76 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %77 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %mul94 = mul nsw i64 %is, 21
  %78 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %arrayidx100 = getelementptr inbounds double, double* %ri, i64 undef
  %79 = bitcast double* %arrayidx100 to <vscale x 1 x double>*
  %80 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %79, <vscale x 1 x i64> %11, i64 8) #4
  %mul104 = mul nsw i64 %is, 13
  %arrayidx105 = getelementptr inbounds double, double* %ri, i64 %mul104
  %81 = bitcast double* %arrayidx105 to <vscale x 1 x double>*
  %82 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %81, <vscale x 1 x i64> %11, i64 8) #4
  %83 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %80, <vscale x 1 x double> %82, i64 8)
  %84 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %80, <vscale x 1 x double> %82, i64 8)
  %85 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %83, i64 8)
  %86 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %74, <vscale x 1 x double> undef, <vscale x 1 x double> %85, i64 8)
  %87 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %74, <vscale x 1 x double> undef, <vscale x 1 x double> %85, i64 8)
  %88 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %87, <vscale x 1 x double> %87, i64 8)
  %89 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %84, i64 8)
  %90 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %75, <vscale x 1 x double> undef, i64 8)
  %91 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %84, i64 8)
  %92 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %90, <vscale x 1 x double> %3, <vscale x 1 x double> %91, i64 8)
  %93 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %92, <vscale x 1 x double> %92, i64 8)
  %94 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %91, <vscale x 1 x double> %3, <vscale x 1 x double> %90, i64 8)
  %mul109 = shl nsw i64 %is, 1
  %arrayidx110 = getelementptr inbounds double, double* %ri, i64 %mul109
  %95 = bitcast double* %arrayidx110 to <vscale x 1 x double>*
  %96 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %95, <vscale x 1 x i64> %11, i64 8) #4
  %97 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %98 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %96, <vscale x 1 x double> %97, i64 8)
  %99 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %96, <vscale x 1 x double> %97, i64 8)
  %mul121 = mul nsw i64 %is, 6
  %arrayidx122 = getelementptr inbounds double, double* %ri, i64 %mul121
  %100 = bitcast double* %arrayidx122 to <vscale x 1 x double>*
  %101 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %100, <vscale x 1 x i64> %11, i64 8) #4
  %102 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %101, i64 8)
  %103 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %101, <vscale x 1 x double> undef, i64 8)
  %mul125 = mul nsw i64 %is, 10
  %arrayidx126 = getelementptr inbounds double, double* %ri, i64 %mul125
  %104 = bitcast double* %arrayidx126 to <vscale x 1 x double>*
  %105 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %104, <vscale x 1 x i64> %11, i64 8) #4
  %106 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %105, <vscale x 1 x double> undef, i64 8)
  %107 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %105, <vscale x 1 x double> undef, i64 8)
  %mul133 = mul nsw i64 %is, 30
  %arrayidx134 = getelementptr inbounds double, double* %ri, i64 %mul133
  %108 = bitcast double* %arrayidx134 to <vscale x 1 x double>*
  %109 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %108, <vscale x 1 x i64> %11, i64 8) #4
  %mul137 = mul nsw i64 %is, 14
  %110 = bitcast double* undef to <vscale x 1 x double>*
  %111 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %110, <vscale x 1 x i64> %11, i64 8) #4
  %112 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %109, <vscale x 1 x double> %111, i64 8)
  %113 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %109, <vscale x 1 x double> %111, i64 8)
  %114 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %113, <vscale x 1 x double> %103, i64 8)
  %115 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %99, <vscale x 1 x double> %107, i64 8)
  %116 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %98, <vscale x 1 x double> %3, <vscale x 1 x double> %106, i64 8)
  %117 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %116, <vscale x 1 x double> %116, i64 8)
  %118 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %112, <vscale x 1 x double> %3, <vscale x 1 x double> %102, i64 8)
  %119 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %118, <vscale x 1 x double> %118, i64 8)
  %120 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %119, <vscale x 1 x double> %117, i64 8)
  %121 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %106, <vscale x 1 x double> %3, <vscale x 1 x double> %98, i64 8)
  %122 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %102, <vscale x 1 x double> %3, <vscale x 1 x double> %112, i64 8)
  %123 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %121, <vscale x 1 x double> %122, i64 8)
  %124 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %99, <vscale x 1 x double> %107, i64 8)
  %125 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %124, i64 8)
  %126 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %38, <vscale x 1 x double> %39, i64 8)
  %127 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %126, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %128 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %127, <vscale x 1 x double> %127, i64 8)
  %129 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %126, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %130 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %114, <vscale x 1 x double> %115, i64 8)
  %131 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %64, <vscale x 1 x double> undef, i64 8)
  %132 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %130, <vscale x 1 x double> undef, <vscale x 1 x double> %131, i64 8)
  %133 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %132, <vscale x 1 x double> %132, i64 8)
  %134 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %130, <vscale x 1 x double> undef, <vscale x 1 x double> %131, i64 8)
  %135 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %133, <vscale x 1 x double> %133, <vscale x 1 x double> %133, <vscale x 1 x i1> %12, i64 8) #4
  %136 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %135, <vscale x 1 x i64> %13, i64 8) #4
  %137 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %128, <vscale x 1 x double> %136, i64 8)
  %138 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %137, <vscale x 1 x i64> %9, i64 8) #4
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %138, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %mul145 = shl nsw i64 %os, 2
  %arrayidx146 = getelementptr inbounds double, double* %ro, i64 %mul145
  %139 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %134, <vscale x 1 x double> %134, <vscale x 1 x double> %134, <vscale x 1 x i1> %12, i64 8) #4
  %140 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %139, <vscale x 1 x i64> %13, i64 8) #4
  %141 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %129, <vscale x 1 x double> %140, i64 8)
  %142 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %141, <vscale x 1 x i64> %9, i64 8) #4
  %143 = bitcast double* %arrayidx146 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %142, <vscale x 1 x double>* %143, <vscale x 1 x i64> %15, i64 8) #4
  %144 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %128, <vscale x 1 x double> %136, i64 8)
  %145 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %144, <vscale x 1 x i64> %9, i64 8) #4
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %145, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %146 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %129, <vscale x 1 x double> %140, i64 8)
  %147 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %146, <vscale x 1 x i64> %9, i64 8) #4
  %148 = bitcast double* undef to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %147, <vscale x 1 x double>* %148, <vscale x 1 x i64> %15, i64 8) #4
  %149 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %38, <vscale x 1 x double> %39, i64 8)
  %150 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %115, <vscale x 1 x double> %114, i64 8)
  %151 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %149, <vscale x 1 x double> %150, i64 8)
  %152 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %149, <vscale x 1 x double> %150, i64 8)
  %153 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %63, i64 8)
  %154 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> %9, i64 8) #4
  %155 = bitcast double* undef to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x double>* %155, <vscale x 1 x i64> %15, i64 8) #4
  %156 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %151, <vscale x 1 x double> undef, i64 8)
  %157 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %156, <vscale x 1 x i64> %9, i64 8) #4
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %157, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %158 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> %9, i64 8) #4
  %mul1661318.pn = mul nsw i64 %os, 24
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %158, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %159 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %160 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %159, <vscale x 1 x double> %159, i64 8)
  %161 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %160, <vscale x 1 x double> %2, <vscale x 1 x double> undef, i64 8)
  %162 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %160, <vscale x 1 x double> %2, <vscale x 1 x double> undef, i64 8)
  %163 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %40, <vscale x 1 x double> undef, <vscale x 1 x double> %125, i64 8)
  %164 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %68, <vscale x 1 x double> %93, i64 8)
  %165 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %2, <vscale x 1 x double> %164, i64 8)
  %166 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %2, <vscale x 1 x double> %164, i64 8)
  %167 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %166, <vscale x 1 x double> %166, i64 8)
  %mul170 = mul nsw i64 %os, 10
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %mul174 = mul nsw i64 %os, 26
  %arrayidx175 = getelementptr inbounds double, double* %ro, i64 %mul174
  %168 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> %9, i64 8) #4
  %169 = bitcast double* %arrayidx175 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %168, <vscale x 1 x double>* %169, <vscale x 1 x i64> %15, i64 8) #4
  %mul1781323 = mul nsw i64 %os, 22
  %arrayidx1791324 = getelementptr inbounds double, double* %ro, i64 %mul1781323
  %170 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %171 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %93, <vscale x 1 x double> %68, i64 8)
  %172 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %2, <vscale x 1 x double> %171, i64 8)
  %173 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %172, <vscale x 1 x double> %172, i64 8)
  %174 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %40, <vscale x 1 x double> undef, <vscale x 1 x double> %125, i64 8)
  %175 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %69, <vscale x 1 x double> %94, i64 8)
  %176 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %174, <vscale x 1 x double> %2, <vscale x 1 x double> %175, i64 8)
  %177 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %174, <vscale x 1 x double> %2, <vscale x 1 x double> %175, i64 8)
  %mul186 = mul nsw i64 %os, 14
  %arrayidx187 = getelementptr inbounds double, double* %ro, i64 %mul186
  %178 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> %13, i64 8) #4
  %179 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %173, <vscale x 1 x double> %178, i64 8)
  %180 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %179, <vscale x 1 x i64> %9, i64 8) #4
  %181 = bitcast double* %arrayidx187 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %180, <vscale x 1 x double>* %181, <vscale x 1 x i64> %15, i64 8) #4
  %182 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %177, <vscale x 1 x double> %177, <vscale x 1 x double> %177, <vscale x 1 x i1> %12, i64 8) #4
  %183 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %182, <vscale x 1 x i64> %13, i64 8) #4
  %184 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %183, i64 8)
  %185 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %184, <vscale x 1 x i64> %9, i64 8) #4
  %186 = bitcast double* undef to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %185, <vscale x 1 x double>* %186, <vscale x 1 x i64> %15, i64 8) #4
  %187 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %173, <vscale x 1 x double> %178, i64 8)
  %188 = bitcast double* undef to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x double>* %188, <vscale x 1 x i64> %15, i64 8) #4
  %189 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %183, i64 8)
  %190 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %189, <vscale x 1 x i64> %9, i64 8) #4
  %191 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %46, <vscale x 1 x double> %2, <vscale x 1 x double> %123, i64 8)
  %192 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %43, <vscale x 1 x double> %2, <vscale x 1 x double> %120, i64 8)
  %193 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %194 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %46, <vscale x 1 x double> %2, <vscale x 1 x double> %123, i64 8)
  %195 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %196 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %62, <vscale x 1 x double> %1, <vscale x 1 x double> undef, i64 8)
  %197 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %196, <vscale x 1 x double> %196, i64 8)
  %198 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %195, <vscale x 1 x double> %197, i64 8)
  %199 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %88, <vscale x 1 x double> %1, <vscale x 1 x double> undef, i64 8)
  %200 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %199, <vscale x 1 x double> undef, i64 8)
  %201 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %191, <vscale x 1 x double> %0, <vscale x 1 x double> %200, i64 8)
  %202 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %201, <vscale x 1 x double> %201, i64 8)
  %203 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %192, <vscale x 1 x double> %0, <vscale x 1 x double> %198, i64 8)
  %204 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %203, <vscale x 1 x double> %203, i64 8)
  %mul202 = mul nsw i64 %os, 13
  %arrayidx203 = getelementptr inbounds double, double* %ro, i64 %mul202
  %205 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %204, <vscale x 1 x double> %204, <vscale x 1 x double> %204, <vscale x 1 x i1> %12, i64 8) #4
  %206 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %205, <vscale x 1 x i64> %13, i64 8) #4
  %207 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %202, <vscale x 1 x double> %206, i64 8)
  %arrayidx206 = getelementptr inbounds double, double* %ro, i64 %os
  %208 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %207, <vscale x 1 x i64> %9, i64 8) #4
  %209 = bitcast double* %arrayidx203 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %208, <vscale x 1 x double>* %209, <vscale x 1 x i64> %15, i64 8) #4
  %210 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %202, <vscale x 1 x double> %206, i64 8)
  %211 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %210, <vscale x 1 x i64> %9, i64 8) #4
  %arrayidx2081347 = getelementptr inbounds double, double* %ro, i64 undef
  %212 = bitcast double* %arrayidx2081347 to <vscale x 1 x double>*
  unreachable
}

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vid.nxv1i64(i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readonly
declare <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* nocapture, <vscale x 1 x i64>, i64) #2

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind writeonly
declare void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, <vscale x 1 x i64>, i64) #3

; Function Attrs: nounwind readnone
declare <vscale x 1 x i1> @llvm.epi.mask.cast.nxv1i1.nxv1i64(<vscale x 1 x i64>) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1
